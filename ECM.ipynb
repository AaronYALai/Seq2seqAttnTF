{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting\n",
    "#### Prime numbers as simulated emotion words and emotion categories:\n",
    "- ECM should use external memory when predicting primes\n",
    "- Primes are equally split into (num_emo) \"emotion\" categories\n",
    "- sequence with most primes from a certain category is tagged with that \"emotion\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a number is a prime\n",
    "def is_prime(n):\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(3, int(np.sqrt(n)) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "num_emo = 4\n",
    "\n",
    "nums = np.arange(N)\n",
    "check_prime = np.vectorize(is_prime)\n",
    "primes = nums[check_prime(nums)]\n",
    "\n",
    "# equally split primes into categories\n",
    "s_primes = np.array_split(primes, num_emo)\n",
    "s_primes = [s_p.tolist() for s_p in s_primes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "source_data = []\n",
    "target_data = []\n",
    "choice_data = []\n",
    "category_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    # 1: emotion words/primes, 0: generic words\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "qdf = pd.DataFrame(data={\"0\": choice_data})\n",
    "cdf = pd.DataFrame(data={\"0\": category_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)\n",
    "qdf.to_csv(\"./example/train_choice.txt\", header=None, index=None)\n",
    "cdf.to_csv(\"./example/train_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_choice_data = []\n",
    "dev_category_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    dev_choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    dev_category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "qddf = pd.DataFrame(data={\"0\": dev_choice_data})\n",
    "cddf = pd.DataFrame(data={\"0\": dev_category_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)\n",
    "qddf.to_csv(\"./example/dev_choice.txt\", header=None, index=None)\n",
    "cddf.to_csv(\"./example/dev_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECM wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "ECMState = collections.namedtuple(\n",
    "    \"ECMState\", (\"cell_states\", \"h\", \"context\", \"internal_memory\"))\n",
    "\n",
    "\n",
    "class ECMWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Emotion Chatting Machine: H. Zhou, et al. AAAI 2018\n",
    "    (https://arxiv.org/abs/1704.01074)\n",
    "    Emotion Category Embedding, Internal and External Memory Modules\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "        emo_cat_embs: category embeddings, [batch_size, emo_cat_units]\n",
    "        emo_cat: emotion category, [batch_size]\n",
    "        emo_int_units: dimension of internal emotion memory\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype, emo_cat_embs, emo_cat, num_emo,\n",
    "                 emo_int_units, emo_init=None):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = ECMState(self._cell.state_size,\n",
    "                                    num_units, memory.shape[-1].value,\n",
    "                                    emo_int_units)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "        # ECM hyperparameters\n",
    "        self._emo_cat_embs = emo_cat_embs\n",
    "        self._emo_cat = emo_cat\n",
    "        self._emo_int_units = emo_int_units\n",
    "\n",
    "        # internal memory\n",
    "        if emo_init is None:\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.int_memory = tf.Variable(\n",
    "            initializer(shape=(num_emo, emo_int_units)),\n",
    "            name=\"emo_memory\", dtype=dtype)\n",
    "\n",
    "        self.read_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_read_gate\")\n",
    "        self.write_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_write_gate\")\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for ECM wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        Returns:\n",
    "            h_0: [batch_size, num_units]\n",
    "            context_0: [batch_size, num_units]\n",
    "            M_emo_0: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "        M_emo_0 = tf.gather(self.int_memory, self._emo_cat)\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        ecm_state_0 = ECMState(cell_states, h_0, context_0, M_emo_0)\n",
    "\n",
    "        return ecm_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def _read_internal_memory(self, M_emo, read_inputs):\n",
    "        \"\"\"\n",
    "        Read the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            read_inputs: [batch_size, d]\n",
    "        Returns:\n",
    "            M_read: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_read = tf.nn.sigmoid(self.read_g(read_inputs))\n",
    "        return (M_emo * gate_read)\n",
    "\n",
    "    def _write_internal_memory(self, M_emo, new_h):\n",
    "        \"\"\"\n",
    "        Write the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            new_h: [batch_size, num_units]\n",
    "        Returns:\n",
    "            M_write: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_write = tf.nn.sigmoid(self.write_g(new_h))\n",
    "        return (M_emo * gate_write)\n",
    "\n",
    "    def __call__(self, inputs, ecm_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs, context, int_memory)\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context, M_emo = ecm_states\n",
    "\n",
    "        # read internal memory\n",
    "        read_inputs = tf.concat([inputs, h, context], axis=-1)\n",
    "        M_read = self._read_internal_memory(M_emo, read_inputs)\n",
    "\n",
    "        # pass into RNN_cell to get the output\n",
    "        x = [inputs, h, context, self._emo_cat_embs, M_read]\n",
    "        x = tf.concat(x, axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        # update states\n",
    "        new_M_emo = self._write_internal_memory(M_emo, new_h)\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_ecm_states = ECMState(cell_states, new_h, new_context, new_M_emo)\n",
    "\n",
    "        return (new_h, new_ecm_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ECMBeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states, output_layer,\n",
    "                 emo_output_layer, emo_choice_layer, batch_size, dtype,\n",
    "                 beam_size, vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._origin_vocab_size = vocab_size\n",
    "        self._vocab_size = vocab_size * 2\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "\n",
    "        self._output_layer = output_layer\n",
    "        self._emo_output_layer = emo_output_layer\n",
    "        self._emo_choice_layer = emo_choice_layer\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "\n",
    "        indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat_embs\"):\n",
    "            self._cell._emo_cat_embs = tf.gather(\n",
    "                self._cell._emo_cat_embs, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat\"):\n",
    "            self._emo_cat = tf.gather(self._cell._emo_cat, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new log probs\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        gen_log_probs = tf.nn.log_softmax(self._output_layer(new_h))\n",
    "        emo_log_probs = tf.nn.log_softmax(self._emo_output_layer(new_h))\n",
    "        alphas = tf.nn.sigmoid(self._emo_choice_layer(new_h))\n",
    "\n",
    "        gen_log_probs = gen_log_probs + tf.log(1 - alphas)\n",
    "        emo_log_probs = emo_log_probs + tf.log(alphas)\n",
    "        raw_log_probs = tf.concat([gen_log_probs, emo_log_probs], axis=-1)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        raw_log_probs = split_batch_beam(raw_log_probs, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: mask log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = mask_log_probs(\n",
    "            raw_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(\n",
    "            self._embeddings, (new_ids % self._origin_vocab_size))\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=raw_log_probs, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_ECM_decoder(encoder_outputs, encoder_states, embeddings, num_layers,\n",
    "                      num_units, cell_type, num_emo, emo_cat, emo_cat_units,\n",
    "                      emo_int_units, state_pass=True, infer_batch_size=None,\n",
    "                      attn_num_units=128, target_ids=None, beam_size=None,\n",
    "                      max_iter=20, dtype=tf.float32, forget_bias=1.0,\n",
    "                      name=\"ECM_decoder\"):\n",
    "    \"\"\"\n",
    "    ECM decoder: build ECM decoder with emotion category embedding,\n",
    "             internal & external memory modules\n",
    "        target_ids: [batch_size, max_time]\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "        num_emo: number of emotions\n",
    "        emo_cat: emotion catogories, [batch_size]\n",
    "        emo_cat_units: dimension of emotion category embeddings\n",
    "        emo_int_units: dimension of emotion internal memory\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: (generic_logits, emo_ext_logits, alphas, int_M_emo),\n",
    "            first 3 shape: [batch_size, max_time, d]\n",
    "            int_M_emo: [batch_size, emo_int_units]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # create emotion category embeddings\n",
    "        emo_init = tf.contrib.layers.xavier_initializer()\n",
    "        emo_cat_embeddings = tf.Variable(\n",
    "            emo_init(shape=(num_emo, emo_cat_units)),\n",
    "            name=\"emo_cat_embeddings\", dtype=dtype)\n",
    "        emo_cat_embs = tf.nn.embedding_lookup(emo_cat_embeddings, emo_cat)\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with ECM internal memory module\n",
    "        memory = encoder_outputs\n",
    "\n",
    "        cell = ECMWrapper(\n",
    "            cell, memory, dec_init_states, attn_num_units, num_units, dtype,\n",
    "            emo_cat_embs, emo_cat, num_emo, emo_int_units)\n",
    "\n",
    "        dec_init_states = cell.initial_state()\n",
    "\n",
    "        # ECM external memory module\n",
    "        emo_output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"emo_output_projection\")\n",
    "\n",
    "        emo_choice_layer = tf.layers.Dense(\n",
    "            1, use_bias=False, name=\"emo_choice_alpha\")\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits & final internal memory states\n",
    "            generic_logits = output_layer(decoder_outputs)\n",
    "            emo_ext_logits = emo_output_layer(decoder_outputs)\n",
    "            alphas = tf.nn.sigmoid(emo_choice_layer(decoder_outputs))\n",
    "            int_M_emo = decoder_states.internal_memory\n",
    "\n",
    "            train_outputs = (generic_logits, emo_ext_logits, alphas, int_M_emo)\n",
    "\n",
    "        # Decode - for inference, beam search\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            decoder_cell = ECMBeamSearchDecodeCell(\n",
    "                embeddings, cell, dec_init_states, output_layer,\n",
    "                emo_output_layer, emo_choice_layer,\n",
    "                infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                div_gamma=None, div_prob=None)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return cell, train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "emo_cat = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=True, target_ids=target_ids, name=\"ECM_decoder2\")\n",
    "\n",
    "g_logits, e_logits, alphas, M_emo = train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.00099624, 0.00099855, 0.00099248, ..., 0.00100086,\n",
       "          0.00099934, 0.00099578],\n",
       "         [0.00099797, 0.00099881, 0.00099346, ..., 0.00100298,\n",
       "          0.00100237, 0.0009953 ],\n",
       "         [0.00099984, 0.00099884, 0.00099453, ..., 0.00100559,\n",
       "          0.00100482, 0.00099417],\n",
       "         [0.00100173, 0.0009989 , 0.0009955 , ..., 0.00100848,\n",
       "          0.00100675, 0.00099273]],\n",
       " \n",
       "        [[0.00099792, 0.00099826, 0.00099837, ..., 0.00099963,\n",
       "          0.00099901, 0.00099767],\n",
       "         [0.00099751, 0.00100034, 0.00100131, ..., 0.00100095,\n",
       "          0.00099924, 0.0009958 ],\n",
       "         [0.00099669, 0.00100145, 0.00100437, ..., 0.0010017 ,\n",
       "          0.00099889, 0.00099425],\n",
       "         [0.00099637, 0.00100211, 0.00100693, ..., 0.00100148,\n",
       "          0.00099712, 0.00099196]]], dtype=float32), array([[[0.4986367 ],\n",
       "         [0.49700615],\n",
       "         [0.49558854],\n",
       "         [0.49435878]],\n",
       " \n",
       "        [[0.49875903],\n",
       "         [0.4987854 ],\n",
       "         [0.49895647],\n",
       "         [0.49929285]]], dtype=float32), array([[-8.01626034e-03,  1.12221884e-02, -1.09821046e-02,\n",
       "          1.00002280e-02,  1.49419587e-02, -1.64342579e-02,\n",
       "          3.10657895e-04,  1.17216762e-02, -1.29043302e-02,\n",
       "          3.59999086e-03,  5.92648098e-03,  1.78904589e-02,\n",
       "         -4.15209495e-03, -6.07073540e-04, -3.29534127e-03,\n",
       "         -1.02270506e-02,  1.23274885e-02, -8.57175887e-03,\n",
       "         -1.04800565e-02, -2.13722186e-03,  9.84835345e-03,\n",
       "         -7.59860640e-03,  2.44302349e-03, -1.80764403e-02,\n",
       "          3.13163642e-03, -2.00371709e-04,  1.34185806e-03,\n",
       "         -1.55993979e-02, -1.14320442e-02, -9.38956905e-03,\n",
       "          1.36713516e-02,  1.22912396e-02, -9.29868984e-05,\n",
       "         -1.52540421e-02,  1.39600290e-02,  1.51203908e-02,\n",
       "         -4.16999590e-03,  1.54279554e-02,  8.01532250e-03,\n",
       "         -1.57441776e-02,  1.55647378e-02,  1.65310763e-02,\n",
       "          1.13943489e-02,  1.23953046e-02, -1.25783999e-02,\n",
       "         -1.70454439e-02,  1.23630220e-03, -6.51669223e-03,\n",
       "         -7.74354069e-03, -5.71676064e-03,  5.21223992e-03,\n",
       "          1.58090070e-02, -8.42252001e-03, -1.48746213e-02,\n",
       "         -9.86615079e-04, -1.86879635e-02, -1.24999546e-02,\n",
       "         -4.91991034e-03, -4.46350593e-03,  7.51572940e-03,\n",
       "         -1.52441384e-02,  1.13930963e-02,  1.41776167e-02,\n",
       "         -9.13986098e-03],\n",
       "        [-3.36133549e-03,  1.23040127e-02, -7.92331621e-03,\n",
       "         -1.29533606e-02,  3.15775024e-03,  6.51427172e-03,\n",
       "          6.66892665e-05,  1.30844787e-02, -3.56249604e-03,\n",
       "          5.06405812e-03,  1.75731368e-02, -1.15959924e-02,\n",
       "          3.83192929e-03, -6.55998662e-03, -1.64296180e-02,\n",
       "          1.08292801e-02,  1.00511620e-02,  1.56626496e-02,\n",
       "          5.07835997e-03,  9.06762260e-04, -4.02761390e-03,\n",
       "          3.41373472e-03,  9.78485798e-04,  1.73723698e-02,\n",
       "          1.09492680e-02,  8.25056341e-03,  1.03767160e-02,\n",
       "         -1.15828635e-02,  1.06214527e-02,  8.80514365e-03,\n",
       "         -7.31924723e-04,  1.18259331e-02, -6.51971111e-03,\n",
       "          3.16869072e-03, -1.56288892e-02, -1.49703510e-02,\n",
       "         -1.52650161e-03, -2.35445984e-03, -1.53683638e-03,\n",
       "         -1.21022100e-02, -5.26147382e-03,  4.17366577e-03,\n",
       "          4.15072124e-03,  5.94996091e-04,  8.59818794e-03,\n",
       "          1.48250945e-02, -1.07788667e-02, -3.85235508e-05,\n",
       "         -1.29903331e-02, -1.15263145e-02,  1.04818167e-02,\n",
       "          1.10036228e-02, -1.73817966e-02,  7.00076576e-03,\n",
       "         -1.70355185e-03,  7.45061180e-03, -1.11785121e-02,\n",
       "          1.40135549e-02, -1.59955006e-02,  2.57069874e-03,\n",
       "          1.23114111e-02,  1.78400669e-02, -6.41814759e-03,\n",
       "          1.90335291e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run([tf.nn.softmax(g_logits), alphas, M_emo],\n",
    "                   feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                              target_ids: [[8, 8, 8], [8, 10, 12]],\n",
    "                              emo_cat: [0, 1]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:36: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=False, infer_batch_size=3,\n",
    "    beam_size=5, max_iter=10, name=\"ECM_infer1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-7.6059017, -7.6064587, -7.6063237, ..., -7.6000853,\n",
       "          -7.601037 , -7.6001244],\n",
       "         [-7.6059017, -7.6064587, -7.6063237, ..., -7.6000853,\n",
       "          -7.601037 , -7.6001244],\n",
       "         [-7.6059017, -7.6064587, -7.6063237, ..., -7.6000853,\n",
       "          -7.601037 , -7.6001244],\n",
       "         [-7.6059017, -7.6064587, -7.6063237, ..., -7.6000853,\n",
       "          -7.601037 , -7.6001244],\n",
       "         [-7.6059017, -7.6064587, -7.6063237, ..., -7.6000853,\n",
       "          -7.601037 , -7.6001244]],\n",
       "\n",
       "        [[-7.6068826, -7.6076126, -7.607885 , ..., -7.5979705,\n",
       "          -7.5986285, -7.598356 ],\n",
       "         [-7.607822 , -7.6077495, -7.609184 , ..., -7.5972395,\n",
       "          -7.5976105, -7.598313 ],\n",
       "         [-7.607469 , -7.608576 , -7.60863  , ..., -7.597279 ,\n",
       "          -7.5986404, -7.5984015],\n",
       "         [-7.6068826, -7.6076126, -7.607885 , ..., -7.5979705,\n",
       "          -7.5986285, -7.598356 ],\n",
       "         [-7.6065984, -7.6079326, -7.608126 , ..., -7.5978994,\n",
       "          -7.598462 , -7.5979457]],\n",
       "\n",
       "        [[-7.608042 , -7.609619 , -7.609924 , ..., -7.5956793,\n",
       "          -7.5964446, -7.597135 ],\n",
       "         [-7.608042 , -7.609619 , -7.609924 , ..., -7.5956793,\n",
       "          -7.5964446, -7.597135 ],\n",
       "         [-7.607267 , -7.6082067, -7.60864  , ..., -7.59656  ,\n",
       "          -7.5964866, -7.5971646],\n",
       "         [-7.6086736, -7.6083136, -7.6106286, ..., -7.595451 ,\n",
       "          -7.595047 , -7.5971494],\n",
       "         [-7.6076245, -7.608828 , -7.609487 , ..., -7.596448 ,\n",
       "          -7.5969195, -7.5978627]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.605783 , -7.6057796, -7.607061 , ..., -7.5978503,\n",
       "          -7.593358 , -7.595265 ],\n",
       "         [-7.605783 , -7.6057796, -7.607061 , ..., -7.5978503,\n",
       "          -7.593358 , -7.595265 ],\n",
       "         [-7.6051583, -7.605792 , -7.6066   , ..., -7.597895 ,\n",
       "          -7.593365 , -7.5954676],\n",
       "         [-7.605998 , -7.605883 , -7.606857 , ..., -7.5971775,\n",
       "          -7.593518 , -7.595427 ],\n",
       "         [-7.6051583, -7.605792 , -7.6066   , ..., -7.597895 ,\n",
       "          -7.593365 , -7.5954676]],\n",
       "\n",
       "        [[-7.60475  , -7.6044345, -7.605808 , ..., -7.599052 ,\n",
       "          -7.594103 , -7.595522 ],\n",
       "         [-7.60475  , -7.6044345, -7.605808 , ..., -7.599052 ,\n",
       "          -7.594103 , -7.595522 ],\n",
       "         [-7.6040325, -7.6041822, -7.6051464, ..., -7.5999417,\n",
       "          -7.5948367, -7.596625 ],\n",
       "         [-7.6040325, -7.6041822, -7.6051464, ..., -7.5999417,\n",
       "          -7.5948367, -7.596625 ],\n",
       "         [-7.604695 , -7.604342 , -7.60574  , ..., -7.5990486,\n",
       "          -7.594113 , -7.595489 ]],\n",
       "\n",
       "        [[-7.6030025, -7.6029444, -7.6042833, ..., -7.6005273,\n",
       "          -7.5952406, -7.596147 ],\n",
       "         [-7.6038394, -7.6030354, -7.604538 , ..., -7.5998178,\n",
       "          -7.5953884, -7.596107 ],\n",
       "         [-7.602532 , -7.6026654, -7.6036634, ..., -7.6008887,\n",
       "          -7.596352 , -7.5974164],\n",
       "         [-7.6029267, -7.6028843, -7.604221 , ..., -7.6005096,\n",
       "          -7.5952425, -7.596114 ],\n",
       "         [-7.6016903, -7.6025686, -7.603398 , ..., -7.6016006,\n",
       "          -7.5962095, -7.597464 ]]],\n",
       "\n",
       "\n",
       "       [[[-7.603318 , -7.6017437, -7.601396 , ..., -7.6039968,\n",
       "          -7.603391 , -7.602845 ],\n",
       "         [-7.603318 , -7.6017437, -7.601396 , ..., -7.6039968,\n",
       "          -7.603391 , -7.602845 ],\n",
       "         [-7.603318 , -7.6017437, -7.601396 , ..., -7.6039968,\n",
       "          -7.603391 , -7.602845 ],\n",
       "         [-7.603318 , -7.6017437, -7.601396 , ..., -7.6039968,\n",
       "          -7.603391 , -7.602845 ],\n",
       "         [-7.603318 , -7.6017437, -7.601396 , ..., -7.6039968,\n",
       "          -7.603391 , -7.602845 ]],\n",
       "\n",
       "        [[-7.602206 , -7.597845 , -7.596846 , ..., -7.6054873,\n",
       "          -7.603993 , -7.6045275],\n",
       "         [-7.602538 , -7.597089 , -7.5971045, ..., -7.605335 ,\n",
       "          -7.6046677, -7.6055317],\n",
       "         [-7.6026516, -7.597555 , -7.597604 , ..., -7.605451 ,\n",
       "          -7.6041822, -7.605359 ],\n",
       "         [-7.6026516, -7.597555 , -7.597604 , ..., -7.605451 ,\n",
       "          -7.6041822, -7.605359 ],\n",
       "         [-7.601907 , -7.5969634, -7.5969057, ..., -7.605749 ,\n",
       "          -7.6041965, -7.604735 ]],\n",
       "\n",
       "        [[-7.601382 , -7.591648 , -7.591531 , ..., -7.6070924,\n",
       "          -7.606882 , -7.609452 ],\n",
       "         [-7.601382 , -7.591648 , -7.591531 , ..., -7.6070924,\n",
       "          -7.606882 , -7.609452 ],\n",
       "         [-7.6011667, -7.5929403, -7.5914173, ..., -7.607067 ,\n",
       "          -7.6056166, -7.607794 ],\n",
       "         [-7.601003 , -7.5926886, -7.5916514, ..., -7.6074557,\n",
       "          -7.605678 , -7.6080394],\n",
       "         [-7.601382 , -7.591648 , -7.591531 , ..., -7.6070924,\n",
       "          -7.606882 , -7.609452 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.60172  , -7.5620623, -7.5665274, ..., -7.606406 ,\n",
       "          -7.6297674, -7.637107 ],\n",
       "         [-7.600611 , -7.5613146, -7.5656333, ..., -7.607438 ,\n",
       "          -7.631097 , -7.638671 ],\n",
       "         [-7.599822 , -7.5608344, -7.5650525, ..., -7.6083364,\n",
       "          -7.6320515, -7.6398973],\n",
       "         [-7.6015387, -7.5619926, -7.567352 , ..., -7.6066084,\n",
       "          -7.6291027, -7.637375 ],\n",
       "         [-7.6012926, -7.5627704, -7.5665245, ..., -7.606385 ,\n",
       "          -7.628887 , -7.6364627]],\n",
       "\n",
       "        [[-7.6011877, -7.5582256, -7.563626 , ..., -7.60596  ,\n",
       "          -7.6344724, -7.6418657],\n",
       "         [-7.6000757, -7.5574737, -7.562738 , ..., -7.6069984,\n",
       "          -7.635806 , -7.6434307],\n",
       "         [-7.6025352, -7.55917  , -7.564744 , ..., -7.6048813,\n",
       "          -7.6328807, -7.640118 ],\n",
       "         [-7.6020174, -7.5595717, -7.5646663, ..., -7.6048555,\n",
       "          -7.6323233, -7.6398587],\n",
       "         [-7.6024065, -7.5591846, -7.5654454, ..., -7.605052 ,\n",
       "          -7.632478 , -7.640564 ]],\n",
       "\n",
       "        [[-7.6005664, -7.555124 , -7.561195 , ..., -7.605487 ,\n",
       "          -7.638764 , -7.646067 ],\n",
       "         [-7.6019173, -7.5560718, -7.5623074, ..., -7.6044006,\n",
       "          -7.637168 , -7.644317 ],\n",
       "         [-7.603422 , -7.557127 , -7.5635643, ..., -7.6033244,\n",
       "          -7.635415 , -7.6425   ],\n",
       "         [-7.6028204, -7.557235 , -7.563407 , ..., -7.603307 ,\n",
       "          -7.635153 , -7.6425686],\n",
       "         [-7.6033473, -7.557207 , -7.5641026, ..., -7.603468 ,\n",
       "          -7.6353006, -7.6430993]]],\n",
       "\n",
       "\n",
       "       [[[-7.605763 , -7.6049232, -7.605115 , ..., -7.6019936,\n",
       "          -7.601941 , -7.5993004],\n",
       "         [-7.605763 , -7.6049232, -7.605115 , ..., -7.6019936,\n",
       "          -7.601941 , -7.5993004],\n",
       "         [-7.605763 , -7.6049232, -7.605115 , ..., -7.6019936,\n",
       "          -7.601941 , -7.5993004],\n",
       "         [-7.605763 , -7.6049232, -7.605115 , ..., -7.6019936,\n",
       "          -7.601941 , -7.5993004],\n",
       "         [-7.605763 , -7.6049232, -7.605115 , ..., -7.6019936,\n",
       "          -7.601941 , -7.5993004]],\n",
       "\n",
       "        [[-7.6078067, -7.6050477, -7.606963 , ..., -7.6006303,\n",
       "          -7.599254 , -7.596298 ],\n",
       "         [-7.6068892, -7.605779 , -7.6059704, ..., -7.6008615,\n",
       "          -7.600089 , -7.5968986],\n",
       "         [-7.6078105, -7.606518 , -7.6069493, ..., -7.6001496,\n",
       "          -7.600095 , -7.5959806],\n",
       "         [-7.607356 , -7.605109 , -7.606761 , ..., -7.600978 ,\n",
       "          -7.6006875, -7.5968843],\n",
       "         [-7.6066647, -7.6060386, -7.605532 , ..., -7.6009426,\n",
       "          -7.6004887, -7.595965 ]],\n",
       "\n",
       "        [[-7.6068983, -7.6059327, -7.6059885, ..., -7.600507 ,\n",
       "          -7.598831 , -7.5964065],\n",
       "         [-7.608382 , -7.6049013, -7.6075616, ..., -7.6000023,\n",
       "          -7.5975933, -7.5953627],\n",
       "         [-7.6083565, -7.607216 , -7.607454 , ..., -7.5992775,\n",
       "          -7.598761 , -7.5948486],\n",
       "         [-7.6078224, -7.606669 , -7.6069784, ..., -7.59978  ,\n",
       "          -7.5988407, -7.595482 ],\n",
       "         [-7.606691 , -7.6064863, -7.60564  , ..., -7.600577 ,\n",
       "          -7.599342 , -7.594808 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.6006165, -7.601955 , -7.604185 , ..., -7.597742 ,\n",
       "          -7.5932364, -7.593775 ],\n",
       "         [-7.600585 , -7.5997314, -7.604381 , ..., -7.5980825,\n",
       "          -7.5924506, -7.5941544],\n",
       "         [-7.6007376, -7.602036 , -7.6043506, ..., -7.597615 ,\n",
       "          -7.5932403, -7.593608 ],\n",
       "         [-7.599148 , -7.599963 , -7.6029825, ..., -7.599132 ,\n",
       "          -7.593297 , -7.595545 ],\n",
       "         [-7.5990763, -7.6005754, -7.6038165, ..., -7.5992484,\n",
       "          -7.5936904, -7.593806 ]],\n",
       "\n",
       "        [[-7.5995216, -7.6010733, -7.6035066, ..., -7.597275 ,\n",
       "          -7.593183 , -7.594302 ],\n",
       "         [-7.5996413, -7.601186 , -7.6036596, ..., -7.597144 ,\n",
       "          -7.593199 , -7.594124 ],\n",
       "         [-7.599431 , -7.599093 , -7.6036563, ..., -7.597511 ,\n",
       "          -7.592515 , -7.5946064],\n",
       "         [-7.5981727, -7.599196 , -7.6024494, ..., -7.598523 ,\n",
       "          -7.5932465, -7.5958905],\n",
       "         [-7.5980606, -7.5997086, -7.603282 , ..., -7.5986524,\n",
       "          -7.593603 , -7.5943537]],\n",
       "\n",
       "        [[-7.598547 , -7.6003585, -7.602886 , ..., -7.5968018,\n",
       "          -7.593283 , -7.5948925],\n",
       "         [-7.5986586, -7.600489 , -7.6030207, ..., -7.5966773,\n",
       "          -7.593309 , -7.5947165],\n",
       "         [-7.598394 , -7.598609 , -7.602986 , ..., -7.5969553,\n",
       "          -7.592728 , -7.5951324],\n",
       "         [-7.597309 , -7.598614 , -7.601957 , ..., -7.5979147,\n",
       "          -7.593362 , -7.596302 ],\n",
       "         [-7.5971518, -7.5990276, -7.602749 , ..., -7.598056 ,\n",
       "          -7.593682 , -7.5949683]]]], dtype=float32), ids=array([[[1162, 1060, 1060, 1258, 1162],\n",
       "        [1017, 1017, 1017, 1017, 1060],\n",
       "        [1017, 1017, 1017, 1465, 1017],\n",
       "        [1017, 1465, 1465, 1017, 1017],\n",
       "        [1465, 1465, 1017, 1465, 1465],\n",
       "        [1465, 1465, 1465, 1465, 1465],\n",
       "        [1465, 1465, 1465, 1465, 1465],\n",
       "        [1465, 1026, 1026, 1465, 1026],\n",
       "        [1026, 1026, 1026, 1026, 1026],\n",
       "        [1026, 1026, 1950, 1950, 1026],\n",
       "        [1950, 1026, 1026, 1950, 1950]],\n",
       "\n",
       "       [[ 756,  565,  720, 1717,  573],\n",
       "        [ 565,  565,  565,  565,  565],\n",
       "        [ 565,  565,  565,  565,  565],\n",
       "        [ 565,  565,  741,  565,  741],\n",
       "        [ 741,  741,  741,  741,  741],\n",
       "        [ 741,  464,  299,  741,  741],\n",
       "        [ 464,  464,  741,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464]],\n",
       "\n",
       "       [[1514, 1939, 1258, 1514, 1231],\n",
       "        [1514, 1514, 1514, 1939, 1514],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810]]], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs,\n",
    "                       feed_dict={\n",
    "                           source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]],\n",
    "                           emo_cat: [1, 2, 3],\n",
    "                       })\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1162, 1060, 1060, 1258, 1162],\n",
       "        [1017, 1017, 1017, 1017, 1060],\n",
       "        [1017, 1017, 1017, 1465, 1017],\n",
       "        [1017, 1465, 1465, 1017, 1017],\n",
       "        [1465, 1465, 1017, 1465, 1465],\n",
       "        [1465, 1465, 1465, 1465, 1465],\n",
       "        [1465, 1465, 1465, 1465, 1465],\n",
       "        [1465, 1026, 1026, 1465, 1026],\n",
       "        [1026, 1026, 1026, 1026, 1026],\n",
       "        [1026, 1026, 1950, 1950, 1026],\n",
       "        [1950, 1026, 1026, 1950, 1950]],\n",
       "\n",
       "       [[ 756,  565,  720, 1717,  573],\n",
       "        [ 565,  565,  565,  565,  565],\n",
       "        [ 565,  565,  565,  565,  565],\n",
       "        [ 565,  565,  741,  565,  741],\n",
       "        [ 741,  741,  741,  741,  741],\n",
       "        [ 741,  464,  299,  741,  741],\n",
       "        [ 464,  464,  741,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464],\n",
       "        [ 464,  464,  464,  464,  464]],\n",
       "\n",
       "       [[1514, 1939, 1258, 1514, 1231],\n",
       "        [1514, 1514, 1514, 1939, 1514],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810],\n",
       "        [1810, 1810, 1810, 1810, 1810]]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECM loss\n",
    "def compute_ECM_loss(source_ids, target_ids, sequence_mask, choice_qs,\n",
    "                     embeddings, enc_num_layers, enc_num_units, enc_cell_type,\n",
    "                     enc_bidir, dec_num_layers, dec_num_units, dec_cell_type,\n",
    "                     state_pass, num_emo, emo_cat, emo_cat_units,\n",
    "                     emo_int_units, infer_batch_size, beam_size=None,\n",
    "                     max_iter=20, attn_num_units=128, l2_regularize=None,\n",
    "                     name=\"ECM\"):\n",
    "    \"\"\"\n",
    "    Creates an ECM model and returns CE loss plus regularization terms.\n",
    "        choice_qs: [batch_size, max_time], true choice btw generic/emo words\n",
    "        emo_cat: [batch_size], emotion categories of each target sequence\n",
    "\n",
    "    Returns\n",
    "        CE: cross entropy, used to compute perplexity\n",
    "        total_loss: objective of the entire model\n",
    "        train_outs: (cell, log_probs, alphas, final_int_mem_states)\n",
    "            alphas - predicted choices\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, \n",
    "            num_emo, emo_cat, emo_cat_units, emo_int_units,\n",
    "            state_pass, infer_batch_size, attn_num_units,\n",
    "            target_ids, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        g_logits, e_logits, alphas, int_M_emo = train_outputs\n",
    "        g_probs = tf.nn.softmax(g_logits) * (1 - alphas)\n",
    "        e_probs = tf.nn.softmax(e_logits) * alphas\n",
    "        train_log_probs = tf.log(g_probs + e_probs)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            alphas = tf.squeeze(alphas, axis=-1)\n",
    "            choice_qs = tf.pad(choice_qs, [[0, 0], [0, 1]], constant_values=0)\n",
    "\n",
    "            # compute losses\n",
    "            g_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=g_logits, labels=final_ids) - tf.log(1 - alphas)\n",
    "\n",
    "            e_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=e_logits, labels=final_ids) - tf.log(alphas)\n",
    "\n",
    "            losses = g_losses * (1 - choice_qs) + e_losses * choice_qs\n",
    "\n",
    "            # alpha and internal memory regularizations\n",
    "            alpha_reg = tf.reduce_mean(choice_qs * -tf.log(alphas))\n",
    "            int_mem_reg = tf.reduce_mean(tf.norm(int_M_emo, axis=1))\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses) + alpha_reg + int_mem_reg\n",
    "\n",
    "            # prepare for perplexity computations\n",
    "            CE = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_log_probs, labels=final_ids)\n",
    "            CE = tf.boolean_mask(CE, sequence_mask)\n",
    "            CE = tf.reduce_sum(CE)\n",
    "\n",
    "            train_outs = (cell, train_log_probs, alphas, int_M_emo)\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_outs, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_outs, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_ECM_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "\n",
    "    num_emo = config[\"decoder\"][\"num_emotions\"]\n",
    "    emo_cat_units = config[\"decoder\"][\"emo_cat_units\"]\n",
    "    emo_int_units = config[\"decoder\"][\"emo_int_units\"]\n",
    "\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            num_emo, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "            beam_size, max_iter, attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_ECM_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    q_filename = train_config[\"train_choice_file\"]\n",
    "    c_filename = train_config[\"train_category_file\"]\n",
    "\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "    dev_q_filename = train_config[\"dev_choice_file\"]\n",
    "    dev_c_filename = train_config[\"dev_category_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, q_filename, c_filename,\n",
    "            s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    "            dev_q_filename, dev_c_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"EmotionChattingMachine\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"ECM\",\n",
    "        \"attn_num_units\": 128,\n",
    "        \"num_emotions\": 4,\n",
    "        \"emo_cat_units\": 32,\n",
    "        \"emo_int_units\": 64,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_category_file\": \"./example/dev_category.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./ECM_prediction.txt\",\n",
    "        \"choice_path\": \"./choice_pred.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_ECM/\",\n",
    "        \"restore_from\": \"./log_ECM/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"train_choice_file\": \"./example/train_choice.txt\",\n",
    "        \"train_category_file\": \"./example/train_category.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"dev_choice_file\": \"./example/dev_choice.txt\",\n",
    "        \"dev_category_file\": \"./example/dev_category.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./ECM_training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./ECM_perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./configs/config_ECM.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./configs/config_ECM.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "choice_qs = tf.placeholder(tf.float32, [None, None], name=\"choice\")\n",
    "emo_cat = tf.placeholder(tf.int32, [None], name=\"emotion_category\")\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " num_emo, emo_cat_units, emo_int_units,\n",
    " infer_batch_size, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_ECM_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "\n",
    "CE, loss, train_outs, infer_outputs = compute_ECM_loss(\n",
    "    source_ids, target_ids, sequence_mask, choice_qs, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    num_emo, emo_cat, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "    beam_size, max_iter, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_ECM/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, q_filename, c_filename, s_max_leng, t_max_leng,\n",
    " dev_s_filename, dev_t_filename, dev_q_filename, dev_c_filename,\n",
    " loss_fig, perp_fig) = get_ECM_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "\n",
    "choice_data = loadfile(q_filename, is_source=False, max_length=t_max_leng)\n",
    "choice_data[choice_data < 0] = 0\n",
    "choice_data = choice_data.astype(np.float32)\n",
    "\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "\n",
    "    dev_choice_data = loadfile(dev_q_filename, is_source=False,\n",
    "                               max_length=t_max_leng)\n",
    "    dev_choice_data[dev_choice_data < 0] = 0\n",
    "    dev_choice_data = dev_choice_data.astype(np.float32)\n",
    "\n",
    "    dev_category_data = pd.read_csv(\n",
    "        dev_c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 7.700670, perp: 1001.271, dev_prep: 1001.070, (0.658 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 20, loss = 6.510849, perp: 380.124, dev_prep: 306.446, (0.128 sec/step)\n",
      "step 40, loss = 6.079188, perp: 271.503, dev_prep: 306.245, (0.090 sec/step)\n",
      "step 60, loss = 5.867760, perp: 228.509, dev_prep: 223.668, (0.080 sec/step)\n",
      "step 80, loss = 5.657935, perp: 199.332, dev_prep: 242.102, (0.140 sec/step)\n",
      "step 100, loss = 5.837060, perp: 242.459, dev_prep: 220.134, (0.123 sec/step)\n",
      "step 120, loss = 5.414120, perp: 163.938, dev_prep: 217.160, (0.139 sec/step)\n",
      "step 140, loss = 5.318721, perp: 161.315, dev_prep: 168.591, (0.139 sec/step)\n",
      "step 160, loss = 5.019207, perp: 126.801, dev_prep: 158.962, (0.133 sec/step)\n",
      "step 180, loss = 5.108153, perp: 140.581, dev_prep: 138.760, (0.127 sec/step)\n",
      "step 200, loss = 5.026722, perp: 123.702, dev_prep: 124.265, (0.106 sec/step)\n",
      "step 220, loss = 4.907303, perp: 112.621, dev_prep: 121.902, (0.123 sec/step)\n",
      "step 240, loss = 4.903839, perp: 112.404, dev_prep: 135.769, (0.090 sec/step)\n",
      "step 260, loss = 4.907052, perp: 113.268, dev_prep: 129.392, (0.147 sec/step)\n",
      "step 280, loss = 4.939095, perp: 117.552, dev_prep: 105.950, (0.095 sec/step)\n",
      "step 300, loss = 4.716976, perp: 95.074, dev_prep: 111.143, (0.139 sec/step)\n",
      "step 320, loss = 4.867764, perp: 111.372, dev_prep: 124.889, (0.126 sec/step)\n",
      "step 340, loss = 4.764027, perp: 97.326, dev_prep: 108.713, (0.123 sec/step)\n",
      "step 360, loss = 4.699242, perp: 93.806, dev_prep: 110.892, (0.115 sec/step)\n",
      "step 380, loss = 4.746591, perp: 89.368, dev_prep: 92.983, (0.094 sec/step)\n",
      "step 400, loss = 4.553209, perp: 77.857, dev_prep: 86.187, (0.089 sec/step)\n",
      "step 420, loss = 4.499788, perp: 76.363, dev_prep: 88.790, (0.129 sec/step)\n",
      "step 440, loss = 4.545366, perp: 78.268, dev_prep: 73.518, (0.111 sec/step)\n",
      "step 460, loss = 4.478376, perp: 72.990, dev_prep: 76.590, (0.095 sec/step)\n",
      "step 480, loss = 4.478021, perp: 75.189, dev_prep: 85.305, (0.131 sec/step)\n",
      "step 500, loss = 4.651644, perp: 87.322, dev_prep: 65.436, (0.098 sec/step)\n",
      "step 520, loss = 4.417486, perp: 67.451, dev_prep: 69.220, (0.094 sec/step)\n",
      "step 540, loss = 4.370657, perp: 64.964, dev_prep: 66.646, (0.149 sec/step)\n",
      "step 560, loss = 4.389948, perp: 68.062, dev_prep: 64.563, (0.110 sec/step)\n",
      "step 580, loss = 4.260469, perp: 59.101, dev_prep: 68.284, (0.100 sec/step)\n",
      "step 600, loss = 4.512103, perp: 73.054, dev_prep: 64.564, (0.102 sec/step)\n",
      "step 620, loss = 4.360809, perp: 65.551, dev_prep: 65.040, (0.099 sec/step)\n",
      "step 640, loss = 4.238336, perp: 57.596, dev_prep: 67.225, (0.127 sec/step)\n",
      "step 660, loss = 4.295647, perp: 59.014, dev_prep: 61.077, (0.086 sec/step)\n",
      "step 680, loss = 4.326480, perp: 63.826, dev_prep: 58.415, (0.117 sec/step)\n",
      "step 700, loss = 4.246092, perp: 59.125, dev_prep: 57.573, (0.093 sec/step)\n",
      "step 720, loss = 4.183239, perp: 55.177, dev_prep: 56.575, (0.120 sec/step)\n",
      "step 740, loss = 4.163648, perp: 55.694, dev_prep: 55.951, (0.124 sec/step)\n",
      "step 760, loss = 4.189764, perp: 55.701, dev_prep: 56.367, (0.104 sec/step)\n",
      "step 780, loss = 4.204843, perp: 56.279, dev_prep: 55.608, (0.119 sec/step)\n",
      "step 800, loss = 4.270286, perp: 60.395, dev_prep: 61.611, (0.119 sec/step)\n",
      "step 820, loss = 4.194889, perp: 53.913, dev_prep: 52.724, (0.141 sec/step)\n",
      "step 840, loss = 4.332945, perp: 62.600, dev_prep: 56.366, (0.117 sec/step)\n",
      "step 860, loss = 4.154366, perp: 53.070, dev_prep: 50.581, (0.123 sec/step)\n",
      "step 880, loss = 4.060356, perp: 47.372, dev_prep: 49.975, (0.100 sec/step)\n",
      "step 900, loss = 4.078088, perp: 50.015, dev_prep: 58.299, (0.084 sec/step)\n",
      "step 920, loss = 4.252027, perp: 58.041, dev_prep: 48.832, (0.094 sec/step)\n",
      "step 940, loss = 4.065180, perp: 48.831, dev_prep: 45.204, (0.100 sec/step)\n",
      "step 960, loss = 4.041907, perp: 49.157, dev_prep: 47.114, (0.122 sec/step)\n",
      "step 980, loss = 4.010862, perp: 45.592, dev_prep: 45.231, (0.131 sec/step)\n",
      "step 1000, loss = 3.977856, perp: 44.818, dev_prep: 46.799, (0.140 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 1020, loss = 3.996239, perp: 45.549, dev_prep: 47.971, (0.137 sec/step)\n",
      "step 1040, loss = 4.010128, perp: 46.316, dev_prep: 52.440, (0.113 sec/step)\n",
      "step 1060, loss = 4.054132, perp: 48.153, dev_prep: 49.442, (0.137 sec/step)\n",
      "step 1080, loss = 4.036100, perp: 46.658, dev_prep: 52.963, (0.125 sec/step)\n",
      "step 1100, loss = 4.001522, perp: 46.139, dev_prep: 46.728, (0.091 sec/step)\n",
      "step 1120, loss = 3.852156, perp: 39.531, dev_prep: 49.031, (0.095 sec/step)\n",
      "step 1140, loss = 3.945788, perp: 42.020, dev_prep: 47.918, (0.130 sec/step)\n",
      "step 1160, loss = 3.987581, perp: 45.379, dev_prep: 40.575, (0.089 sec/step)\n",
      "step 1180, loss = 3.884652, perp: 42.021, dev_prep: 46.406, (0.146 sec/step)\n",
      "step 1200, loss = 3.880031, perp: 40.454, dev_prep: 44.744, (0.090 sec/step)\n",
      "step 1220, loss = 4.019681, perp: 46.688, dev_prep: 48.292, (0.110 sec/step)\n",
      "step 1240, loss = 3.946832, perp: 43.455, dev_prep: 46.227, (0.107 sec/step)\n",
      "step 1260, loss = 3.797172, perp: 38.149, dev_prep: 48.284, (0.119 sec/step)\n",
      "step 1280, loss = 4.029236, perp: 46.878, dev_prep: 43.617, (0.089 sec/step)\n",
      "step 1300, loss = 3.972877, perp: 44.416, dev_prep: 42.790, (0.134 sec/step)\n",
      "step 1320, loss = 3.928155, perp: 42.401, dev_prep: 41.052, (0.135 sec/step)\n",
      "step 1340, loss = 3.809502, perp: 37.664, dev_prep: 50.838, (0.095 sec/step)\n",
      "step 1360, loss = 3.921384, perp: 43.169, dev_prep: 45.898, (0.091 sec/step)\n",
      "step 1380, loss = 3.803062, perp: 37.979, dev_prep: 42.322, (0.111 sec/step)\n",
      "step 1400, loss = 3.861651, perp: 38.728, dev_prep: 39.840, (0.104 sec/step)\n",
      "step 1420, loss = 3.840540, perp: 40.917, dev_prep: 50.652, (0.090 sec/step)\n",
      "step 1440, loss = 3.967776, perp: 44.671, dev_prep: 39.445, (0.132 sec/step)\n",
      "step 1460, loss = 3.914568, perp: 41.874, dev_prep: 46.817, (0.105 sec/step)\n",
      "step 1480, loss = 3.844090, perp: 39.110, dev_prep: 42.334, (0.160 sec/step)\n",
      "step 1500, loss = 3.908954, perp: 42.268, dev_prep: 38.668, (0.100 sec/step)\n",
      "step 1520, loss = 3.915917, perp: 41.114, dev_prep: 36.319, (0.137 sec/step)\n",
      "step 1540, loss = 3.843879, perp: 39.226, dev_prep: 41.738, (0.093 sec/step)\n",
      "step 1560, loss = 3.773823, perp: 38.111, dev_prep: 42.968, (0.141 sec/step)\n",
      "step 1580, loss = 3.703384, perp: 34.515, dev_prep: 39.160, (0.125 sec/step)\n",
      "step 1600, loss = 3.847461, perp: 40.318, dev_prep: 40.323, (0.097 sec/step)\n",
      "step 1620, loss = 3.688896, perp: 34.170, dev_prep: 34.381, (0.097 sec/step)\n",
      "step 1640, loss = 3.740983, perp: 36.098, dev_prep: 43.565, (0.106 sec/step)\n",
      "step 1660, loss = 3.761254, perp: 35.704, dev_prep: 40.147, (0.103 sec/step)\n",
      "step 1680, loss = 3.740394, perp: 35.684, dev_prep: 35.802, (0.117 sec/step)\n",
      "step 1700, loss = 3.799294, perp: 37.632, dev_prep: 42.199, (0.095 sec/step)\n",
      "step 1720, loss = 3.898647, perp: 40.900, dev_prep: 34.748, (0.095 sec/step)\n",
      "step 1740, loss = 3.746971, perp: 36.109, dev_prep: 37.033, (0.098 sec/step)\n",
      "step 1760, loss = 3.806606, perp: 37.755, dev_prep: 34.376, (0.097 sec/step)\n",
      "step 1780, loss = 3.714940, perp: 34.932, dev_prep: 38.728, (0.108 sec/step)\n",
      "step 1800, loss = 3.774744, perp: 36.068, dev_prep: 41.992, (0.093 sec/step)\n",
      "step 1820, loss = 3.757613, perp: 35.185, dev_prep: 41.274, (0.101 sec/step)\n",
      "step 1840, loss = 3.742101, perp: 35.922, dev_prep: 39.836, (0.095 sec/step)\n",
      "step 1860, loss = 3.883266, perp: 39.526, dev_prep: 43.541, (0.129 sec/step)\n",
      "step 1880, loss = 3.727050, perp: 35.497, dev_prep: 43.886, (0.131 sec/step)\n",
      "step 1900, loss = 3.668178, perp: 32.790, dev_prep: 33.164, (0.127 sec/step)\n",
      "step 1920, loss = 3.681294, perp: 33.140, dev_prep: 35.831, (0.099 sec/step)\n",
      "step 1940, loss = 3.669619, perp: 34.305, dev_prep: 36.315, (0.118 sec/step)\n",
      "step 1960, loss = 3.740176, perp: 35.339, dev_prep: 37.572, (0.138 sec/step)\n",
      "step 1980, loss = 3.638255, perp: 31.853, dev_prep: 34.763, (0.113 sec/step)\n",
      "step 2000, loss = 3.542912, perp: 30.078, dev_prep: 41.202, (0.128 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 2020, loss = 3.706564, perp: 33.725, dev_prep: 34.520, (0.093 sec/step)\n",
      "step 2040, loss = 3.604724, perp: 31.303, dev_prep: 40.409, (0.133 sec/step)\n",
      "step 2060, loss = 3.638467, perp: 32.479, dev_prep: 39.602, (0.118 sec/step)\n",
      "step 2080, loss = 3.725497, perp: 34.671, dev_prep: 36.689, (0.118 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.639465, perp: 32.089, dev_prep: 34.929, (0.123 sec/step)\n",
      "step 2120, loss = 3.684243, perp: 34.887, dev_prep: 34.770, (0.140 sec/step)\n",
      "step 2140, loss = 3.679197, perp: 34.196, dev_prep: 32.825, (0.149 sec/step)\n",
      "step 2160, loss = 3.723943, perp: 34.995, dev_prep: 36.500, (0.135 sec/step)\n",
      "step 2180, loss = 3.690879, perp: 33.673, dev_prep: 36.743, (0.101 sec/step)\n",
      "step 2200, loss = 3.772880, perp: 37.802, dev_prep: 32.546, (0.107 sec/step)\n",
      "step 2220, loss = 3.715886, perp: 34.101, dev_prep: 37.460, (0.127 sec/step)\n",
      "step 2240, loss = 3.786971, perp: 36.352, dev_prep: 34.180, (0.110 sec/step)\n",
      "step 2260, loss = 3.610036, perp: 31.061, dev_prep: 30.867, (0.144 sec/step)\n",
      "step 2280, loss = 3.570675, perp: 30.317, dev_prep: 34.674, (0.086 sec/step)\n",
      "step 2300, loss = 3.685348, perp: 33.590, dev_prep: 38.133, (0.124 sec/step)\n",
      "step 2320, loss = 3.556677, perp: 29.362, dev_prep: 34.674, (0.083 sec/step)\n",
      "step 2340, loss = 3.699535, perp: 34.718, dev_prep: 36.185, (0.124 sec/step)\n",
      "step 2360, loss = 3.614943, perp: 30.588, dev_prep: 34.054, (0.136 sec/step)\n",
      "step 2380, loss = 3.760128, perp: 35.886, dev_prep: 36.482, (0.110 sec/step)\n",
      "step 2400, loss = 3.685834, perp: 34.064, dev_prep: 38.018, (0.088 sec/step)\n",
      "step 2420, loss = 3.571941, perp: 30.822, dev_prep: 33.296, (0.105 sec/step)\n",
      "step 2440, loss = 3.484390, perp: 27.806, dev_prep: 32.838, (0.095 sec/step)\n",
      "step 2460, loss = 3.666005, perp: 33.030, dev_prep: 34.217, (0.119 sec/step)\n",
      "step 2480, loss = 3.605551, perp: 30.540, dev_prep: 33.661, (0.095 sec/step)\n",
      "step 2500, loss = 3.592831, perp: 30.669, dev_prep: 32.578, (0.088 sec/step)\n",
      "step 2520, loss = 3.590244, perp: 30.407, dev_prep: 33.336, (0.089 sec/step)\n",
      "step 2540, loss = 3.592870, perp: 30.305, dev_prep: 32.884, (0.080 sec/step)\n",
      "step 2560, loss = 3.510519, perp: 28.448, dev_prep: 32.284, (0.088 sec/step)\n",
      "step 2580, loss = 3.494968, perp: 28.498, dev_prep: 34.370, (0.116 sec/step)\n",
      "step 2600, loss = 3.610686, perp: 31.540, dev_prep: 31.052, (0.113 sec/step)\n",
      "step 2620, loss = 3.490267, perp: 28.237, dev_prep: 34.347, (0.141 sec/step)\n",
      "step 2640, loss = 3.370222, perp: 25.565, dev_prep: 29.688, (0.096 sec/step)\n",
      "step 2660, loss = 3.599157, perp: 32.284, dev_prep: 33.044, (0.096 sec/step)\n",
      "step 2680, loss = 3.635149, perp: 32.045, dev_prep: 28.298, (0.090 sec/step)\n",
      "step 2700, loss = 3.670991, perp: 32.861, dev_prep: 30.569, (0.101 sec/step)\n",
      "step 2720, loss = 3.458349, perp: 27.291, dev_prep: 35.690, (0.113 sec/step)\n",
      "step 2740, loss = 3.541266, perp: 29.946, dev_prep: 35.282, (0.118 sec/step)\n",
      "step 2760, loss = 3.525003, perp: 28.712, dev_prep: 32.372, (0.109 sec/step)\n",
      "step 2780, loss = 3.367553, perp: 25.114, dev_prep: 29.979, (0.136 sec/step)\n",
      "step 2800, loss = 3.481367, perp: 28.629, dev_prep: 30.932, (0.097 sec/step)\n",
      "step 2820, loss = 3.509697, perp: 27.854, dev_prep: 29.447, (0.109 sec/step)\n",
      "step 2840, loss = 3.333449, perp: 24.218, dev_prep: 29.891, (0.118 sec/step)\n",
      "step 2860, loss = 3.558254, perp: 30.599, dev_prep: 32.717, (0.109 sec/step)\n",
      "step 2880, loss = 3.335250, perp: 24.292, dev_prep: 28.717, (0.094 sec/step)\n",
      "step 2900, loss = 3.487896, perp: 28.140, dev_prep: 29.925, (0.119 sec/step)\n",
      "step 2920, loss = 3.659651, perp: 32.806, dev_prep: 28.339, (0.099 sec/step)\n",
      "step 2940, loss = 3.336536, perp: 25.067, dev_prep: 31.744, (0.098 sec/step)\n",
      "step 2960, loss = 3.473471, perp: 28.239, dev_prep: 31.624, (0.127 sec/step)\n",
      "step 2980, loss = 3.446995, perp: 26.877, dev_prep: 27.251, (0.127 sec/step)\n",
      "step 3000, loss = 3.581240, perp: 30.453, dev_prep: 31.169, (0.088 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 3020, loss = 3.481192, perp: 27.827, dev_prep: 31.128, (0.117 sec/step)\n",
      "step 3040, loss = 3.421912, perp: 26.239, dev_prep: 30.664, (0.091 sec/step)\n",
      "step 3060, loss = 3.458746, perp: 26.647, dev_prep: 28.760, (0.147 sec/step)\n",
      "step 3080, loss = 3.412182, perp: 26.338, dev_prep: 28.587, (0.143 sec/step)\n",
      "step 3100, loss = 3.484543, perp: 27.363, dev_prep: 30.972, (0.099 sec/step)\n",
      "step 3120, loss = 3.425763, perp: 26.476, dev_prep: 28.655, (0.119 sec/step)\n",
      "step 3140, loss = 3.417479, perp: 26.416, dev_prep: 31.295, (0.136 sec/step)\n",
      "step 3160, loss = 3.330462, perp: 24.104, dev_prep: 30.269, (0.086 sec/step)\n",
      "step 3180, loss = 3.519699, perp: 28.928, dev_prep: 27.364, (0.094 sec/step)\n",
      "step 3200, loss = 3.629338, perp: 33.238, dev_prep: 30.143, (0.090 sec/step)\n",
      "step 3220, loss = 3.397453, perp: 25.803, dev_prep: 27.855, (0.129 sec/step)\n",
      "step 3240, loss = 3.499567, perp: 28.106, dev_prep: 28.050, (0.117 sec/step)\n",
      "step 3260, loss = 3.375687, perp: 25.434, dev_prep: 28.125, (0.121 sec/step)\n",
      "step 3280, loss = 3.560862, perp: 29.713, dev_prep: 28.121, (0.118 sec/step)\n",
      "step 3300, loss = 3.417230, perp: 27.250, dev_prep: 30.069, (0.094 sec/step)\n",
      "step 3320, loss = 3.413440, perp: 25.949, dev_prep: 26.259, (0.093 sec/step)\n",
      "step 3340, loss = 3.423357, perp: 26.562, dev_prep: 28.817, (0.131 sec/step)\n",
      "step 3360, loss = 3.324520, perp: 24.429, dev_prep: 28.545, (0.092 sec/step)\n",
      "step 3380, loss = 3.390099, perp: 25.683, dev_prep: 24.866, (0.108 sec/step)\n",
      "step 3400, loss = 3.319919, perp: 23.366, dev_prep: 22.667, (0.093 sec/step)\n",
      "step 3420, loss = 3.334659, perp: 24.205, dev_prep: 27.959, (0.098 sec/step)\n",
      "step 3440, loss = 3.338674, perp: 24.481, dev_prep: 31.164, (0.096 sec/step)\n",
      "step 3460, loss = 3.357614, perp: 24.568, dev_prep: 25.109, (0.087 sec/step)\n",
      "step 3480, loss = 3.458616, perp: 27.171, dev_prep: 26.616, (0.115 sec/step)\n",
      "step 3500, loss = 3.346805, perp: 23.952, dev_prep: 24.783, (0.096 sec/step)\n",
      "step 3520, loss = 3.356653, perp: 25.107, dev_prep: 28.950, (0.132 sec/step)\n",
      "step 3540, loss = 3.269007, perp: 22.338, dev_prep: 28.279, (0.087 sec/step)\n",
      "step 3560, loss = 3.333067, perp: 23.524, dev_prep: 25.082, (0.133 sec/step)\n",
      "step 3580, loss = 3.308337, perp: 24.113, dev_prep: 25.419, (0.089 sec/step)\n",
      "step 3600, loss = 3.340855, perp: 24.179, dev_prep: 25.398, (0.125 sec/step)\n",
      "step 3620, loss = 3.359217, perp: 24.474, dev_prep: 26.697, (0.116 sec/step)\n",
      "step 3640, loss = 3.330789, perp: 23.896, dev_prep: 24.716, (0.118 sec/step)\n",
      "step 3660, loss = 3.350430, perp: 24.317, dev_prep: 26.041, (0.079 sec/step)\n",
      "step 3680, loss = 3.281829, perp: 23.798, dev_prep: 25.731, (0.131 sec/step)\n",
      "step 3700, loss = 3.392359, perp: 25.910, dev_prep: 26.305, (0.097 sec/step)\n",
      "step 3720, loss = 3.281337, perp: 23.486, dev_prep: 26.072, (0.136 sec/step)\n",
      "step 3740, loss = 3.309843, perp: 23.790, dev_prep: 24.729, (0.118 sec/step)\n",
      "step 3760, loss = 3.285712, perp: 23.503, dev_prep: 23.764, (0.094 sec/step)\n",
      "step 3780, loss = 3.259549, perp: 23.155, dev_prep: 26.507, (0.133 sec/step)\n",
      "step 3800, loss = 3.276200, perp: 22.494, dev_prep: 23.691, (0.106 sec/step)\n",
      "step 3820, loss = 3.234561, perp: 21.523, dev_prep: 22.779, (0.118 sec/step)\n",
      "step 3840, loss = 3.229685, perp: 22.416, dev_prep: 24.504, (0.100 sec/step)\n",
      "step 3860, loss = 3.059490, perp: 18.917, dev_prep: 22.261, (0.119 sec/step)\n",
      "step 3880, loss = 3.288173, perp: 22.965, dev_prep: 23.254, (0.136 sec/step)\n",
      "step 3900, loss = 3.233777, perp: 21.727, dev_prep: 25.745, (0.111 sec/step)\n",
      "step 3920, loss = 3.170384, perp: 20.956, dev_prep: 22.557, (0.092 sec/step)\n",
      "step 3940, loss = 3.221474, perp: 22.126, dev_prep: 24.740, (0.086 sec/step)\n",
      "step 3960, loss = 3.274879, perp: 21.709, dev_prep: 21.343, (0.117 sec/step)\n",
      "step 3980, loss = 3.187343, perp: 21.566, dev_prep: 24.405, (0.102 sec/step)\n",
      "step 4000, loss = 3.177162, perp: 20.483, dev_prep: 23.356, (0.097 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 4020, loss = 3.156126, perp: 21.545, dev_prep: 22.759, (0.141 sec/step)\n",
      "step 4040, loss = 3.130275, perp: 20.260, dev_prep: 22.097, (0.117 sec/step)\n",
      "step 4060, loss = 3.128453, perp: 19.831, dev_prep: 22.598, (0.125 sec/step)\n",
      "step 4080, loss = 3.065589, perp: 18.847, dev_prep: 20.822, (0.123 sec/step)\n",
      "step 4100, loss = 3.182532, perp: 20.862, dev_prep: 21.288, (0.121 sec/step)\n",
      "step 4120, loss = 3.222062, perp: 22.034, dev_prep: 20.750, (0.106 sec/step)\n",
      "step 4140, loss = 3.150206, perp: 20.358, dev_prep: 20.762, (0.117 sec/step)\n",
      "step 4160, loss = 3.267124, perp: 22.509, dev_prep: 21.233, (0.127 sec/step)\n",
      "step 4180, loss = 3.189341, perp: 21.713, dev_prep: 20.956, (0.085 sec/step)\n",
      "step 4200, loss = 3.136992, perp: 20.136, dev_prep: 19.159, (0.107 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.146264, perp: 19.742, dev_prep: 21.025, (0.088 sec/step)\n",
      "step 4240, loss = 3.145083, perp: 20.668, dev_prep: 21.443, (0.117 sec/step)\n",
      "step 4260, loss = 3.143301, perp: 20.541, dev_prep: 20.647, (0.133 sec/step)\n",
      "step 4280, loss = 3.083209, perp: 19.568, dev_prep: 20.620, (0.122 sec/step)\n",
      "step 4300, loss = 3.051638, perp: 18.926, dev_prep: 19.501, (0.137 sec/step)\n",
      "step 4320, loss = 3.204196, perp: 21.398, dev_prep: 21.396, (0.108 sec/step)\n",
      "step 4340, loss = 3.192544, perp: 20.887, dev_prep: 23.315, (0.099 sec/step)\n",
      "step 4360, loss = 3.045411, perp: 18.838, dev_prep: 21.915, (0.138 sec/step)\n",
      "step 4380, loss = 3.031381, perp: 18.221, dev_prep: 21.020, (0.112 sec/step)\n",
      "step 4400, loss = 3.139168, perp: 20.574, dev_prep: 21.305, (0.096 sec/step)\n",
      "step 4420, loss = 3.119521, perp: 19.537, dev_prep: 21.267, (0.118 sec/step)\n",
      "step 4440, loss = 3.196044, perp: 21.615, dev_prep: 22.488, (0.095 sec/step)\n",
      "step 4460, loss = 3.052405, perp: 18.757, dev_prep: 20.386, (0.116 sec/step)\n",
      "step 4480, loss = 2.991321, perp: 17.755, dev_prep: 19.391, (0.101 sec/step)\n",
      "step 4500, loss = 3.019916, perp: 18.679, dev_prep: 20.507, (0.105 sec/step)\n",
      "step 4520, loss = 2.927445, perp: 16.595, dev_prep: 19.307, (0.125 sec/step)\n",
      "step 4540, loss = 2.915842, perp: 16.395, dev_prep: 18.328, (0.124 sec/step)\n",
      "step 4560, loss = 2.946956, perp: 17.262, dev_prep: 17.098, (0.114 sec/step)\n",
      "step 4580, loss = 3.056771, perp: 18.566, dev_prep: 19.067, (0.110 sec/step)\n",
      "step 4600, loss = 2.966289, perp: 17.665, dev_prep: 19.469, (0.121 sec/step)\n",
      "step 4620, loss = 3.102298, perp: 19.678, dev_prep: 17.453, (0.103 sec/step)\n",
      "step 4640, loss = 3.055977, perp: 18.922, dev_prep: 21.297, (0.112 sec/step)\n",
      "step 4660, loss = 2.817453, perp: 15.056, dev_prep: 20.385, (0.132 sec/step)\n",
      "step 4680, loss = 3.021293, perp: 17.626, dev_prep: 18.374, (0.122 sec/step)\n",
      "step 4700, loss = 2.882570, perp: 16.331, dev_prep: 19.068, (0.122 sec/step)\n",
      "step 4720, loss = 2.974250, perp: 17.240, dev_prep: 17.881, (0.098 sec/step)\n",
      "step 4740, loss = 2.929656, perp: 16.511, dev_prep: 17.140, (0.137 sec/step)\n",
      "step 4760, loss = 2.864676, perp: 15.984, dev_prep: 17.996, (0.133 sec/step)\n",
      "step 4780, loss = 2.994014, perp: 18.355, dev_prep: 20.310, (0.142 sec/step)\n",
      "step 4800, loss = 2.944957, perp: 17.346, dev_prep: 19.617, (0.134 sec/step)\n",
      "step 4820, loss = 2.905684, perp: 16.359, dev_prep: 20.617, (0.092 sec/step)\n",
      "step 4840, loss = 2.781699, perp: 14.841, dev_prep: 18.554, (0.129 sec/step)\n",
      "step 4860, loss = 2.987610, perp: 17.753, dev_prep: 19.053, (0.105 sec/step)\n",
      "step 4880, loss = 2.855437, perp: 15.833, dev_prep: 19.756, (0.130 sec/step)\n",
      "step 4900, loss = 2.867962, perp: 15.440, dev_prep: 17.924, (0.097 sec/step)\n",
      "step 4920, loss = 2.886724, perp: 16.109, dev_prep: 18.214, (0.085 sec/step)\n",
      "step 4940, loss = 2.743944, perp: 14.370, dev_prep: 18.049, (0.141 sec/step)\n",
      "step 4960, loss = 2.917329, perp: 16.626, dev_prep: 16.645, (0.106 sec/step)\n",
      "step 4980, loss = 2.889117, perp: 16.324, dev_prep: 17.040, (0.092 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        choice_batch = choice_data[rand_indexes]\n",
    "        emotions = category_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            choice_qs: choice_batch,\n",
    "            emo_cat: emotions,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    choice_qs: dev_choice_data[dev_inds],\n",
    "                    emo_cat: dev_category_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Total loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"Perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check trained ECM model: internal memory and external choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norms of final-state internal memory:\n",
      " [5.8109624e-08 6.1483412e-08 7.9233459e-08 5.3402613e-07 4.6361686e-07\n",
      " 5.4265971e-08 4.7717037e-08 2.7363188e-07 6.7214211e-07 5.1064379e-08\n",
      " 8.9976645e-08 7.6299948e-08 1.7532869e-07 1.4895031e-07 7.3128547e-08\n",
      " 1.4425767e-07 4.8578030e-08 5.1575235e-08 2.2557742e-07 4.9845660e-08\n",
      " 6.6184064e-08 3.9477396e-07 1.2796507e-07 1.3549224e-07 1.6832453e-07\n",
      " 2.8010202e-08 7.3157963e-08 5.7981747e-08 6.5442620e-08 1.6821761e-07\n",
      " 1.5140505e-07 4.9132261e-08 1.8629159e-07 1.1630866e-07 1.5553020e-07\n",
      " 7.6273487e-08 2.6091179e-08 5.7132593e-07 3.3482319e-08 8.4353623e-08\n",
      " 4.2639886e-07 7.7525989e-08 4.4324727e-07 2.7616656e-07 1.7940015e-07\n",
      " 1.8837622e-07 6.2030495e-08 1.0902692e-06 8.8501380e-08 1.0921155e-07\n",
      " 1.2323272e-07 1.3059471e-07 3.9571793e-07 1.7413058e-07 9.7357535e-08\n",
      " 1.1273612e-07 2.2440955e-07 5.5027623e-08 6.9544164e-08 3.6238859e-08\n",
      " 6.1655030e-08 1.1997091e-07 6.8521935e-08 1.6011938e-07]\n"
     ]
    }
   ],
   "source": [
    "(cell, train_log_probs, alphas, int_M_emo) = train_outs\n",
    "\n",
    "M_norms = sess.run(tf.norm(int_M_emo, axis=1), feed_dict)\n",
    "print(\"Norms of final-state internal memory:\\n\", M_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x265.846 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x276.48 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alphas (predicted choice) and true choices\n",
    "rand_indexes = np.random.choice(n_data, 6)\n",
    "source_batch = source_data[rand_indexes]\n",
    "target_batch = target_data[rand_indexes]\n",
    "emotions = category_data[rand_indexes]\n",
    "\n",
    "choice_preds = sess.run(alphas,\n",
    "                        feed_dict={\n",
    "                            source_ids: source_batch,\n",
    "                            target_ids: target_batch,\n",
    "                            emo_cat: emotions})\n",
    "choice_batch = choice_data[rand_indexes]\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(choice_preds)\n",
    "plt.title(\"Predicted Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.savefig(\"./predict_choice\")\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(choice_batch)\n",
    "plt.title(\"True Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.savefig(\"./true_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "c_filename = config[\"inference\"][\"infer_category_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "category_data = np.concatenate((category_data, np.zeros(n_pad)))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "    batch_cat = category_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs,\n",
    "                      feed_dict={source_ids: batch, emo_cat: batch_cat})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "choice_pred = (final_result >= vocab_size).astype(np.int)\n",
    "final_result[final_result >= vocab_size] -= (vocab_size + embed_shift)\n",
    "\n",
    "# transform to output format\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = (final_result.astype(int)).astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "choice_pred = choice_pred.astype(str).tolist()\n",
    "choice_pred = list(map(lambda t: \" \".join(t), choice_pred))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "\n",
    "cdf = pd.DataFrame(data={\"0\": choice_pred})\n",
    "cdf.to_csv(config[\"inference\"][\"choice_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.184753006729878"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "q = dev_choice_data[random_indexes]\n",
    "c = dev_category_data[random_indexes]\n",
    "\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    choice_qs: q,\n",
    "    emo_cat: c,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
