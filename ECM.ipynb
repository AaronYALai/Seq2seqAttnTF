{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting\n",
    "#### Prime numbers as simulated emotion words and emotion categories:\n",
    "- ECM should use external memory when predicting primes\n",
    "- Primes are equally split into (num_emo) \"emotion\" categories\n",
    "- sequence with most primes from a certain category is tagged with that \"emotion\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a number is a prime\n",
    "def is_prime(n):\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(3, int(np.sqrt(n)) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "num_emo = 4\n",
    "\n",
    "nums = np.arange(N)\n",
    "check_prime = np.vectorize(is_prime)\n",
    "primes = nums[check_prime(nums)]\n",
    "\n",
    "# equally split primes into categories\n",
    "s_primes = np.array_split(primes, num_emo)\n",
    "s_primes = [s_p.tolist() for s_p in s_primes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "source_data = []\n",
    "target_data = []\n",
    "choice_data = []\n",
    "category_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    # 1: emotion words/primes, 0: generic words\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "qdf = pd.DataFrame(data={\"0\": choice_data})\n",
    "cdf = pd.DataFrame(data={\"0\": category_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)\n",
    "qdf.to_csv(\"./example/train_choice.txt\", header=None, index=None)\n",
    "cdf.to_csv(\"./example/train_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_choice_data = []\n",
    "dev_category_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    dev_choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    dev_category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "qddf = pd.DataFrame(data={\"0\": dev_choice_data})\n",
    "cddf = pd.DataFrame(data={\"0\": dev_category_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)\n",
    "qddf.to_csv(\"./example/dev_choice.txt\", header=None, index=None)\n",
    "cddf.to_csv(\"./example/dev_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECM wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "ECMState = collections.namedtuple(\n",
    "    \"ECMState\", (\"cell_states\", \"h\", \"context\", \"internal_memory\"))\n",
    "\n",
    "\n",
    "class ECMWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Emotion Chatting Machine: H. Zhou, et al. AAAI 2018\n",
    "    (https://arxiv.org/abs/1704.01074)\n",
    "    Emotion Category Embedding, Internal and External Memory Modules\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "        emo_cat_embs: category embeddings, [batch_size, emo_cat_units]\n",
    "        emo_cat: emotion category, [batch_size]\n",
    "        emo_int_units: dimension of internal emotion memory\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype, emo_cat_embs, emo_cat, num_emo,\n",
    "                 emo_int_units, emo_init=None):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = ECMState(self._cell.state_size,\n",
    "                                    num_units, memory.shape[-1].value,\n",
    "                                    emo_int_units)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "        # ECM hyperparameters\n",
    "        self._emo_cat_embs = emo_cat_embs\n",
    "        self._emo_cat = emo_cat\n",
    "        self._emo_int_units = emo_int_units\n",
    "\n",
    "        # internal memory\n",
    "        if emo_init is None:\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.int_memory = tf.Variable(\n",
    "            initializer(shape=(num_emo, emo_int_units)),\n",
    "            name=\"emo_memory\", dtype=dtype)\n",
    "\n",
    "        self.read_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_read_gate\")\n",
    "        self.write_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_write_gate\")\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for ECM wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        Returns:\n",
    "            h_0: [batch_size, num_units]\n",
    "            context_0: [batch_size, num_units]\n",
    "            M_emo_0: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "        M_emo_0 = tf.gather(self.int_memory, self._emo_cat)\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        ecm_state_0 = ECMState(cell_states, h_0, context_0, M_emo_0)\n",
    "\n",
    "        return ecm_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def _read_internal_memory(self, M_emo, read_inputs):\n",
    "        \"\"\"\n",
    "        Read the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            read_inputs: [batch_size, d]\n",
    "        Returns:\n",
    "            M_read: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_read = tf.nn.sigmoid(self.read_g(read_inputs))\n",
    "        return (M_emo * gate_read)\n",
    "\n",
    "    def _write_internal_memory(self, M_emo, new_h):\n",
    "        \"\"\"\n",
    "        Write the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            new_h: [batch_size, num_units]\n",
    "        Returns:\n",
    "            M_write: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_write = tf.nn.sigmoid(self.write_g(new_h))\n",
    "        return (M_emo * gate_write)\n",
    "\n",
    "    def __call__(self, inputs, ecm_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs, context, int_memory)\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context, M_emo = ecm_states\n",
    "\n",
    "        # read internal memory\n",
    "        read_inputs = tf.concat([inputs, h, context], axis=-1)\n",
    "        M_read = self._read_internal_memory(M_emo, read_inputs)\n",
    "\n",
    "        # pass into RNN_cell to get the output\n",
    "        x = [inputs, h, context, self._emo_cat_embs, M_read]\n",
    "        x = tf.concat(x, axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        # update states\n",
    "        new_M_emo = self._write_internal_memory(M_emo, new_h)\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_ecm_states = ECMState(cell_states, new_h, new_context, new_M_emo)\n",
    "\n",
    "        return (new_h, new_ecm_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ECMBeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states, output_layer,\n",
    "                 emo_output_layer, emo_choice_layer, batch_size, dtype,\n",
    "                 beam_size, vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size * 2\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "\n",
    "        self._output_layer = output_layer\n",
    "        self._emo_output_layer = emo_output_layer\n",
    "        self._emo_choice_layer = emo_choice_layer\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "\n",
    "        indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat_embs\"):\n",
    "            self._cell._emo_cat_embs = tf.gather(\n",
    "                self._cell._emo_cat_embs, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat\"):\n",
    "            self._emo_cat = tf.gather(self._cell._emo_cat, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new log probs\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        gen_log_probs = tf.nn.log_softmax(self._output_layer(new_h))\n",
    "        emo_log_probs = tf.nn.log_softmax(self._emo_output_layer(new_h))\n",
    "        alphas = tf.nn.sigmoid(self._emo_choice_layer(new_h))\n",
    "\n",
    "        gen_log_probs = gen_log_probs + tf.log(1 - alphas)\n",
    "        emo_log_probs = emo_log_probs + tf.log(alphas)\n",
    "        raw_log_probs = tf.concat([gen_log_probs, emo_log_probs], axis=-1)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        raw_log_probs = split_batch_beam(raw_log_probs, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: mask log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = mask_log_probs(\n",
    "            raw_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=raw_log_probs, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_ECM_decoder(encoder_outputs, encoder_states, embeddings, num_layers,\n",
    "                      num_units, cell_type, num_emo, emo_cat, emo_cat_units,\n",
    "                      emo_int_units, state_pass=True, infer_batch_size=None,\n",
    "                      attn_num_units=128, target_ids=None, beam_size=None,\n",
    "                      max_iter=20, dtype=tf.float32, forget_bias=1.0,\n",
    "                      name=\"ECM_decoder\"):\n",
    "    \"\"\"\n",
    "    ECM decoder: build ECM decoder with emotion category embedding,\n",
    "             internal & external memory modules\n",
    "        target_ids: [batch_size, max_time]\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "        num_emo: number of emotions\n",
    "        emo_cat: emotion catogories, [batch_size]\n",
    "        emo_cat_units: dimension of emotion category embeddings\n",
    "        emo_int_units: dimension of emotion internal memory\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: (generic_logits, emo_ext_logits, alphas, int_M_emo),\n",
    "            first 3 shape: [batch_size, max_time, d]\n",
    "            int_M_emo: [batch_size, emo_int_units]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # create emotion category embeddings\n",
    "        emo_init = tf.contrib.layers.xavier_initializer()\n",
    "        emo_cat_embeddings = tf.Variable(\n",
    "            emo_init(shape=(num_emo, emo_cat_units)),\n",
    "            name=\"emo_cat_embeddings\", dtype=dtype)\n",
    "        emo_cat_embs = tf.nn.embedding_lookup(emo_cat_embeddings, emo_cat)\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with ECM internal memory module\n",
    "        memory = encoder_outputs\n",
    "\n",
    "        cell = ECMWrapper(\n",
    "            cell, memory, dec_init_states, attn_num_units, num_units, dtype,\n",
    "            emo_cat_embs, emo_cat, num_emo, emo_int_units)\n",
    "\n",
    "        dec_init_states = cell.initial_state()\n",
    "\n",
    "        # ECM external memory module\n",
    "        emo_output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"emo_output_projection\")\n",
    "\n",
    "        emo_choice_layer = tf.layers.Dense(\n",
    "            1, use_bias=False, name=\"emo_choice_alpha\")\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits & final internal memory states\n",
    "            generic_logits = output_layer(decoder_outputs)\n",
    "            emo_ext_logits = emo_output_layer(decoder_outputs)\n",
    "            alphas = tf.nn.sigmoid(emo_choice_layer(decoder_outputs))\n",
    "            int_M_emo = decoder_states.internal_memory\n",
    "\n",
    "            train_outputs = (generic_logits, emo_ext_logits, alphas, int_M_emo)\n",
    "\n",
    "        # Decode - for inference, beam search\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            decoder_cell = ECMBeamSearchDecodeCell(\n",
    "                embeddings, cell, dec_init_states, output_layer,\n",
    "                emo_output_layer, emo_choice_layer,\n",
    "                infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                div_gamma=None, div_prob=None)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return cell, train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "emo_cat = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=True, target_ids=target_ids, name=\"ECM_decoder2\")\n",
    "\n",
    "g_logits, e_logits, alphas, M_emo = train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.00099399, 0.00099546, 0.00099786, ..., 0.00099384,\n",
       "          0.000994  , 0.00099873],\n",
       "         [0.00099065, 0.00099451, 0.00099731, ..., 0.00099369,\n",
       "          0.00099464, 0.00099683],\n",
       "         [0.00098688, 0.00099397, 0.00099648, ..., 0.00099368,\n",
       "          0.00099548, 0.00099467],\n",
       "         [0.00098334, 0.00099339, 0.00099542, ..., 0.00099401,\n",
       "          0.00099619, 0.00099235]],\n",
       " \n",
       "        [[0.00099706, 0.0009959 , 0.00099653, ..., 0.00099612,\n",
       "          0.00099592, 0.00099827],\n",
       "         [0.00099807, 0.00099614, 0.00099769, ..., 0.00099422,\n",
       "          0.00099654, 0.00099987],\n",
       "         [0.00099867, 0.00099688, 0.00099974, ..., 0.00099267,\n",
       "          0.00099715, 0.00100134],\n",
       "         [0.0009994 , 0.00099712, 0.00100092, ..., 0.00099163,\n",
       "          0.00099744, 0.00100259]]], dtype=float32), array([[[0.5015328 ],\n",
       "         [0.5011277 ],\n",
       "         [0.5006759 ],\n",
       "         [0.5000834 ]],\n",
       " \n",
       "        [[0.5007482 ],\n",
       "         [0.50288886],\n",
       "         [0.50523216],\n",
       "         [0.5068375 ]]], dtype=float32), array([[-0.00482778, -0.00556182, -0.00051656, -0.00348223, -0.00346302,\n",
       "         -0.01082243, -0.01050973,  0.00294916, -0.00464845,  0.00045529,\n",
       "         -0.01163754, -0.0104087 ,  0.00913185, -0.00989099, -0.00777735,\n",
       "          0.00435058, -0.00026215, -0.0141704 ,  0.00195486, -0.01071593,\n",
       "         -0.00868296,  0.00624515, -0.00378135,  0.01711328, -0.01420602,\n",
       "         -0.0013119 ,  0.01074399, -0.0055574 ,  0.01566078, -0.00747189,\n",
       "         -0.01612257, -0.01308708,  0.01038219, -0.00015131, -0.00106234,\n",
       "          0.00171621, -0.01189629,  0.00973332, -0.00140155, -0.00052629,\n",
       "         -0.01612815, -0.01434046,  0.01723634,  0.0122022 , -0.00752131,\n",
       "          0.01239709, -0.00745224, -0.01544834, -0.01267921, -0.00623262,\n",
       "          0.01376059, -0.00291654, -0.00304901,  0.01806   ,  0.01218476,\n",
       "         -0.01668602, -0.01154741,  0.01558455,  0.0024552 , -0.01800625,\n",
       "         -0.01125726, -0.00516599,  0.01756166,  0.01469353],\n",
       "        [ 0.01421653, -0.01637184, -0.00472644,  0.01551259,  0.00640357,\n",
       "          0.00453187, -0.01340037,  0.00899578,  0.00644279,  0.00843152,\n",
       "         -0.01671316,  0.01098815, -0.00531146, -0.01033317,  0.00898493,\n",
       "          0.01752951,  0.01873824, -0.01251782, -0.00603412, -0.01768029,\n",
       "          0.01119542, -0.00132135, -0.01212956,  0.01807098,  0.01422457,\n",
       "         -0.01135983, -0.00163081, -0.00067829,  0.01005738,  0.00025126,\n",
       "          0.00504123,  0.01659106,  0.00345564,  0.01750771, -0.01618576,\n",
       "          0.00194736,  0.01627106,  0.00336132,  0.00282885,  0.00639864,\n",
       "          0.00654683,  0.01479169, -0.00035303,  0.0146396 ,  0.00729637,\n",
       "          0.01223991, -0.01323811, -0.00180184,  0.00862206,  0.01160448,\n",
       "         -0.01047036, -0.00597078, -0.01236159,  0.00605742,  0.00680356,\n",
       "         -0.00499547, -0.00289125,  0.01298013,  0.01732486, -0.00503212,\n",
       "         -0.01349294,  0.01058454,  0.00724691, -0.00829033]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run([tf.nn.softmax(g_logits), alphas, M_emo],\n",
    "                   feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                              target_ids: [[8, 8, 8], [8, 10, 12]],\n",
    "                              emo_cat: [0, 1]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=False, infer_batch_size=3,\n",
    "    beam_size=5, max_iter=10, name=\"ECM_infer1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-7.6018524, -7.6028605, -7.6030807, ..., -7.605753 ,\n",
       "          -7.606496 , -7.606839 ],\n",
       "         [-7.6018524, -7.6028605, -7.6030807, ..., -7.605753 ,\n",
       "          -7.606496 , -7.606839 ],\n",
       "         [-7.6018524, -7.6028605, -7.6030807, ..., -7.605753 ,\n",
       "          -7.606496 , -7.606839 ],\n",
       "         [-7.6018524, -7.6028605, -7.6030807, ..., -7.605753 ,\n",
       "          -7.606496 , -7.606839 ],\n",
       "         [-7.6018524, -7.6028605, -7.6030807, ..., -7.605753 ,\n",
       "          -7.606496 , -7.606839 ]],\n",
       "\n",
       "        [[-7.599648 , -7.603358 , -7.603022 , ..., -7.606    ,\n",
       "          -7.608891 , -7.6084824],\n",
       "         [-7.601156 , -7.6044245, -7.6030293, ..., -7.605966 ,\n",
       "          -7.608283 , -7.60747  ],\n",
       "         [-7.599648 , -7.603358 , -7.603022 , ..., -7.606    ,\n",
       "          -7.608891 , -7.6084824],\n",
       "         [-7.601156 , -7.6044245, -7.6030293, ..., -7.605966 ,\n",
       "          -7.608283 , -7.60747  ],\n",
       "         [-7.599648 , -7.603358 , -7.603022 , ..., -7.606    ,\n",
       "          -7.608891 , -7.6084824]],\n",
       "\n",
       "        [[-7.598748 , -7.604988 , -7.602693 , ..., -7.6059055,\n",
       "          -7.610922 , -7.608736 ],\n",
       "         [-7.5974593, -7.604369 , -7.603366 , ..., -7.60567  ,\n",
       "          -7.610808 , -7.609503 ],\n",
       "         [-7.597856 , -7.6056366, -7.603203 , ..., -7.6064806,\n",
       "          -7.6105137, -7.6091833],\n",
       "         [-7.5995803, -7.6055484, -7.604205 , ..., -7.6064286,\n",
       "          -7.608053 , -7.6079364],\n",
       "         [-7.5989666, -7.6054397, -7.6033845, ..., -7.60564  ,\n",
       "          -7.610201 , -7.6084886]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.597724 , -7.614297 , -7.6101084, ..., -7.604039 ,\n",
       "          -7.608073 , -7.6068835],\n",
       "         [-7.597724 , -7.614297 , -7.6101084, ..., -7.604039 ,\n",
       "          -7.608073 , -7.6068835],\n",
       "         [-7.597724 , -7.614297 , -7.6101084, ..., -7.604039 ,\n",
       "          -7.608073 , -7.6068835],\n",
       "         [-7.597724 , -7.614297 , -7.6101084, ..., -7.604039 ,\n",
       "          -7.608073 , -7.6068835],\n",
       "         [-7.597724 , -7.614297 , -7.6101084, ..., -7.604039 ,\n",
       "          -7.608073 , -7.6068835]],\n",
       "\n",
       "        [[-7.5989914, -7.61607  , -7.6116514, ..., -7.603136 ,\n",
       "          -7.6066747, -7.605525 ],\n",
       "         [-7.5989914, -7.61607  , -7.6116514, ..., -7.603136 ,\n",
       "          -7.6066747, -7.605525 ],\n",
       "         [-7.5989914, -7.61607  , -7.6116514, ..., -7.603136 ,\n",
       "          -7.6066747, -7.605525 ],\n",
       "         [-7.5989914, -7.61607  , -7.6116514, ..., -7.603136 ,\n",
       "          -7.6066747, -7.605525 ],\n",
       "         [-7.5989914, -7.61607  , -7.6116514, ..., -7.603136 ,\n",
       "          -7.6066747, -7.605525 ]],\n",
       "\n",
       "        [[-7.600375 , -7.6177177, -7.613106 , ..., -7.602183 ,\n",
       "          -7.60533  , -7.6041374],\n",
       "         [-7.600375 , -7.6177177, -7.613106 , ..., -7.602183 ,\n",
       "          -7.60533  , -7.6041374],\n",
       "         [-7.600375 , -7.6177177, -7.613106 , ..., -7.602183 ,\n",
       "          -7.60533  , -7.6041374],\n",
       "         [-7.600375 , -7.6177177, -7.613106 , ..., -7.602183 ,\n",
       "          -7.60533  , -7.6041374],\n",
       "         [-7.600375 , -7.6177177, -7.613106 , ..., -7.602183 ,\n",
       "          -7.60533  , -7.6041374]]],\n",
       "\n",
       "\n",
       "       [[[-7.603432 , -7.6028194, -7.602816 , ..., -7.6061735,\n",
       "          -7.6043854, -7.606062 ],\n",
       "         [-7.603432 , -7.6028194, -7.602816 , ..., -7.6061735,\n",
       "          -7.6043854, -7.606062 ],\n",
       "         [-7.603432 , -7.6028194, -7.602816 , ..., -7.6061735,\n",
       "          -7.6043854, -7.606062 ],\n",
       "         [-7.603432 , -7.6028194, -7.602816 , ..., -7.6061735,\n",
       "          -7.6043854, -7.606062 ],\n",
       "         [-7.603432 , -7.6028194, -7.602816 , ..., -7.6061735,\n",
       "          -7.6043854, -7.606062 ]],\n",
       "\n",
       "        [[-7.603281 , -7.602205 , -7.5998726, ..., -7.6087503,\n",
       "          -7.604823 , -7.606937 ],\n",
       "         [-7.603281 , -7.602205 , -7.5998726, ..., -7.6087503,\n",
       "          -7.604823 , -7.606937 ],\n",
       "         [-7.603262 , -7.60345  , -7.600775 , ..., -7.6080804,\n",
       "          -7.604063 , -7.60665  ],\n",
       "         [-7.603281 , -7.602205 , -7.5998726, ..., -7.6087503,\n",
       "          -7.604823 , -7.606937 ],\n",
       "         [-7.603281 , -7.602205 , -7.5998726, ..., -7.6087503,\n",
       "          -7.604823 , -7.606937 ]],\n",
       "\n",
       "        [[-7.6027107, -7.601941 , -7.5957775, ..., -7.6112933,\n",
       "          -7.605046 , -7.6079955],\n",
       "         [-7.6034155, -7.603034 , -7.59717  , ..., -7.609533 ,\n",
       "          -7.6042013, -7.6065893],\n",
       "         [-7.6027107, -7.601941 , -7.5957775, ..., -7.6112933,\n",
       "          -7.605046 , -7.6079955],\n",
       "         [-7.6034155, -7.603034 , -7.59717  , ..., -7.609533 ,\n",
       "          -7.6042013, -7.6065893],\n",
       "         [-7.604076 , -7.6028385, -7.59684  , ..., -7.6104865,\n",
       "          -7.603755 , -7.606371 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.5963097, -7.59606  , -7.571065 , ..., -7.62148  ,\n",
       "          -7.5994043, -7.6107874],\n",
       "         [-7.5963097, -7.59606  , -7.571065 , ..., -7.62148  ,\n",
       "          -7.5994043, -7.6107874],\n",
       "         [-7.596862 , -7.5962787, -7.5712156, ..., -7.6213794,\n",
       "          -7.5992026, -7.6106105],\n",
       "         [-7.5979004, -7.5969934, -7.571458 , ..., -7.6215525,\n",
       "          -7.598709 , -7.61013  ],\n",
       "         [-7.596862 , -7.5962787, -7.5712156, ..., -7.6213794,\n",
       "          -7.5992026, -7.6106105]],\n",
       "\n",
       "        [[-7.5951414, -7.595437 , -7.56812  , ..., -7.6225047,\n",
       "          -7.5981026, -7.6112676],\n",
       "         [-7.5951414, -7.595437 , -7.56812  , ..., -7.6225047,\n",
       "          -7.5981026, -7.6112676],\n",
       "         [-7.5961814, -7.5961576, -7.5683694, ..., -7.622675 ,\n",
       "          -7.5976048, -7.610788 ],\n",
       "         [-7.5935426, -7.5944867, -7.5677176, ..., -7.622441 ,\n",
       "          -7.5988116, -7.611929 ],\n",
       "         [-7.594588 , -7.595217 , -7.5679684, ..., -7.6226077,\n",
       "          -7.598306 , -7.611445 ]],\n",
       "\n",
       "        [[-7.591802 , -7.5935006, -7.5649915, ..., -7.623284 ,\n",
       "          -7.5976834, -7.6112466],\n",
       "         [-7.5928426, -7.594236 , -7.565246 , ..., -7.623455 ,\n",
       "          -7.597177 , -7.610769 ],\n",
       "         [-7.593398 , -7.594454 , -7.565396 , ..., -7.623351 ,\n",
       "          -7.596974 , -7.610594 ],\n",
       "         [-7.59357  , -7.5943785, -7.56545  , ..., -7.6232615,\n",
       "          -7.597038 , -7.610684 ],\n",
       "         [-7.594433 , -7.5951786, -7.565649 , ..., -7.6235247,\n",
       "          -7.596474 , -7.610119 ]]],\n",
       "\n",
       "\n",
       "       [[[-7.600202 , -7.6025977, -7.601756 , ..., -7.605217 ,\n",
       "          -7.60671  , -7.60471  ],\n",
       "         [-7.600202 , -7.6025977, -7.601756 , ..., -7.605217 ,\n",
       "          -7.60671  , -7.60471  ],\n",
       "         [-7.600202 , -7.6025977, -7.601756 , ..., -7.605217 ,\n",
       "          -7.60671  , -7.60471  ],\n",
       "         [-7.600202 , -7.6025977, -7.601756 , ..., -7.605217 ,\n",
       "          -7.60671  , -7.60471  ],\n",
       "         [-7.600202 , -7.6025977, -7.601756 , ..., -7.605217 ,\n",
       "          -7.60671  , -7.60471  ]],\n",
       "\n",
       "        [[-7.5968766, -7.603381 , -7.599846 , ..., -7.604848 ,\n",
       "          -7.6080823, -7.6031814],\n",
       "         [-7.5960655, -7.60328  , -7.599096 , ..., -7.604624 ,\n",
       "          -7.609239 , -7.603987 ],\n",
       "         [-7.5947537, -7.601978 , -7.5983315, ..., -7.6056924,\n",
       "          -7.610148 , -7.603779 ],\n",
       "         [-7.596735 , -7.6035624, -7.6002407, ..., -7.6044726,\n",
       "          -7.6086383, -7.6029806],\n",
       "         [-7.596186 , -7.6026344, -7.598737 , ..., -7.6050487,\n",
       "          -7.609447 , -7.603589 ]],\n",
       "\n",
       "        [[-7.5921702, -7.604879 , -7.596463 , ..., -7.6026945,\n",
       "          -7.610633 , -7.6024036],\n",
       "         [-7.59034  , -7.602888 , -7.5953255, ..., -7.6043334,\n",
       "          -7.6120787, -7.602254 ],\n",
       "         [-7.5921702, -7.604879 , -7.596463 , ..., -7.6026945,\n",
       "          -7.610633 , -7.6024036],\n",
       "         [-7.5933223, -7.60495  , -7.5974045, ..., -7.603029 ,\n",
       "          -7.6090083, -7.6014423],\n",
       "         [-7.5921702, -7.604879 , -7.596463 , ..., -7.6026945,\n",
       "          -7.610633 , -7.6024036]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.5786543, -7.6094933, -7.5889406, ..., -7.5877414,\n",
       "          -7.6030173, -7.5883307],\n",
       "         [-7.5786543, -7.6094933, -7.5889406, ..., -7.5877414,\n",
       "          -7.6030173, -7.5883307],\n",
       "         [-7.5808225, -7.611016 , -7.5906568, ..., -7.5857167,\n",
       "          -7.6021013, -7.586937 ],\n",
       "         [-7.5805645, -7.6114407, -7.588009 , ..., -7.587249 ,\n",
       "          -7.6024218, -7.588826 ],\n",
       "         [-7.57826  , -7.6096992, -7.5871577, ..., -7.5888543,\n",
       "          -7.603587 , -7.5895104]],\n",
       "\n",
       "        [[-7.578708 , -7.6099095, -7.589305 , ..., -7.585235 ,\n",
       "          -7.6005335, -7.585954 ],\n",
       "         [-7.578708 , -7.6099095, -7.589305 , ..., -7.585235 ,\n",
       "          -7.6005335, -7.585954 ],\n",
       "         [-7.580076 , -7.612057 , -7.5903234, ..., -7.583503 ,\n",
       "          -7.598923 , -7.585773 ],\n",
       "         [-7.580076 , -7.612057 , -7.5903234, ..., -7.583503 ,\n",
       "          -7.598923 , -7.585773 ],\n",
       "         [-7.578708 , -7.6099095, -7.589305 , ..., -7.585235 ,\n",
       "          -7.6005335, -7.585954 ]],\n",
       "\n",
       "        [[-7.5788765, -7.6101427, -7.589526 , ..., -7.5830855,\n",
       "          -7.598232 , -7.58386  ],\n",
       "         [-7.5777316, -7.60983  , -7.589158 , ..., -7.5832644,\n",
       "          -7.597376 , -7.5847483],\n",
       "         [-7.5785317, -7.610615 , -7.5867996, ..., -7.5846987,\n",
       "          -7.598712 , -7.585702 ],\n",
       "         [-7.5772305, -7.609845 , -7.589183 , ..., -7.583305 ,\n",
       "          -7.596836 , -7.585055 ],\n",
       "         [-7.578463 , -7.6105537, -7.58701  , ..., -7.5846157,\n",
       "          -7.5988355, -7.5855002]]]], dtype=float32), ids=array([[[ 868,  212,  678,  868,  641],\n",
       "        [ 868,  868,  868,  868,  868],\n",
       "        [ 868,  384,  868,  225,  384],\n",
       "        [ 384,  384,  384,  384,  384],\n",
       "        [ 384,  384,  384,  384,  384],\n",
       "        [1707, 1333, 1386, 1707,  384],\n",
       "        [1707, 1707, 1707, 1386, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1386],\n",
       "        [1707, 1707, 1707, 1707, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1707]],\n",
       "\n",
       "       [[ 400,  400,  342,  400,  400],\n",
       "        [ 400,  400,  342,  400,  400],\n",
       "        [ 400,  848,  400,  848,  279],\n",
       "        [ 848,  848,  848,  344,  848],\n",
       "        [ 848,  848,  344,  848,  344],\n",
       "        [ 848,  344,  848,  848,  344],\n",
       "        [ 344,  848,  344,   37,  848],\n",
       "        [ 344,  344,   37,   37,   37],\n",
       "        [  37,   37,  344,   37,  344],\n",
       "        [  37,   37,  344,   37,   37],\n",
       "        [  37,  344,   37,   37,   37]],\n",
       "\n",
       "       [[ 202,  173,  906,  825,  306],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  339,  173,   92,   38],\n",
       "        [ 339,  339,  339,  339,  339],\n",
       "        [ 339,  339,  339,  339,  339],\n",
       "        [ 339,  339,  339,   92,  339],\n",
       "        [ 339,  339,  339,  272,   92],\n",
       "        [ 339,  339,  272,  272,  339],\n",
       "        [ 339,  272,  339,  272, 1633]]], dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs,\n",
    "                       feed_dict={\n",
    "                           source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]],\n",
    "                           emo_cat: [1, 2, 3],\n",
    "                       })\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 868,  212,  678,  868,  641],\n",
       "        [ 868,  868,  868,  868,  868],\n",
       "        [ 868,  384,  868,  225,  384],\n",
       "        [ 384,  384,  384,  384,  384],\n",
       "        [ 384,  384,  384,  384,  384],\n",
       "        [1707, 1333, 1386, 1707,  384],\n",
       "        [1707, 1707, 1707, 1386, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1386],\n",
       "        [1707, 1707, 1707, 1707, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1707],\n",
       "        [1707, 1707, 1707, 1707, 1707]],\n",
       "\n",
       "       [[ 400,  400,  342,  400,  400],\n",
       "        [ 400,  400,  342,  400,  400],\n",
       "        [ 400,  848,  400,  848,  279],\n",
       "        [ 848,  848,  848,  344,  848],\n",
       "        [ 848,  848,  344,  848,  344],\n",
       "        [ 848,  344,  848,  848,  344],\n",
       "        [ 344,  848,  344,   37,  848],\n",
       "        [ 344,  344,   37,   37,   37],\n",
       "        [  37,   37,  344,   37,  344],\n",
       "        [  37,   37,  344,   37,   37],\n",
       "        [  37,  344,   37,   37,   37]],\n",
       "\n",
       "       [[ 202,  173,  906,  825,  306],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  202,  202,  202,  202],\n",
       "        [ 202,  339,  173,   92,   38],\n",
       "        [ 339,  339,  339,  339,  339],\n",
       "        [ 339,  339,  339,  339,  339],\n",
       "        [ 339,  339,  339,   92,  339],\n",
       "        [ 339,  339,  339,  272,   92],\n",
       "        [ 339,  339,  272,  272,  339],\n",
       "        [ 339,  272,  339,  272, 1633]]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECM loss\n",
    "def compute_ECM_loss(source_ids, target_ids, sequence_mask, choice_qs,\n",
    "                     embeddings, enc_num_layers, enc_num_units, enc_cell_type,\n",
    "                     enc_bidir, dec_num_layers, dec_num_units, dec_cell_type,\n",
    "                     state_pass, num_emo, emo_cat, emo_cat_units,\n",
    "                     emo_int_units, infer_batch_size, beam_size=None,\n",
    "                     max_iter=20, attn_num_units=128, l2_regularize=None,\n",
    "                     name=\"ECM\"):\n",
    "    \"\"\"\n",
    "    Creates an ECM model and returns CE loss plus regularization terms.\n",
    "        choice_qs: [batch_size, max_time], true choice btw generic/emo words\n",
    "        emo_cat: [batch_size], emotion categories of each target sequence\n",
    "\n",
    "    Returns\n",
    "        CE: cross entropy, used to compute perplexity\n",
    "        total_loss: objective of the entire model\n",
    "        train_outs: (cell, log_probs, alphas, final_int_mem_states)\n",
    "            alphas - predicted choices\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, \n",
    "            num_emo, emo_cat, emo_cat_units, emo_int_units,\n",
    "            state_pass, infer_batch_size, attn_num_units,\n",
    "            target_ids, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        g_logits, e_logits, alphas, int_M_emo = train_outputs\n",
    "        g_probs = tf.nn.softmax(g_logits) * (1 - alphas)\n",
    "        e_probs = tf.nn.softmax(e_logits) * alphas\n",
    "        train_log_probs = tf.log(g_probs + e_probs)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            alphas = tf.squeeze(alphas, axis=-1)\n",
    "            choice_qs = tf.pad(choice_qs, [[0, 0], [0, 1]], constant_values=0)\n",
    "\n",
    "            # compute losses\n",
    "            g_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=g_logits, labels=final_ids) - tf.log(1 - alphas)\n",
    "\n",
    "            e_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=e_logits, labels=final_ids) - tf.log(alphas)\n",
    "\n",
    "            losses = g_losses * (1 - choice_qs) + e_losses * choice_qs\n",
    "\n",
    "            # alpha and internal memory regularizations\n",
    "            alpha_reg = tf.reduce_mean(choice_qs * -tf.log(alphas))\n",
    "            int_mem_reg = tf.reduce_mean(tf.norm(int_M_emo, axis=1))\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses) + alpha_reg + int_mem_reg\n",
    "\n",
    "            # prepare for perplexity computations\n",
    "            CE = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_log_probs, labels=final_ids)\n",
    "            CE = tf.boolean_mask(CE, sequence_mask)\n",
    "            CE = tf.reduce_sum(CE)\n",
    "\n",
    "            train_outs = (cell, train_log_probs, alphas, int_M_emo)\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_outs, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_outs, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_ECM_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "\n",
    "    num_emo = config[\"decoder\"][\"num_emotions\"]\n",
    "    emo_cat_units = config[\"decoder\"][\"emo_cat_units\"]\n",
    "    emo_int_units = config[\"decoder\"][\"emo_int_units\"]\n",
    "\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            num_emo, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "            beam_size, max_iter, attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_ECM_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    q_filename = train_config[\"train_choice_file\"]\n",
    "    c_filename = train_config[\"train_category_file\"]\n",
    "\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "    dev_q_filename = train_config[\"dev_choice_file\"]\n",
    "    dev_c_filename = train_config[\"dev_category_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, q_filename, c_filename,\n",
    "            s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    "            dev_q_filename, dev_c_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"EmotionChattingMachine\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"ECM\",\n",
    "        \"attn_num_units\": 128,\n",
    "        \"num_emotions\": 4,\n",
    "        \"emo_cat_units\": 32,\n",
    "        \"emo_int_units\": 64,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_category_file\": \"./example/dev_category.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./ECM_prediction.txt\",\n",
    "        \"choice_path\": \"./choice_pred.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_ECM/\",\n",
    "        \"restore_from\": \"./log_ECM/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"train_choice_file\": \"./example/train_choice.txt\",\n",
    "        \"train_category_file\": \"./example/train_category.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"dev_choice_file\": \"./example/dev_choice.txt\",\n",
    "        \"dev_category_file\": \"./example/dev_category.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./ECM_training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./ECM_perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./configs/config_ECM.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./configs/config_ECM.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "choice_qs = tf.placeholder(tf.float32, [None, None], name=\"choice\")\n",
    "emo_cat = tf.placeholder(tf.int32, [None], name=\"emotion_category\")\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " num_emo, emo_cat_units, emo_int_units,\n",
    " infer_batch_size, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_ECM_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "\n",
    "CE, loss, train_outs, infer_outputs = compute_ECM_loss(\n",
    "    source_ids, target_ids, sequence_mask, choice_qs, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    num_emo, emo_cat, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "    beam_size, max_iter, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_ECM/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, q_filename, c_filename, s_max_leng, t_max_leng,\n",
    " dev_s_filename, dev_t_filename, dev_q_filename, dev_c_filename,\n",
    " loss_fig, perp_fig) = get_ECM_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "\n",
    "choice_data = loadfile(q_filename, is_source=False, max_length=t_max_leng)\n",
    "choice_data[choice_data < 0] = 0\n",
    "choice_data = choice_data.astype(np.float32)\n",
    "\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "\n",
    "    dev_choice_data = loadfile(dev_q_filename, is_source=False,\n",
    "                               max_length=t_max_leng)\n",
    "    dev_choice_data[dev_choice_data < 0] = 0\n",
    "    dev_choice_data = dev_choice_data.astype(np.float32)\n",
    "\n",
    "    dev_category_data = pd.read_csv(\n",
    "        dev_c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 7.693036, perp: 999.844, dev_prep: 1000.707, (0.676 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 20, loss = 6.247147, perp: 298.565, dev_prep: 334.609, (0.150 sec/step)\n",
      "step 40, loss = 6.054335, perp: 260.068, dev_prep: 248.217, (0.148 sec/step)\n",
      "step 60, loss = 5.859473, perp: 202.918, dev_prep: 223.490, (0.143 sec/step)\n",
      "step 80, loss = 5.631475, perp: 190.229, dev_prep: 200.477, (0.144 sec/step)\n",
      "step 100, loss = 5.328067, perp: 149.617, dev_prep: 151.992, (0.150 sec/step)\n",
      "step 120, loss = 5.185548, perp: 140.637, dev_prep: 113.964, (0.143 sec/step)\n",
      "step 140, loss = 5.147771, perp: 139.504, dev_prep: 151.875, (0.139 sec/step)\n",
      "step 160, loss = 5.023625, perp: 124.517, dev_prep: 118.710, (0.134 sec/step)\n",
      "step 180, loss = 4.963584, perp: 118.829, dev_prep: 103.906, (0.132 sec/step)\n",
      "step 200, loss = 5.023022, perp: 125.363, dev_prep: 127.903, (0.133 sec/step)\n",
      "step 220, loss = 4.902790, perp: 113.835, dev_prep: 148.045, (0.133 sec/step)\n",
      "step 240, loss = 4.825352, perp: 103.530, dev_prep: 126.138, (0.129 sec/step)\n",
      "step 260, loss = 4.840503, perp: 105.664, dev_prep: 110.558, (0.141 sec/step)\n",
      "step 280, loss = 4.840953, perp: 107.351, dev_prep: 109.588, (0.142 sec/step)\n",
      "step 300, loss = 4.933694, perp: 115.772, dev_prep: 129.965, (0.143 sec/step)\n",
      "step 320, loss = 4.837057, perp: 106.098, dev_prep: 117.628, (0.135 sec/step)\n",
      "step 340, loss = 4.968909, perp: 121.182, dev_prep: 139.531, (0.137 sec/step)\n",
      "step 360, loss = 4.636096, perp: 87.990, dev_prep: 118.680, (0.135 sec/step)\n",
      "step 380, loss = 4.906698, perp: 111.095, dev_prep: 106.437, (0.144 sec/step)\n",
      "step 400, loss = 4.661081, perp: 86.886, dev_prep: 100.177, (0.143 sec/step)\n",
      "step 420, loss = 4.693584, perp: 93.895, dev_prep: 82.508, (0.140 sec/step)\n",
      "step 440, loss = 4.584332, perp: 80.499, dev_prep: 75.226, (0.136 sec/step)\n",
      "step 460, loss = 4.578738, perp: 81.999, dev_prep: 72.999, (0.126 sec/step)\n",
      "step 480, loss = 4.577440, perp: 82.756, dev_prep: 77.154, (0.148 sec/step)\n",
      "step 500, loss = 4.489685, perp: 74.300, dev_prep: 89.950, (0.140 sec/step)\n",
      "step 520, loss = 4.443069, perp: 72.068, dev_prep: 61.667, (0.136 sec/step)\n",
      "step 540, loss = 4.536876, perp: 79.380, dev_prep: 76.694, (0.139 sec/step)\n",
      "step 560, loss = 4.531623, perp: 79.687, dev_prep: 79.182, (0.131 sec/step)\n",
      "step 580, loss = 4.478134, perp: 73.340, dev_prep: 64.689, (0.139 sec/step)\n",
      "step 600, loss = 4.207220, perp: 56.728, dev_prep: 62.668, (0.148 sec/step)\n",
      "step 620, loss = 4.330258, perp: 62.145, dev_prep: 66.921, (0.145 sec/step)\n",
      "step 640, loss = 4.402120, perp: 68.884, dev_prep: 69.325, (0.135 sec/step)\n",
      "step 660, loss = 4.366829, perp: 67.224, dev_prep: 63.748, (0.136 sec/step)\n",
      "step 680, loss = 4.250539, perp: 58.438, dev_prep: 58.688, (0.134 sec/step)\n",
      "step 700, loss = 4.319872, perp: 62.883, dev_prep: 58.544, (0.150 sec/step)\n",
      "step 720, loss = 4.254609, perp: 58.205, dev_prep: 59.977, (0.133 sec/step)\n",
      "step 740, loss = 4.170723, perp: 55.016, dev_prep: 60.600, (0.132 sec/step)\n",
      "step 760, loss = 4.271898, perp: 59.588, dev_prep: 58.129, (0.148 sec/step)\n",
      "step 780, loss = 4.367036, perp: 63.774, dev_prep: 58.001, (0.139 sec/step)\n",
      "step 800, loss = 4.157362, perp: 54.620, dev_prep: 58.444, (0.146 sec/step)\n",
      "step 820, loss = 4.183827, perp: 55.469, dev_prep: 60.934, (0.139 sec/step)\n",
      "step 840, loss = 4.182504, perp: 54.324, dev_prep: 53.059, (0.149 sec/step)\n",
      "step 860, loss = 4.148138, perp: 53.206, dev_prep: 53.517, (0.135 sec/step)\n",
      "step 880, loss = 4.099466, perp: 49.410, dev_prep: 63.061, (0.143 sec/step)\n",
      "step 900, loss = 4.114885, perp: 50.855, dev_prep: 52.860, (0.148 sec/step)\n",
      "step 920, loss = 4.030072, perp: 48.366, dev_prep: 52.624, (0.140 sec/step)\n",
      "step 940, loss = 4.139988, perp: 52.806, dev_prep: 47.776, (0.142 sec/step)\n",
      "step 960, loss = 3.998370, perp: 46.810, dev_prep: 52.127, (0.134 sec/step)\n",
      "step 980, loss = 4.100028, perp: 51.943, dev_prep: 52.646, (0.144 sec/step)\n",
      "step 1000, loss = 4.066130, perp: 47.362, dev_prep: 52.795, (0.139 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 1020, loss = 4.081690, perp: 49.083, dev_prep: 47.460, (0.145 sec/step)\n",
      "step 1040, loss = 3.947360, perp: 43.697, dev_prep: 49.931, (0.135 sec/step)\n",
      "step 1060, loss = 4.113507, perp: 51.531, dev_prep: 54.379, (0.137 sec/step)\n",
      "step 1080, loss = 3.936963, perp: 42.931, dev_prep: 52.960, (0.129 sec/step)\n",
      "step 1100, loss = 3.922741, perp: 42.714, dev_prep: 49.694, (0.141 sec/step)\n",
      "step 1120, loss = 4.089124, perp: 50.290, dev_prep: 49.507, (0.145 sec/step)\n",
      "step 1140, loss = 4.020492, perp: 47.704, dev_prep: 46.595, (0.148 sec/step)\n",
      "step 1160, loss = 3.953074, perp: 44.429, dev_prep: 40.493, (0.138 sec/step)\n",
      "step 1180, loss = 3.986614, perp: 44.610, dev_prep: 50.941, (0.152 sec/step)\n",
      "step 1200, loss = 4.019073, perp: 47.877, dev_prep: 45.663, (0.148 sec/step)\n",
      "step 1220, loss = 3.865851, perp: 39.946, dev_prep: 45.121, (0.136 sec/step)\n",
      "step 1240, loss = 4.024813, perp: 47.702, dev_prep: 43.012, (0.140 sec/step)\n",
      "step 1260, loss = 3.925910, perp: 42.988, dev_prep: 44.180, (0.135 sec/step)\n",
      "step 1280, loss = 4.050066, perp: 49.039, dev_prep: 46.250, (0.132 sec/step)\n",
      "step 1300, loss = 3.837679, perp: 39.929, dev_prep: 42.290, (0.141 sec/step)\n",
      "step 1320, loss = 3.859846, perp: 39.108, dev_prep: 40.719, (0.139 sec/step)\n",
      "step 1340, loss = 3.899998, perp: 41.640, dev_prep: 38.150, (0.132 sec/step)\n",
      "step 1360, loss = 3.845027, perp: 39.536, dev_prep: 49.476, (0.145 sec/step)\n",
      "step 1380, loss = 3.894860, perp: 42.549, dev_prep: 41.661, (0.144 sec/step)\n",
      "step 1400, loss = 3.718722, perp: 34.536, dev_prep: 40.624, (0.131 sec/step)\n",
      "step 1420, loss = 3.843411, perp: 39.171, dev_prep: 41.483, (0.143 sec/step)\n",
      "step 1440, loss = 3.769488, perp: 36.252, dev_prep: 46.350, (0.139 sec/step)\n",
      "step 1460, loss = 3.839235, perp: 39.416, dev_prep: 43.836, (0.129 sec/step)\n",
      "step 1480, loss = 3.946941, perp: 44.127, dev_prep: 38.141, (0.134 sec/step)\n",
      "step 1500, loss = 3.767490, perp: 36.733, dev_prep: 41.051, (0.149 sec/step)\n",
      "step 1520, loss = 4.040667, perp: 47.797, dev_prep: 41.592, (0.147 sec/step)\n",
      "step 1540, loss = 3.791584, perp: 37.050, dev_prep: 37.851, (0.144 sec/step)\n",
      "step 1560, loss = 3.703519, perp: 35.651, dev_prep: 36.515, (0.144 sec/step)\n",
      "step 1580, loss = 3.811441, perp: 37.821, dev_prep: 39.333, (0.140 sec/step)\n",
      "step 1600, loss = 3.774035, perp: 38.280, dev_prep: 38.513, (0.129 sec/step)\n",
      "step 1620, loss = 3.705273, perp: 34.190, dev_prep: 45.283, (0.153 sec/step)\n",
      "step 1640, loss = 3.616256, perp: 32.052, dev_prep: 39.417, (0.137 sec/step)\n",
      "step 1660, loss = 3.675678, perp: 33.266, dev_prep: 40.034, (0.132 sec/step)\n",
      "step 1680, loss = 3.662812, perp: 33.438, dev_prep: 38.634, (0.145 sec/step)\n",
      "step 1700, loss = 3.715582, perp: 34.326, dev_prep: 40.136, (0.147 sec/step)\n",
      "step 1720, loss = 3.788704, perp: 38.068, dev_prep: 40.564, (0.142 sec/step)\n",
      "step 1740, loss = 3.790552, perp: 37.875, dev_prep: 37.612, (0.142 sec/step)\n",
      "step 1760, loss = 3.783106, perp: 35.940, dev_prep: 42.917, (0.141 sec/step)\n",
      "step 1780, loss = 3.775779, perp: 37.664, dev_prep: 43.970, (0.148 sec/step)\n",
      "step 1800, loss = 3.758391, perp: 36.172, dev_prep: 33.140, (0.142 sec/step)\n",
      "step 1820, loss = 3.745696, perp: 36.698, dev_prep: 41.436, (0.145 sec/step)\n",
      "step 1840, loss = 3.760340, perp: 36.590, dev_prep: 39.207, (0.136 sec/step)\n",
      "step 1860, loss = 3.712266, perp: 35.455, dev_prep: 39.373, (0.130 sec/step)\n",
      "step 1880, loss = 3.796787, perp: 36.900, dev_prep: 40.213, (0.135 sec/step)\n",
      "step 1900, loss = 3.755296, perp: 36.852, dev_prep: 38.631, (0.146 sec/step)\n",
      "step 1920, loss = 3.717335, perp: 33.040, dev_prep: 38.013, (0.142 sec/step)\n",
      "step 1940, loss = 3.807978, perp: 37.863, dev_prep: 37.417, (0.137 sec/step)\n",
      "step 1960, loss = 3.707921, perp: 34.423, dev_prep: 37.373, (0.138 sec/step)\n",
      "step 1980, loss = 3.733685, perp: 35.233, dev_prep: 39.274, (0.135 sec/step)\n",
      "step 2000, loss = 3.785158, perp: 37.586, dev_prep: 35.939, (0.129 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 2020, loss = 3.724158, perp: 36.333, dev_prep: 35.913, (0.147 sec/step)\n",
      "step 2040, loss = 3.671019, perp: 33.203, dev_prep: 35.221, (0.136 sec/step)\n",
      "step 2060, loss = 3.666342, perp: 33.395, dev_prep: 34.166, (0.142 sec/step)\n",
      "step 2080, loss = 3.709075, perp: 35.276, dev_prep: 43.214, (0.130 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.750901, perp: 36.023, dev_prep: 33.486, (0.129 sec/step)\n",
      "step 2120, loss = 3.711032, perp: 34.830, dev_prep: 32.174, (0.135 sec/step)\n",
      "step 2140, loss = 3.781222, perp: 36.718, dev_prep: 34.539, (0.136 sec/step)\n",
      "step 2160, loss = 3.673753, perp: 32.633, dev_prep: 37.454, (0.135 sec/step)\n",
      "step 2180, loss = 3.560051, perp: 31.069, dev_prep: 37.943, (0.140 sec/step)\n",
      "step 2200, loss = 3.595893, perp: 31.049, dev_prep: 32.999, (0.148 sec/step)\n",
      "step 2220, loss = 3.690671, perp: 34.423, dev_prep: 35.089, (0.142 sec/step)\n",
      "step 2240, loss = 3.656971, perp: 33.372, dev_prep: 34.712, (0.142 sec/step)\n",
      "step 2260, loss = 3.628876, perp: 31.467, dev_prep: 34.846, (0.135 sec/step)\n",
      "step 2280, loss = 3.627746, perp: 32.114, dev_prep: 35.192, (0.139 sec/step)\n",
      "step 2300, loss = 3.553067, perp: 29.657, dev_prep: 31.849, (0.133 sec/step)\n",
      "step 2320, loss = 3.725417, perp: 33.592, dev_prep: 31.383, (0.133 sec/step)\n",
      "step 2340, loss = 3.702958, perp: 34.139, dev_prep: 32.455, (0.137 sec/step)\n",
      "step 2360, loss = 3.605660, perp: 32.104, dev_prep: 33.169, (0.141 sec/step)\n",
      "step 2380, loss = 3.615413, perp: 31.417, dev_prep: 36.112, (0.134 sec/step)\n",
      "step 2400, loss = 3.649815, perp: 32.427, dev_prep: 35.736, (0.144 sec/step)\n",
      "step 2420, loss = 3.599215, perp: 31.703, dev_prep: 30.936, (0.133 sec/step)\n",
      "step 2440, loss = 3.597341, perp: 31.457, dev_prep: 29.444, (0.131 sec/step)\n",
      "step 2460, loss = 3.548541, perp: 28.245, dev_prep: 32.956, (0.145 sec/step)\n",
      "step 2480, loss = 3.626002, perp: 31.015, dev_prep: 30.278, (0.132 sec/step)\n",
      "step 2500, loss = 3.687580, perp: 33.693, dev_prep: 32.660, (0.145 sec/step)\n",
      "step 2520, loss = 3.702415, perp: 34.333, dev_prep: 32.786, (0.132 sec/step)\n",
      "step 2540, loss = 3.441070, perp: 26.665, dev_prep: 33.297, (0.137 sec/step)\n",
      "step 2560, loss = 3.552678, perp: 29.302, dev_prep: 29.226, (0.138 sec/step)\n",
      "step 2580, loss = 3.506796, perp: 28.432, dev_prep: 33.257, (0.138 sec/step)\n",
      "step 2600, loss = 3.499446, perp: 28.536, dev_prep: 27.704, (0.133 sec/step)\n",
      "step 2620, loss = 3.656690, perp: 32.364, dev_prep: 29.239, (0.130 sec/step)\n",
      "step 2640, loss = 3.525412, perp: 28.993, dev_prep: 35.143, (0.137 sec/step)\n",
      "step 2660, loss = 3.572428, perp: 30.072, dev_prep: 31.577, (0.144 sec/step)\n",
      "step 2680, loss = 3.704504, perp: 34.337, dev_prep: 32.192, (0.146 sec/step)\n",
      "step 2700, loss = 3.431323, perp: 26.026, dev_prep: 33.389, (0.131 sec/step)\n",
      "step 2720, loss = 3.563780, perp: 30.845, dev_prep: 33.710, (0.134 sec/step)\n",
      "step 2740, loss = 3.581655, perp: 29.806, dev_prep: 30.405, (0.147 sec/step)\n",
      "step 2760, loss = 3.537366, perp: 28.460, dev_prep: 28.015, (0.144 sec/step)\n",
      "step 2780, loss = 3.542200, perp: 29.522, dev_prep: 31.327, (0.138 sec/step)\n",
      "step 2800, loss = 3.592415, perp: 30.673, dev_prep: 31.204, (0.132 sec/step)\n",
      "step 2820, loss = 3.501300, perp: 28.225, dev_prep: 31.624, (0.142 sec/step)\n",
      "step 2840, loss = 3.497987, perp: 29.002, dev_prep: 39.066, (0.138 sec/step)\n",
      "step 2860, loss = 3.503628, perp: 27.812, dev_prep: 33.540, (0.137 sec/step)\n",
      "step 2880, loss = 3.410895, perp: 25.448, dev_prep: 29.982, (0.140 sec/step)\n",
      "step 2900, loss = 3.470806, perp: 27.796, dev_prep: 31.118, (0.137 sec/step)\n",
      "step 2920, loss = 3.539332, perp: 28.912, dev_prep: 32.756, (0.140 sec/step)\n",
      "step 2940, loss = 3.584097, perp: 29.451, dev_prep: 31.093, (0.129 sec/step)\n",
      "step 2960, loss = 3.516557, perp: 29.127, dev_prep: 30.728, (0.147 sec/step)\n",
      "step 2980, loss = 3.401393, perp: 25.978, dev_prep: 31.181, (0.141 sec/step)\n",
      "step 3000, loss = 3.534683, perp: 29.902, dev_prep: 29.792, (0.140 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 3020, loss = 3.456310, perp: 27.054, dev_prep: 29.943, (0.140 sec/step)\n",
      "step 3040, loss = 3.469991, perp: 27.056, dev_prep: 29.740, (0.142 sec/step)\n",
      "step 3060, loss = 3.658754, perp: 32.496, dev_prep: 28.536, (0.132 sec/step)\n",
      "step 3080, loss = 3.609261, perp: 29.787, dev_prep: 30.873, (0.137 sec/step)\n",
      "step 3100, loss = 3.399674, perp: 25.502, dev_prep: 29.138, (0.138 sec/step)\n",
      "step 3120, loss = 3.413249, perp: 26.381, dev_prep: 28.524, (0.144 sec/step)\n",
      "step 3140, loss = 3.415505, perp: 25.848, dev_prep: 26.532, (0.136 sec/step)\n",
      "step 3160, loss = 3.665946, perp: 32.447, dev_prep: 29.030, (0.132 sec/step)\n",
      "step 3180, loss = 3.385527, perp: 25.708, dev_prep: 35.135, (0.133 sec/step)\n",
      "step 3200, loss = 3.583773, perp: 30.775, dev_prep: 32.091, (0.138 sec/step)\n",
      "step 3220, loss = 3.437026, perp: 26.452, dev_prep: 30.334, (0.140 sec/step)\n",
      "step 3240, loss = 3.379566, perp: 25.382, dev_prep: 30.273, (0.142 sec/step)\n",
      "step 3260, loss = 3.597722, perp: 30.361, dev_prep: 33.194, (0.138 sec/step)\n",
      "step 3280, loss = 3.291552, perp: 23.702, dev_prep: 26.213, (0.139 sec/step)\n",
      "step 3300, loss = 3.412679, perp: 25.413, dev_prep: 29.732, (0.128 sec/step)\n",
      "step 3320, loss = 3.462295, perp: 26.870, dev_prep: 27.308, (0.132 sec/step)\n",
      "step 3340, loss = 3.363214, perp: 25.197, dev_prep: 27.020, (0.141 sec/step)\n",
      "step 3360, loss = 3.471674, perp: 26.867, dev_prep: 31.428, (0.126 sec/step)\n",
      "step 3380, loss = 3.422695, perp: 27.000, dev_prep: 29.419, (0.136 sec/step)\n",
      "step 3400, loss = 3.361729, perp: 24.246, dev_prep: 26.017, (0.146 sec/step)\n",
      "step 3420, loss = 3.451128, perp: 27.842, dev_prep: 28.574, (0.142 sec/step)\n",
      "step 3440, loss = 3.368319, perp: 24.991, dev_prep: 28.262, (0.142 sec/step)\n",
      "step 3460, loss = 3.310672, perp: 23.754, dev_prep: 30.618, (0.134 sec/step)\n",
      "step 3480, loss = 3.428019, perp: 26.460, dev_prep: 25.398, (0.133 sec/step)\n",
      "step 3500, loss = 3.296833, perp: 23.137, dev_prep: 24.811, (0.146 sec/step)\n",
      "step 3520, loss = 3.483296, perp: 27.127, dev_prep: 25.090, (0.128 sec/step)\n",
      "step 3540, loss = 3.371032, perp: 25.171, dev_prep: 24.571, (0.139 sec/step)\n",
      "step 3560, loss = 3.531556, perp: 30.330, dev_prep: 29.833, (0.137 sec/step)\n",
      "step 3580, loss = 3.417552, perp: 25.768, dev_prep: 23.302, (0.149 sec/step)\n",
      "step 3600, loss = 3.233905, perp: 21.617, dev_prep: 25.462, (0.132 sec/step)\n",
      "step 3620, loss = 3.393982, perp: 26.680, dev_prep: 27.078, (0.139 sec/step)\n",
      "step 3640, loss = 3.277593, perp: 23.294, dev_prep: 25.020, (0.128 sec/step)\n",
      "step 3660, loss = 3.259907, perp: 23.310, dev_prep: 23.500, (0.134 sec/step)\n",
      "step 3680, loss = 3.339313, perp: 24.721, dev_prep: 27.557, (0.140 sec/step)\n",
      "step 3700, loss = 3.258082, perp: 22.564, dev_prep: 24.245, (0.135 sec/step)\n",
      "step 3720, loss = 3.362231, perp: 24.628, dev_prep: 26.053, (0.134 sec/step)\n",
      "step 3740, loss = 3.255232, perp: 23.292, dev_prep: 26.575, (0.130 sec/step)\n",
      "step 3760, loss = 3.286619, perp: 23.188, dev_prep: 23.680, (0.143 sec/step)\n",
      "step 3780, loss = 3.154048, perp: 21.358, dev_prep: 31.285, (0.135 sec/step)\n",
      "step 3800, loss = 3.346495, perp: 24.329, dev_prep: 23.632, (0.144 sec/step)\n",
      "step 3820, loss = 3.303702, perp: 23.572, dev_prep: 27.351, (0.136 sec/step)\n",
      "step 3840, loss = 3.291236, perp: 22.915, dev_prep: 24.725, (0.151 sec/step)\n",
      "step 3860, loss = 3.249071, perp: 21.843, dev_prep: 23.829, (0.133 sec/step)\n",
      "step 3880, loss = 3.229972, perp: 22.523, dev_prep: 24.464, (0.141 sec/step)\n",
      "step 3900, loss = 3.194439, perp: 21.233, dev_prep: 24.224, (0.142 sec/step)\n",
      "step 3920, loss = 3.185408, perp: 21.632, dev_prep: 26.597, (0.142 sec/step)\n",
      "step 3940, loss = 3.322894, perp: 23.533, dev_prep: 23.448, (0.122 sec/step)\n",
      "step 3960, loss = 3.195859, perp: 21.627, dev_prep: 29.421, (0.133 sec/step)\n",
      "step 3980, loss = 3.286663, perp: 23.106, dev_prep: 22.670, (0.142 sec/step)\n",
      "step 4000, loss = 3.409566, perp: 26.363, dev_prep: 23.816, (0.138 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 4020, loss = 3.239233, perp: 22.752, dev_prep: 24.559, (0.134 sec/step)\n",
      "step 4040, loss = 3.114845, perp: 19.455, dev_prep: 23.975, (0.136 sec/step)\n",
      "step 4060, loss = 3.258540, perp: 22.854, dev_prep: 23.432, (0.151 sec/step)\n",
      "step 4080, loss = 3.228811, perp: 22.765, dev_prep: 22.579, (0.144 sec/step)\n",
      "step 4100, loss = 3.296235, perp: 23.316, dev_prep: 24.353, (0.134 sec/step)\n",
      "step 4120, loss = 3.172456, perp: 21.180, dev_prep: 21.615, (0.146 sec/step)\n",
      "step 4140, loss = 3.197126, perp: 21.751, dev_prep: 23.249, (0.132 sec/step)\n",
      "step 4160, loss = 3.166684, perp: 20.744, dev_prep: 24.058, (0.140 sec/step)\n",
      "step 4180, loss = 3.242650, perp: 21.466, dev_prep: 22.243, (0.146 sec/step)\n",
      "step 4200, loss = 3.278091, perp: 23.459, dev_prep: 22.900, (0.142 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.203141, perp: 21.230, dev_prep: 22.272, (0.135 sec/step)\n",
      "step 4240, loss = 3.070545, perp: 19.371, dev_prep: 24.362, (0.138 sec/step)\n",
      "step 4260, loss = 3.251374, perp: 21.490, dev_prep: 25.162, (0.131 sec/step)\n",
      "step 4280, loss = 3.096757, perp: 20.359, dev_prep: 24.704, (0.146 sec/step)\n",
      "step 4300, loss = 3.069761, perp: 19.004, dev_prep: 23.028, (0.130 sec/step)\n",
      "step 4320, loss = 3.131556, perp: 20.611, dev_prep: 26.022, (0.144 sec/step)\n",
      "step 4340, loss = 3.295188, perp: 23.844, dev_prep: 21.067, (0.137 sec/step)\n",
      "step 4360, loss = 3.122752, perp: 20.576, dev_prep: 23.592, (0.140 sec/step)\n",
      "step 4380, loss = 3.151310, perp: 20.433, dev_prep: 23.407, (0.134 sec/step)\n",
      "step 4400, loss = 3.054773, perp: 18.346, dev_prep: 25.241, (0.132 sec/step)\n",
      "step 4420, loss = 3.192267, perp: 21.473, dev_prep: 25.909, (0.131 sec/step)\n",
      "step 4440, loss = 3.041554, perp: 18.667, dev_prep: 22.050, (0.138 sec/step)\n",
      "step 4460, loss = 3.151454, perp: 21.511, dev_prep: 24.979, (0.135 sec/step)\n",
      "step 4480, loss = 3.185341, perp: 20.977, dev_prep: 22.114, (0.141 sec/step)\n",
      "step 4500, loss = 3.099969, perp: 19.752, dev_prep: 22.221, (0.134 sec/step)\n",
      "step 4520, loss = 3.106036, perp: 19.981, dev_prep: 22.807, (0.128 sec/step)\n",
      "step 4540, loss = 3.164875, perp: 21.040, dev_prep: 22.444, (0.134 sec/step)\n",
      "step 4560, loss = 3.031222, perp: 18.578, dev_prep: 20.605, (0.147 sec/step)\n",
      "step 4580, loss = 3.071681, perp: 20.070, dev_prep: 20.751, (0.134 sec/step)\n",
      "step 4600, loss = 3.076549, perp: 19.300, dev_prep: 20.516, (0.139 sec/step)\n",
      "step 4620, loss = 3.098409, perp: 19.229, dev_prep: 21.641, (0.139 sec/step)\n",
      "step 4640, loss = 3.118418, perp: 19.568, dev_prep: 21.262, (0.129 sec/step)\n",
      "step 4660, loss = 3.115457, perp: 19.962, dev_prep: 20.400, (0.139 sec/step)\n",
      "step 4680, loss = 3.042543, perp: 18.596, dev_prep: 21.884, (0.132 sec/step)\n",
      "step 4700, loss = 2.971490, perp: 17.620, dev_prep: 19.621, (0.144 sec/step)\n",
      "step 4720, loss = 3.043023, perp: 18.387, dev_prep: 18.479, (0.143 sec/step)\n",
      "step 4740, loss = 3.043257, perp: 18.727, dev_prep: 21.318, (0.147 sec/step)\n",
      "step 4760, loss = 3.153708, perp: 20.729, dev_prep: 20.153, (0.136 sec/step)\n",
      "step 4780, loss = 3.020749, perp: 18.101, dev_prep: 19.851, (0.146 sec/step)\n",
      "step 4800, loss = 2.987433, perp: 17.985, dev_prep: 20.506, (0.145 sec/step)\n",
      "step 4820, loss = 3.047696, perp: 18.288, dev_prep: 20.282, (0.140 sec/step)\n",
      "step 4840, loss = 3.042722, perp: 19.139, dev_prep: 21.009, (0.142 sec/step)\n",
      "step 4860, loss = 3.080433, perp: 18.602, dev_prep: 20.374, (0.140 sec/step)\n",
      "step 4880, loss = 2.940500, perp: 16.719, dev_prep: 18.360, (0.159 sec/step)\n",
      "step 4900, loss = 2.956944, perp: 17.170, dev_prep: 19.563, (0.133 sec/step)\n",
      "step 4920, loss = 2.897947, perp: 16.653, dev_prep: 20.198, (0.155 sec/step)\n",
      "step 4940, loss = 2.978723, perp: 18.181, dev_prep: 19.305, (0.137 sec/step)\n",
      "step 4960, loss = 3.005294, perp: 18.643, dev_prep: 19.592, (0.131 sec/step)\n",
      "step 4980, loss = 2.997217, perp: 18.440, dev_prep: 21.447, (0.144 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        choice_batch = choice_data[rand_indexes]\n",
    "        emotions = category_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            choice_qs: choice_batch,\n",
    "            emo_cat: emotions,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    choice_qs: dev_choice_data[dev_inds],\n",
    "                    emo_cat: dev_category_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Total loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"Perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check trained ECM model: internal memory and external choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norms of final-state internal memory:\n",
      " [2.8790933e-07 2.6792186e-06 2.4213847e-07 9.7954022e-08 8.2459685e-08\n",
      " 3.9985258e-07 1.7050779e-07 2.7580190e-06 1.2004278e-06 2.8663001e-07\n",
      " 4.3089946e-07 3.0665160e-07 4.6975921e-07 7.4334957e-07 7.0569762e-08\n",
      " 3.1758930e-08 9.0873712e-08 3.5633196e-08 1.4215630e-07 1.0965078e-07\n",
      " 4.3554115e-07 2.2078230e-07 2.2771822e-07 3.2793645e-07 5.5187702e-08\n",
      " 2.7770790e-07 1.5649427e-07 1.7946068e-07 1.8988439e-07 6.8626349e-07\n",
      " 7.8908255e-07 1.4694601e-07 1.0405934e-07 1.4652708e-06 9.5260850e-07\n",
      " 4.2721987e-07 4.7273158e-08 5.9784895e-08 6.2863512e-07 2.7644569e-06\n",
      " 4.9309651e-08 1.3292180e-07 1.1962375e-07 1.4266817e-07 5.7590989e-07\n",
      " 5.3724852e-07 6.1734141e-08 1.7970169e-07 5.1682633e-07 5.6176653e-08\n",
      " 5.7542120e-08 1.2704892e-07 1.1226833e-07 8.8842100e-08 2.3567678e-08\n",
      " 9.0674877e-07 9.5734561e-07 2.5307341e-07 6.4216403e-07 7.8598937e-08\n",
      " 1.0348766e-07 3.9680469e-07 4.9817862e-07 5.6060719e-07]\n"
     ]
    }
   ],
   "source": [
    "(cell, train_log_probs, alphas, int_M_emo) = train_outs\n",
    "\n",
    "M_norms = sess.run(tf.norm(int_M_emo, axis=1), feed_dict)\n",
    "print(\"Norms of final-state internal memory:\\n\", M_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alphas (predicted choice) and true choices\n",
    "rand_indexes = np.random.choice(n_data, 6)\n",
    "source_batch = source_data[rand_indexes]\n",
    "target_batch = target_data[rand_indexes]\n",
    "emotions = category_data[rand_indexes]\n",
    "\n",
    "choice_preds = sess.run(alphas,\n",
    "                        feed_dict={\n",
    "                            source_ids: source_batch,\n",
    "                            target_ids: target_batch,\n",
    "                            emo_cat: emotions})\n",
    "choice_batch = choice_data[rand_indexes]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(choice_preds, interpolation=\"none\")\n",
    "plt.title(\"Predicted Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./predict_choice\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(choice_batch, interpolation=\"none\")\n",
    "plt.title(\"True Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./true_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "c_filename = config[\"inference\"][\"infer_category_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "category_data = np.concatenate((category_data, np.zeros(n_pad)))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "    batch_cat = category_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs,\n",
    "                      feed_dict={source_ids: batch, emo_cat: batch_cat})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "choice_pred = (final_result >= vocab_size).astype(np.int)\n",
    "final_result[final_result >= vocab_size] -= (vocab_size + embed_shift)\n",
    "\n",
    "# transform to output format\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "choice_pred = choice_pred.astype(str).tolist()\n",
    "choice_pred = list(map(lambda t: \" \".join(t), choice_pred))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "\n",
    "cdf = pd.DataFrame(data={\"0\": choice_pred})\n",
    "cdf.to_csv(config[\"inference\"][\"choice_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.328549506730784"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "q = dev_choice_data[random_indexes]\n",
    "c = dev_category_data[random_indexes]\n",
    "\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    choice_qs: q,\n",
    "    emo_cat: c,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
