{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting\n",
    "#### Prime numbers as simulated emotion words and emotion categories:\n",
    "- ECM should use external memory when predicting primes\n",
    "- Primes are equally split into (num_emo) \"emotion\" categories\n",
    "- sequence with most primes from a certain category is tagged with that \"emotion\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a number is a prime\n",
    "def is_prime(n):\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(3, int(np.sqrt(n)) + 1):\n",
    "            if n % i == 0:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "num_emo = 4\n",
    "\n",
    "nums = np.arange(N)\n",
    "check_prime = np.vectorize(is_prime)\n",
    "primes = nums[check_prime(nums)]\n",
    "\n",
    "# equally split primes into categories\n",
    "s_primes = np.array_split(primes, num_emo)\n",
    "s_primes = [s_p.tolist() for s_p in s_primes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "source_data = []\n",
    "target_data = []\n",
    "choice_data = []\n",
    "category_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    # 1: emotion words/primes, 0: generic words\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "qdf = pd.DataFrame(data={\"0\": choice_data})\n",
    "cdf = pd.DataFrame(data={\"0\": category_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)\n",
    "qdf.to_csv(\"./example/train_choice.txt\", header=None, index=None)\n",
    "cdf.to_csv(\"./example/train_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_choice_data = []\n",
    "dev_category_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "    q = check_prime(t).astype(np.int)\n",
    "\n",
    "    counts = np.sum([[(w_t in s_p) for s_p in s_primes] for w_t in t], axis=0)\n",
    "    category = np.argmax(counts)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))\n",
    "    dev_choice_data.append(\" \".join(q.astype(str).tolist()))\n",
    "    dev_category_data.append(str(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "qddf = pd.DataFrame(data={\"0\": dev_choice_data})\n",
    "cddf = pd.DataFrame(data={\"0\": dev_category_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)\n",
    "qddf.to_csv(\"./example/dev_choice.txt\", header=None, index=None)\n",
    "cddf.to_csv(\"./example/dev_category.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ECM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECM wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "ECMState = collections.namedtuple(\n",
    "    \"ECMState\", (\"cell_states\", \"h\", \"context\", \"internal_memory\"))\n",
    "\n",
    "\n",
    "class ECMWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Emotion Chatting Machine: H. Zhou, et al. AAAI 2018\n",
    "    (https://arxiv.org/abs/1704.01074)\n",
    "    Emotion Category Embedding, Internal and External Memory Modules\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "        emo_cat_embs: category embeddings, [batch_size, emo_cat_units]\n",
    "        emo_cat: emotion category, [batch_size]\n",
    "        emo_int_units: dimension of internal emotion memory\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype, emo_cat_embs, emo_cat, num_emo,\n",
    "                 emo_int_units, emo_init=None):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = ECMState(self._cell.state_size,\n",
    "                                    num_units, memory.shape[-1].value,\n",
    "                                    emo_int_units)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "        # ECM hyperparameters\n",
    "        self._emo_cat_embs = emo_cat_embs\n",
    "        self._emo_cat = emo_cat\n",
    "        self._emo_int_units = emo_int_units\n",
    "\n",
    "        # internal memory\n",
    "        if emo_init is None:\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        self.int_memory = tf.Variable(\n",
    "            initializer(shape=(num_emo, emo_int_units)),\n",
    "            name=\"emo_memory\", dtype=dtype)\n",
    "\n",
    "        self.read_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_read_gate\")\n",
    "        self.write_g = tf.layers.Dense(\n",
    "            emo_int_units, use_bias=False, name=\"internal_write_gate\")\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for ECM wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        Returns:\n",
    "            h_0: [batch_size, num_units]\n",
    "            context_0: [batch_size, num_units]\n",
    "            M_emo_0: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "        M_emo_0 = tf.gather(self.int_memory, self._emo_cat)\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        ecm_state_0 = ECMState(cell_states, h_0, context_0, M_emo_0)\n",
    "\n",
    "        return ecm_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def _read_internal_memory(self, M_emo, read_inputs):\n",
    "        \"\"\"\n",
    "        Read the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            read_inputs: [batch_size, d]\n",
    "        Returns:\n",
    "            M_read: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_read = tf.nn.sigmoid(self.read_g(read_inputs))\n",
    "        return (M_emo * gate_read)\n",
    "\n",
    "    def _write_internal_memory(self, M_emo, new_h):\n",
    "        \"\"\"\n",
    "        Write the internal memory\n",
    "            M_emo: [batch_size, emo_int_units]\n",
    "            new_h: [batch_size, num_units]\n",
    "        Returns:\n",
    "            M_write: [batch_size, emo_int_units]\n",
    "        \"\"\"\n",
    "        gate_write = tf.nn.sigmoid(self.write_g(new_h))\n",
    "        return (M_emo * gate_write)\n",
    "\n",
    "    def __call__(self, inputs, ecm_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs, context, int_memory)\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context, M_emo = ecm_states\n",
    "\n",
    "        # read internal memory\n",
    "        read_inputs = tf.concat([inputs, h, context], axis=-1)\n",
    "        M_read = self._read_internal_memory(M_emo, read_inputs)\n",
    "\n",
    "        # pass into RNN_cell to get the output\n",
    "        x = [inputs, h, context, self._emo_cat_embs, M_read]\n",
    "        x = tf.concat(x, axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        # update states\n",
    "        new_M_emo = self._write_internal_memory(M_emo, new_h)\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_ecm_states = ECMState(cell_states, new_h, new_context, new_M_emo)\n",
    "\n",
    "        return (new_h, new_ecm_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ECMBeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states, output_layer,\n",
    "                 emo_output_layer, emo_choice_layer, batch_size, dtype,\n",
    "                 beam_size, vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size * 2\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "\n",
    "        self._output_layer = output_layer\n",
    "        self._emo_output_layer = emo_output_layer\n",
    "        self._emo_choice_layer = emo_choice_layer\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "\n",
    "        indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat_embs\"):\n",
    "            self._cell._emo_cat_embs = tf.gather(\n",
    "                self._cell._emo_cat_embs, indices)\n",
    "\n",
    "        if hasattr(self._cell, \"_emo_cat\"):\n",
    "            self._emo_cat = tf.gather(self._cell._emo_cat, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new log probs\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        gen_log_probs = tf.nn.log_softmax(self._output_layer(new_h))\n",
    "        emo_log_probs = tf.nn.log_softmax(self._emo_output_layer(new_h))\n",
    "        alphas = tf.nn.sigmoid(self._emo_choice_layer(new_h))\n",
    "\n",
    "        gen_log_probs = gen_log_probs + tf.log(1 - alphas)\n",
    "        emo_log_probs = emo_log_probs + tf.log(alphas)\n",
    "        raw_log_probs = tf.concat([gen_log_probs, emo_log_probs], axis=-1)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        raw_log_probs = split_batch_beam(raw_log_probs, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: mask log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = mask_log_probs(\n",
    "            raw_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=raw_log_probs, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_ECM_decoder(encoder_outputs, encoder_states, embeddings, num_layers,\n",
    "                      num_units, cell_type, num_emo, emo_cat, emo_cat_units,\n",
    "                      emo_int_units, state_pass=True, infer_batch_size=None,\n",
    "                      attn_num_units=128, target_ids=None, beam_size=None,\n",
    "                      max_iter=20, dtype=tf.float32, forget_bias=1.0,\n",
    "                      name=\"ECM_decoder\"):\n",
    "    \"\"\"\n",
    "    ECM decoder: build ECM decoder with emotion category embedding,\n",
    "             internal & external memory modules\n",
    "        target_ids: [batch_size, max_time]\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "        num_emo: number of emotions\n",
    "        emo_cat: emotion catogories, [batch_size]\n",
    "        emo_cat_units: dimension of emotion category embeddings\n",
    "        emo_int_units: dimension of emotion internal memory\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: (generic_logits, emo_ext_logits, alphas, int_M_emo),\n",
    "            first 3 shape: [batch_size, max_time, d]\n",
    "            int_M_emo: [batch_size, emo_int_units]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # create emotion category embeddings\n",
    "        emo_init = tf.contrib.layers.xavier_initializer()\n",
    "        emo_cat_embeddings = tf.Variable(\n",
    "            emo_init(shape=(num_emo, emo_cat_units)),\n",
    "            name=\"emo_cat_embeddings\", dtype=dtype)\n",
    "        emo_cat_embs = tf.nn.embedding_lookup(emo_cat_embeddings, emo_cat)\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with ECM internal memory module\n",
    "        memory = encoder_outputs\n",
    "\n",
    "        cell = ECMWrapper(\n",
    "            cell, memory, dec_init_states, attn_num_units, num_units, dtype,\n",
    "            emo_cat_embs, emo_cat, num_emo, emo_int_units)\n",
    "\n",
    "        dec_init_states = cell.initial_state()\n",
    "\n",
    "        # ECM external memory module\n",
    "        emo_output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"emo_output_projection\")\n",
    "\n",
    "        emo_choice_layer = tf.layers.Dense(\n",
    "            1, use_bias=False, name=\"emo_choice_alpha\")\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits & final internal memory states\n",
    "            generic_logits = output_layer(decoder_outputs)\n",
    "            emo_ext_logits = emo_output_layer(decoder_outputs)\n",
    "            alphas = tf.nn.sigmoid(emo_choice_layer(decoder_outputs))\n",
    "            int_M_emo = decoder_states.internal_memory\n",
    "\n",
    "            train_outputs = (generic_logits, emo_ext_logits, alphas, int_M_emo)\n",
    "\n",
    "        # Decode - for inference, beam search\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            decoder_cell = ECMBeamSearchDecodeCell(\n",
    "                embeddings, cell, dec_init_states, output_layer,\n",
    "                emo_output_layer, emo_choice_layer,\n",
    "                infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                div_gamma=None, div_prob=None)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return cell, train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "emo_cat = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=True, target_ids=target_ids, name=\"ECM_decoder2\")\n",
    "\n",
    "g_logits, e_logits, alphas, M_emo = train_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0.00099809, 0.00099668, 0.00099485, ..., 0.00099415,\n",
       "          0.00099486, 0.00099702],\n",
       "         [0.00099871, 0.00099785, 0.00099328, ..., 0.00099121,\n",
       "          0.0009949 , 0.00099629],\n",
       "         [0.00099878, 0.00099895, 0.00099182, ..., 0.00098826,\n",
       "          0.00099453, 0.00099635],\n",
       "         [0.0009983 , 0.00099997, 0.0009906 , ..., 0.00098536,\n",
       "          0.00099416, 0.00099712]],\n",
       " \n",
       "        [[0.00099548, 0.00099515, 0.00099748, ..., 0.00099256,\n",
       "          0.00099447, 0.00099847],\n",
       "         [0.00099501, 0.0009957 , 0.00099703, ..., 0.00099022,\n",
       "          0.0009904 , 0.00099927],\n",
       "         [0.00099461, 0.00099653, 0.00099663, ..., 0.00098886,\n",
       "          0.00098646, 0.00100012],\n",
       "         [0.00099436, 0.00099771, 0.00099647, ..., 0.00098817,\n",
       "          0.00098225, 0.00100156]]], dtype=float32), array([[[0.50025386],\n",
       "         [0.49978355],\n",
       "         [0.49898955],\n",
       "         [0.4981797 ]],\n",
       " \n",
       "        [[0.50080967],\n",
       "         [0.50023526],\n",
       "         [0.49970156],\n",
       "         [0.4992308 ]]], dtype=float32), array([[-8.31353664e-03, -1.54586909e-02,  8.48037563e-03,\n",
       "         -2.36653420e-03,  2.98505067e-03, -7.52492389e-03,\n",
       "         -4.06128168e-03,  3.04713682e-03, -7.85601139e-03,\n",
       "         -1.28519470e-02, -1.67405549e-02,  6.71041105e-03,\n",
       "         -1.02048703e-02, -4.42285556e-03,  1.15308799e-02,\n",
       "          3.66354268e-03, -7.99671095e-03, -6.06308691e-03,\n",
       "          2.93738209e-03,  4.15112963e-03,  2.43068463e-03,\n",
       "          3.29289527e-04, -6.75185630e-03,  7.92880310e-04,\n",
       "         -1.85531117e-02, -5.40207280e-03, -1.36175444e-02,\n",
       "         -4.57424438e-03, -7.35084247e-03, -2.98831472e-03,\n",
       "         -1.74451824e-02, -1.99306360e-03,  7.52486102e-03,\n",
       "          2.20146822e-03,  6.05483027e-03,  8.99223611e-03,\n",
       "          1.46334823e-02,  1.13537395e-02, -1.11662960e-02,\n",
       "         -7.95680098e-03, -1.41132949e-02,  1.54707441e-02,\n",
       "         -8.56491644e-03, -1.55739160e-02, -3.31525807e-03,\n",
       "          1.06612425e-02,  4.00217483e-03, -5.87772951e-03,\n",
       "         -3.84411681e-03, -9.97075811e-03, -2.05979077e-03,\n",
       "         -9.86322016e-03,  1.24949804e-02,  9.27502476e-03,\n",
       "          3.93954013e-03, -4.54281503e-03,  1.13497805e-02,\n",
       "          1.85522693e-03,  1.69489868e-02,  7.42388144e-03,\n",
       "         -1.57605391e-02, -3.11791874e-03, -1.28034363e-02,\n",
       "         -5.77127328e-03],\n",
       "        [ 1.13918055e-02, -1.56605896e-02, -3.21662449e-03,\n",
       "          1.53688691e-03,  2.45652068e-03, -1.59500651e-02,\n",
       "         -1.45075331e-02, -8.43349192e-03,  4.01935959e-03,\n",
       "          1.24229435e-02, -1.50246378e-02, -8.27749167e-03,\n",
       "          7.72869959e-03,  4.89594648e-03,  1.20762894e-02,\n",
       "         -1.81459207e-02, -1.06370468e-02,  4.24215291e-03,\n",
       "          7.11293006e-03, -4.19944525e-03,  1.02035003e-02,\n",
       "         -5.98440587e-04,  4.13338933e-03, -1.56598333e-02,\n",
       "         -1.82949156e-02,  5.40500856e-04,  8.30511656e-03,\n",
       "         -6.95674634e-03,  9.28949521e-05,  1.22764930e-02,\n",
       "          1.57953203e-02,  1.25420401e-02,  9.24987346e-03,\n",
       "         -8.72210972e-03, -6.76350808e-03, -1.23221017e-02,\n",
       "         -1.58801246e-02, -3.24658467e-03,  1.03402399e-02,\n",
       "         -1.62528791e-02,  4.01972234e-03,  1.58977578e-03,\n",
       "         -1.53301805e-02, -1.66453526e-03,  1.08592620e-04,\n",
       "          1.91853999e-03, -8.47721100e-03,  1.38639677e-02,\n",
       "          5.11323009e-03,  1.20250704e-02,  2.98073119e-03,\n",
       "          6.00526528e-03, -1.04582887e-02, -1.07198441e-02,\n",
       "          6.42402424e-03, -1.38863120e-02, -1.02035962e-02,\n",
       "          6.57075943e-05,  1.76503211e-02, -1.69258546e-02,\n",
       "         -1.05586378e-02,  1.14730485e-02,  9.12120566e-03,\n",
       "          5.28981956e-03]], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run([tf.nn.softmax(g_logits), alphas, M_emo],\n",
    "                   feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                              target_ids: [[8, 8, 8], [8, 10, 12]],\n",
    "                              emo_cat: [0, 1]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:36: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "    encoder_outputs, encoder_states, embeddings, num_layers=2, num_units=256,\n",
    "    cell_type=\"LSTM\", num_emo=4, emo_cat=emo_cat, emo_cat_units=32,\n",
    "    emo_int_units=64, state_pass=False, infer_batch_size=3,\n",
    "    beam_size=5, max_iter=10, name=\"ECM_infer1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-7.604128 , -7.6034555, -7.6023645, ..., -7.602513 ,\n",
       "          -7.604357 , -7.608027 ],\n",
       "         [-7.604128 , -7.6034555, -7.6023645, ..., -7.602513 ,\n",
       "          -7.604357 , -7.608027 ],\n",
       "         [-7.604128 , -7.6034555, -7.6023645, ..., -7.602513 ,\n",
       "          -7.604357 , -7.608027 ],\n",
       "         [-7.604128 , -7.6034555, -7.6023645, ..., -7.602513 ,\n",
       "          -7.604357 , -7.608027 ],\n",
       "         [-7.604128 , -7.6034555, -7.6023645, ..., -7.602513 ,\n",
       "          -7.604357 , -7.608027 ]],\n",
       "\n",
       "        [[-7.607124 , -7.606304 , -7.6028185, ..., -7.5990176,\n",
       "          -7.6037626, -7.6100035],\n",
       "         [-7.607974 , -7.606845 , -7.6037307, ..., -7.598666 ,\n",
       "          -7.603488 , -7.6101465],\n",
       "         [-7.607124 , -7.606304 , -7.6028185, ..., -7.5990176,\n",
       "          -7.6037626, -7.6100035],\n",
       "         [-7.607974 , -7.606845 , -7.6037307, ..., -7.598666 ,\n",
       "          -7.603488 , -7.6101465],\n",
       "         [-7.6072817, -7.606355 , -7.603277 , ..., -7.5985703,\n",
       "          -7.6039653, -7.6102047]],\n",
       "\n",
       "        [[-7.6117826, -7.611212 , -7.604873 , ..., -7.594759 ,\n",
       "          -7.601856 , -7.6100717],\n",
       "         [-7.6130667, -7.6120477, -7.6060085, ..., -7.5936203,\n",
       "          -7.6018667, -7.6105385],\n",
       "         [-7.6118507, -7.6111655, -7.6053433, ..., -7.5937185,\n",
       "          -7.602491 , -7.610741 ],\n",
       "         [-7.611615 , -7.611099 , -7.6045094, ..., -7.5943217,\n",
       "          -7.6022778, -7.6104136],\n",
       "         [-7.6117826, -7.611212 , -7.604873 , ..., -7.594759 ,\n",
       "          -7.601856 , -7.6100717]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.6386547, -7.6392784, -7.61874  , ..., -7.569541 ,\n",
       "          -7.5891557, -7.606439 ],\n",
       "         [-7.6386547, -7.6392784, -7.61874  , ..., -7.569541 ,\n",
       "          -7.5891557, -7.606439 ],\n",
       "         [-7.6386547, -7.6392784, -7.61874  , ..., -7.569541 ,\n",
       "          -7.5891557, -7.606439 ],\n",
       "         [-7.6386547, -7.6392784, -7.61874  , ..., -7.569541 ,\n",
       "          -7.5891557, -7.606439 ],\n",
       "         [-7.6386547, -7.6392784, -7.61874  , ..., -7.569541 ,\n",
       "          -7.5891557, -7.606439 ]],\n",
       "\n",
       "        [[-7.6409497, -7.641081 , -7.619697 , ..., -7.567782 ,\n",
       "          -7.5876665, -7.606441 ],\n",
       "         [-7.6409497, -7.641081 , -7.619697 , ..., -7.567782 ,\n",
       "          -7.5876665, -7.606441 ],\n",
       "         [-7.6409497, -7.641081 , -7.619697 , ..., -7.567782 ,\n",
       "          -7.5876665, -7.606441 ],\n",
       "         [-7.6409497, -7.641081 , -7.619697 , ..., -7.567782 ,\n",
       "          -7.5876665, -7.606441 ],\n",
       "         [-7.6409497, -7.641081 , -7.619697 , ..., -7.567782 ,\n",
       "          -7.5876665, -7.606441 ]],\n",
       "\n",
       "        [[-7.64282  , -7.6422043, -7.620382 , ..., -7.566536 ,\n",
       "          -7.586417 , -7.6066732],\n",
       "         [-7.64282  , -7.6422043, -7.620382 , ..., -7.566536 ,\n",
       "          -7.586417 , -7.6066732],\n",
       "         [-7.64282  , -7.6422043, -7.620382 , ..., -7.566536 ,\n",
       "          -7.586417 , -7.6066732],\n",
       "         [-7.64282  , -7.6422043, -7.620382 , ..., -7.566536 ,\n",
       "          -7.586417 , -7.6066732],\n",
       "         [-7.64282  , -7.6422043, -7.620382 , ..., -7.566536 ,\n",
       "          -7.586417 , -7.6066732]]],\n",
       "\n",
       "\n",
       "       [[[-7.6013956, -7.6005354, -7.6021757, ..., -7.602391 ,\n",
       "          -7.6045036, -7.6062264],\n",
       "         [-7.6013956, -7.6005354, -7.6021757, ..., -7.602391 ,\n",
       "          -7.6045036, -7.6062264],\n",
       "         [-7.6013956, -7.6005354, -7.6021757, ..., -7.602391 ,\n",
       "          -7.6045036, -7.6062264],\n",
       "         [-7.6013956, -7.6005354, -7.6021757, ..., -7.602391 ,\n",
       "          -7.6045036, -7.6062264],\n",
       "         [-7.6013956, -7.6005354, -7.6021757, ..., -7.602391 ,\n",
       "          -7.6045036, -7.6062264]],\n",
       "\n",
       "        [[-7.5990443, -7.597004 , -7.6022215, ..., -7.600051 ,\n",
       "          -7.6051006, -7.607353 ],\n",
       "         [-7.599379 , -7.5983663, -7.6021028, ..., -7.5996647,\n",
       "          -7.605297 , -7.60754  ],\n",
       "         [-7.599379 , -7.5983663, -7.6021028, ..., -7.5996647,\n",
       "          -7.605297 , -7.60754  ],\n",
       "         [-7.5990443, -7.597004 , -7.6022215, ..., -7.600051 ,\n",
       "          -7.6051006, -7.607353 ],\n",
       "         [-7.599379 , -7.5983663, -7.6021028, ..., -7.5996647,\n",
       "          -7.605297 , -7.60754  ]],\n",
       "\n",
       "        [[-7.597187 , -7.5937514, -7.6025724, ..., -7.5971217,\n",
       "          -7.6067657, -7.608389 ],\n",
       "         [-7.597187 , -7.5937514, -7.6025724, ..., -7.5971217,\n",
       "          -7.6067657, -7.608389 ],\n",
       "         [-7.596919 , -7.593326 , -7.6032815, ..., -7.597711 ,\n",
       "          -7.6057596, -7.608269 ],\n",
       "         [-7.597187 , -7.5937514, -7.6025724, ..., -7.5971217,\n",
       "          -7.6067657, -7.608389 ],\n",
       "         [-7.5972514, -7.5946994, -7.603169 , ..., -7.597317 ,\n",
       "          -7.6059422, -7.6084576]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.5939765, -7.5794954, -7.611063 , ..., -7.587875 ,\n",
       "          -7.608218 , -7.6149764],\n",
       "         [-7.5939765, -7.5794954, -7.611063 , ..., -7.587875 ,\n",
       "          -7.608218 , -7.6149764],\n",
       "         [-7.5939765, -7.5794954, -7.611063 , ..., -7.587875 ,\n",
       "          -7.608218 , -7.6149764],\n",
       "         [-7.5935135, -7.5786695, -7.611378 , ..., -7.588749 ,\n",
       "          -7.608405 , -7.6150017],\n",
       "         [-7.5935135, -7.5786695, -7.611378 , ..., -7.588749 ,\n",
       "          -7.608405 , -7.6150017]],\n",
       "\n",
       "        [[-7.5940046, -7.578066 , -7.611994 , ..., -7.5873594,\n",
       "          -7.608148 , -7.616185 ],\n",
       "         [-7.5940046, -7.578066 , -7.611994 , ..., -7.5873594,\n",
       "          -7.608148 , -7.616185 ],\n",
       "         [-7.5940046, -7.578066 , -7.611994 , ..., -7.5873594,\n",
       "          -7.608148 , -7.616185 ],\n",
       "         [-7.593603 , -7.577309 , -7.6121955, ..., -7.588304 ,\n",
       "          -7.608398 , -7.616211 ],\n",
       "         [-7.593603 , -7.577309 , -7.6121955, ..., -7.588304 ,\n",
       "          -7.608398 , -7.616211 ]],\n",
       "\n",
       "        [[-7.5940084, -7.576717 , -7.6127453, ..., -7.5871015,\n",
       "          -7.608085 , -7.61737  ],\n",
       "         [-7.5940084, -7.576717 , -7.6127453, ..., -7.5871015,\n",
       "          -7.608085 , -7.61737  ],\n",
       "         [-7.5940084, -7.576717 , -7.6127453, ..., -7.5871015,\n",
       "          -7.608085 , -7.61737  ],\n",
       "         [-7.5936546, -7.5760264, -7.6128416, ..., -7.588079 ,\n",
       "          -7.608389 , -7.6173916],\n",
       "         [-7.5936546, -7.5760264, -7.6128416, ..., -7.588079 ,\n",
       "          -7.608389 , -7.6173916]]],\n",
       "\n",
       "\n",
       "       [[[-7.6012216, -7.6012774, -7.6012497, ..., -7.604541 ,\n",
       "          -7.602949 , -7.6069226],\n",
       "         [-7.6012216, -7.6012774, -7.6012497, ..., -7.604541 ,\n",
       "          -7.602949 , -7.6069226],\n",
       "         [-7.6012216, -7.6012774, -7.6012497, ..., -7.604541 ,\n",
       "          -7.602949 , -7.6069226],\n",
       "         [-7.6012216, -7.6012774, -7.6012497, ..., -7.604541 ,\n",
       "          -7.602949 , -7.6069226],\n",
       "         [-7.6012216, -7.6012774, -7.6012497, ..., -7.604541 ,\n",
       "          -7.602949 , -7.6069226]],\n",
       "\n",
       "        [[-7.599737 , -7.600624 , -7.600026 , ..., -7.6039343,\n",
       "          -7.601695 , -7.6078987],\n",
       "         [-7.5997305, -7.6013784, -7.6005898, ..., -7.6043   ,\n",
       "          -7.599861 , -7.607287 ],\n",
       "         [-7.599429 , -7.600266 , -7.5998483, ..., -7.6036963,\n",
       "          -7.600266 , -7.60869  ],\n",
       "         [-7.599523 , -7.6002774, -7.6001663, ..., -7.6044464,\n",
       "          -7.600733 , -7.6076736],\n",
       "         [-7.5990815, -7.60002  , -7.5997677, ..., -7.6039753,\n",
       "          -7.601709 , -7.608055 ]],\n",
       "\n",
       "        [[-7.5979   , -7.600875 , -7.5999207, ..., -7.602993 ,\n",
       "          -7.5987353, -7.6073174],\n",
       "         [-7.598357 , -7.601283 , -7.600483 , ..., -7.60374  ,\n",
       "          -7.597467 , -7.606909 ],\n",
       "         [-7.5987873, -7.601887 , -7.6002135, ..., -7.602975 ,\n",
       "          -7.598878 , -7.6070976],\n",
       "         [-7.5979   , -7.600875 , -7.5999207, ..., -7.602993 ,\n",
       "          -7.5987353, -7.6073174],\n",
       "         [-7.5984273, -7.601306 , -7.6000767, ..., -7.6025934,\n",
       "          -7.5966754, -7.608211 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.590261 , -7.606741 , -7.6028495, ..., -7.5975876,\n",
       "          -7.5923867, -7.605132 ],\n",
       "         [-7.590261 , -7.606741 , -7.6028495, ..., -7.5975876,\n",
       "          -7.5923867, -7.605132 ],\n",
       "         [-7.590261 , -7.606741 , -7.6028495, ..., -7.5975876,\n",
       "          -7.5923867, -7.605132 ],\n",
       "         [-7.590261 , -7.606741 , -7.6028495, ..., -7.5975876,\n",
       "          -7.5923867, -7.605132 ],\n",
       "         [-7.590261 , -7.606741 , -7.6028495, ..., -7.5975876,\n",
       "          -7.5923867, -7.605132 ]],\n",
       "\n",
       "        [[-7.5887938, -7.6071987, -7.6030293, ..., -7.5973983,\n",
       "          -7.593352 , -7.605572 ],\n",
       "         [-7.5887938, -7.6071987, -7.6030293, ..., -7.5973983,\n",
       "          -7.593352 , -7.605572 ],\n",
       "         [-7.5887938, -7.6071987, -7.6030293, ..., -7.5973983,\n",
       "          -7.593352 , -7.605572 ],\n",
       "         [-7.5887938, -7.6071987, -7.6030293, ..., -7.5973983,\n",
       "          -7.593352 , -7.605572 ],\n",
       "         [-7.5887938, -7.6071987, -7.6030293, ..., -7.5973983,\n",
       "          -7.593352 , -7.605572 ]],\n",
       "\n",
       "        [[-7.587443 , -7.6077247, -7.6031895, ..., -7.5973334,\n",
       "          -7.594376 , -7.6060314],\n",
       "         [-7.587443 , -7.6077247, -7.6031895, ..., -7.5973334,\n",
       "          -7.594376 , -7.6060314],\n",
       "         [-7.587443 , -7.6077247, -7.6031895, ..., -7.5973334,\n",
       "          -7.594376 , -7.6060314],\n",
       "         [-7.587443 , -7.6077247, -7.6031895, ..., -7.5973334,\n",
       "          -7.594376 , -7.6060314],\n",
       "         [-7.587443 , -7.6077247, -7.6031895, ..., -7.5973334,\n",
       "          -7.594376 , -7.6060314]]]], dtype=float32), ids=array([[[ 460,  592,  961, 1845, 1198],\n",
       "        [1917, 1917, 1917, 1917, 1917],\n",
       "        [1917, 1917, 1917, 1429, 1917],\n",
       "        [1917, 1917, 1917, 1917, 1429],\n",
       "        [1917, 1917, 1917, 1917, 1917],\n",
       "        [1917, 1923, 1917, 1917, 1917],\n",
       "        [1923, 1923, 1917, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1734, 1923, 1923]],\n",
       "\n",
       "       [[   3,   45,   45,  985,  894],\n",
       "        [  45,   45,   45,   45,   45],\n",
       "        [  45,   45,   45,   45, 1725],\n",
       "        [1725,   45, 1725,   45, 1042],\n",
       "        [1725, 1725, 1725, 1725, 1042],\n",
       "        [1042, 1042, 1042, 1725, 1725],\n",
       "        [1294, 1042, 1294, 1042, 1042],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294]],\n",
       "\n",
       "       [[   3,  145,  592,  985,  454],\n",
       "        [ 454,  454,  454,  454,  454],\n",
       "        [1806, 1806, 1806, 1806, 1806],\n",
       "        [1806, 1806, 1021, 1806, 1806],\n",
       "        [1806, 1806, 1021, 1021, 1806],\n",
       "        [1806, 1021, 1806, 1021, 1806],\n",
       "        [1021, 1021, 1806, 1806, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1738, 1021]]], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs,\n",
    "                       feed_dict={\n",
    "                           source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]],\n",
    "                           emo_cat: [1, 2, 3],\n",
    "                       })\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 460,  592,  961, 1845, 1198],\n",
       "        [1917, 1917, 1917, 1917, 1917],\n",
       "        [1917, 1917, 1917, 1429, 1917],\n",
       "        [1917, 1917, 1917, 1917, 1429],\n",
       "        [1917, 1917, 1917, 1917, 1917],\n",
       "        [1917, 1923, 1917, 1917, 1917],\n",
       "        [1923, 1923, 1917, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1923, 1923, 1923],\n",
       "        [1923, 1923, 1734, 1923, 1923]],\n",
       "\n",
       "       [[   3,   45,   45,  985,  894],\n",
       "        [  45,   45,   45,   45,   45],\n",
       "        [  45,   45,   45,   45, 1725],\n",
       "        [1725,   45, 1725,   45, 1042],\n",
       "        [1725, 1725, 1725, 1725, 1042],\n",
       "        [1042, 1042, 1042, 1725, 1725],\n",
       "        [1294, 1042, 1294, 1042, 1042],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294],\n",
       "        [1294, 1294, 1294, 1294, 1294]],\n",
       "\n",
       "       [[   3,  145,  592,  985,  454],\n",
       "        [ 454,  454,  454,  454,  454],\n",
       "        [1806, 1806, 1806, 1806, 1806],\n",
       "        [1806, 1806, 1021, 1806, 1806],\n",
       "        [1806, 1806, 1021, 1021, 1806],\n",
       "        [1806, 1021, 1806, 1021, 1806],\n",
       "        [1021, 1021, 1806, 1806, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1021, 1021],\n",
       "        [1021, 1021, 1021, 1738, 1021]]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECM loss\n",
    "def compute_ECM_loss(source_ids, target_ids, sequence_mask, choice_qs,\n",
    "                     embeddings, enc_num_layers, enc_num_units, enc_cell_type,\n",
    "                     enc_bidir, dec_num_layers, dec_num_units, dec_cell_type,\n",
    "                     state_pass, num_emo, emo_cat, emo_cat_units,\n",
    "                     emo_int_units, infer_batch_size, beam_size=None,\n",
    "                     max_iter=20, attn_num_units=128, l2_regularize=None,\n",
    "                     name=\"ECM\"):\n",
    "    \"\"\"\n",
    "    Creates an ECM model and returns CE loss plus regularization terms.\n",
    "        choice_qs: [batch_size, max_time], true choice btw generic/emo words\n",
    "        emo_cat: [batch_size], emotion categories of each target sequence\n",
    "\n",
    "    Returns\n",
    "        CE: cross entropy, used to compute perplexity\n",
    "        total_loss: objective of the entire model\n",
    "        train_outs: (cell, log_probs, alphas, final_int_mem_states)\n",
    "            alphas - predicted choices\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        cell, train_outputs, infer_outputs = build_ECM_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, \n",
    "            num_emo, emo_cat, emo_cat_units, emo_int_units,\n",
    "            state_pass, infer_batch_size, attn_num_units,\n",
    "            target_ids, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        g_logits, e_logits, alphas, int_M_emo = train_outputs\n",
    "        g_probs = tf.nn.softmax(g_logits) * (1 - alphas)\n",
    "        e_probs = tf.nn.softmax(e_logits) * alphas\n",
    "        train_log_probs = tf.log(g_probs + e_probs)\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            alphas = tf.squeeze(alphas, axis=-1)\n",
    "            choice_qs = tf.pad(choice_qs, [[0, 0], [0, 1]], constant_values=0)\n",
    "\n",
    "            # compute losses\n",
    "            g_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=g_logits, labels=final_ids) - tf.log(1 - alphas)\n",
    "\n",
    "            e_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=e_logits, labels=final_ids) - tf.log(alphas)\n",
    "\n",
    "            losses = g_losses * (1 - choice_qs) + e_losses * choice_qs\n",
    "\n",
    "            # alpha and internal memory regularizations\n",
    "            alpha_reg = tf.reduce_mean(choice_qs * -tf.log(alphas))\n",
    "            int_mem_reg = tf.reduce_mean(tf.norm(int_M_emo, axis=1))\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses) + alpha_reg + int_mem_reg\n",
    "\n",
    "            # prepare for perplexity computations\n",
    "            CE = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_log_probs, labels=final_ids)\n",
    "            CE = tf.boolean_mask(CE, sequence_mask)\n",
    "            CE = tf.reduce_sum(CE)\n",
    "\n",
    "            train_outs = (cell, train_log_probs, alphas, int_M_emo)\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_outs, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_outs, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_ECM_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "\n",
    "    num_emo = config[\"decoder\"][\"num_emotions\"]\n",
    "    emo_cat_units = config[\"decoder\"][\"emo_cat_units\"]\n",
    "    emo_int_units = config[\"decoder\"][\"emo_int_units\"]\n",
    "\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            num_emo, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "            beam_size, max_iter, attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_ECM_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    q_filename = train_config[\"train_choice_file\"]\n",
    "    c_filename = train_config[\"train_category_file\"]\n",
    "\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "    dev_q_filename = train_config[\"dev_choice_file\"]\n",
    "    dev_c_filename = train_config[\"dev_category_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, q_filename, c_filename,\n",
    "            s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    "            dev_q_filename, dev_c_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"EmotionChattingMachine\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"ECM\",\n",
    "        \"attn_num_units\": 128,\n",
    "        \"num_emotions\": 4,\n",
    "        \"emo_cat_units\": 32,\n",
    "        \"emo_int_units\": 64,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_category_file\": \"./example/dev_category.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./ECM_prediction.txt\",\n",
    "        \"choice_path\": \"./choice_pred.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_ECM/\",\n",
    "        \"restore_from\": \"./log_ECM/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"train_choice_file\": \"./example/train_choice.txt\",\n",
    "        \"train_category_file\": \"./example/train_category.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"dev_choice_file\": \"./example/dev_choice.txt\",\n",
    "        \"dev_category_file\": \"./example/dev_category.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./ECM_training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./ECM_perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./configs/config_ECM.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./configs/config_ECM.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "choice_qs = tf.placeholder(tf.float32, [None, None], name=\"choice\")\n",
    "emo_cat = tf.placeholder(tf.int32, [None], name=\"emotion_category\")\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " num_emo, emo_cat_units, emo_int_units,\n",
    " infer_batch_size, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_ECM_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "\n",
    "CE, loss, train_outs, infer_outputs = compute_ECM_loss(\n",
    "    source_ids, target_ids, sequence_mask, choice_qs, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    num_emo, emo_cat, emo_cat_units, emo_int_units, infer_batch_size,\n",
    "    beam_size, max_iter, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_ECM/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, q_filename, c_filename, s_max_leng, t_max_leng,\n",
    " dev_s_filename, dev_t_filename, dev_q_filename, dev_c_filename,\n",
    " loss_fig, perp_fig) = get_ECM_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "\n",
    "choice_data = loadfile(q_filename, is_source=False, max_length=t_max_leng)\n",
    "choice_data[choice_data < 0] = 0\n",
    "choice_data = choice_data.astype(np.float32)\n",
    "\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "\n",
    "    dev_choice_data = loadfile(dev_q_filename, is_source=False,\n",
    "                               max_length=t_max_leng)\n",
    "    dev_choice_data[dev_choice_data < 0] = 0\n",
    "    dev_choice_data = dev_choice_data.astype(np.float32)\n",
    "\n",
    "    dev_category_data = pd.read_csv(\n",
    "        dev_c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 7.699737, perp: 1001.103, dev_prep: 1001.154, (0.692 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 20, loss = 6.088544, perp: 255.796, dev_prep: 313.471, (0.140 sec/step)\n",
      "step 40, loss = 5.811570, perp: 207.405, dev_prep: 253.993, (0.155 sec/step)\n",
      "step 60, loss = 5.969004, perp: 246.879, dev_prep: 232.285, (0.137 sec/step)\n",
      "step 80, loss = 5.766850, perp: 216.192, dev_prep: 212.783, (0.168 sec/step)\n",
      "step 100, loss = 5.430977, perp: 166.365, dev_prep: 182.756, (0.140 sec/step)\n",
      "step 120, loss = 5.265982, perp: 145.827, dev_prep: 152.430, (0.181 sec/step)\n",
      "step 140, loss = 5.006571, perp: 122.421, dev_prep: 150.477, (0.179 sec/step)\n",
      "step 160, loss = 5.010723, perp: 122.139, dev_prep: 113.025, (0.171 sec/step)\n",
      "step 180, loss = 4.899721, perp: 110.430, dev_prep: 141.709, (0.151 sec/step)\n",
      "step 200, loss = 5.100912, perp: 134.449, dev_prep: 115.227, (0.178 sec/step)\n",
      "step 220, loss = 4.691104, perp: 92.530, dev_prep: 129.496, (0.167 sec/step)\n",
      "step 240, loss = 4.937002, perp: 114.956, dev_prep: 113.872, (0.150 sec/step)\n",
      "step 260, loss = 4.856279, perp: 107.057, dev_prep: 129.500, (0.180 sec/step)\n",
      "step 280, loss = 4.799408, perp: 100.214, dev_prep: 115.655, (0.174 sec/step)\n",
      "step 300, loss = 5.004722, perp: 125.328, dev_prep: 112.744, (0.139 sec/step)\n",
      "step 320, loss = 4.769854, perp: 98.545, dev_prep: 134.430, (0.158 sec/step)\n",
      "step 340, loss = 4.686020, perp: 89.822, dev_prep: 107.845, (0.129 sec/step)\n",
      "step 360, loss = 4.796840, perp: 101.111, dev_prep: 108.778, (0.191 sec/step)\n",
      "step 380, loss = 4.797613, perp: 103.924, dev_prep: 117.129, (0.116 sec/step)\n",
      "step 400, loss = 4.541265, perp: 77.744, dev_prep: 98.955, (0.189 sec/step)\n",
      "step 420, loss = 4.662152, perp: 83.492, dev_prep: 79.235, (0.163 sec/step)\n",
      "step 440, loss = 4.590928, perp: 79.814, dev_prep: 88.259, (0.245 sec/step)\n",
      "step 460, loss = 4.625462, perp: 84.657, dev_prep: 89.350, (0.159 sec/step)\n",
      "step 480, loss = 4.547513, perp: 80.075, dev_prep: 91.353, (0.119 sec/step)\n",
      "step 500, loss = 4.409554, perp: 69.453, dev_prep: 75.877, (0.104 sec/step)\n",
      "step 520, loss = 4.478297, perp: 71.702, dev_prep: 75.566, (0.141 sec/step)\n",
      "step 540, loss = 4.451223, perp: 72.027, dev_prep: 82.185, (0.191 sec/step)\n",
      "step 560, loss = 4.430426, perp: 71.879, dev_prep: 73.120, (0.187 sec/step)\n",
      "step 580, loss = 4.533713, perp: 80.323, dev_prep: 74.539, (0.144 sec/step)\n",
      "step 600, loss = 4.375027, perp: 65.338, dev_prep: 79.310, (0.166 sec/step)\n",
      "step 620, loss = 4.387076, perp: 66.256, dev_prep: 70.803, (0.123 sec/step)\n",
      "step 640, loss = 4.375194, perp: 66.789, dev_prep: 66.430, (0.165 sec/step)\n",
      "step 660, loss = 4.294917, perp: 62.355, dev_prep: 63.207, (0.150 sec/step)\n",
      "step 680, loss = 4.256598, perp: 59.751, dev_prep: 77.890, (0.111 sec/step)\n",
      "step 700, loss = 4.408154, perp: 68.223, dev_prep: 61.046, (0.138 sec/step)\n",
      "step 720, loss = 4.261224, perp: 59.162, dev_prep: 65.097, (0.137 sec/step)\n",
      "step 740, loss = 4.416168, perp: 69.477, dev_prep: 69.445, (0.148 sec/step)\n",
      "step 760, loss = 4.187576, perp: 56.643, dev_prep: 53.893, (0.151 sec/step)\n",
      "step 780, loss = 4.347474, perp: 64.115, dev_prep: 62.243, (0.134 sec/step)\n",
      "step 800, loss = 4.235355, perp: 56.508, dev_prep: 58.674, (0.147 sec/step)\n",
      "step 820, loss = 4.276284, perp: 60.368, dev_prep: 59.504, (0.136 sec/step)\n",
      "step 840, loss = 4.279856, perp: 61.042, dev_prep: 55.132, (0.190 sec/step)\n",
      "step 860, loss = 4.166403, perp: 54.952, dev_prep: 58.762, (0.155 sec/step)\n",
      "step 880, loss = 4.109621, perp: 51.591, dev_prep: 60.861, (0.138 sec/step)\n",
      "step 900, loss = 4.248008, perp: 58.354, dev_prep: 64.265, (0.205 sec/step)\n",
      "step 920, loss = 4.116633, perp: 52.259, dev_prep: 53.967, (0.195 sec/step)\n",
      "step 940, loss = 4.188891, perp: 55.569, dev_prep: 57.105, (0.130 sec/step)\n",
      "step 960, loss = 4.075261, perp: 48.127, dev_prep: 52.925, (0.198 sec/step)\n",
      "step 980, loss = 4.076900, perp: 50.323, dev_prep: 51.372, (0.152 sec/step)\n",
      "step 1000, loss = 4.056123, perp: 48.271, dev_prep: 53.092, (0.113 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 1020, loss = 4.164908, perp: 55.740, dev_prep: 57.624, (0.133 sec/step)\n",
      "step 1040, loss = 4.041453, perp: 48.435, dev_prep: 53.907, (0.165 sec/step)\n",
      "step 1060, loss = 4.195495, perp: 56.334, dev_prep: 47.331, (0.135 sec/step)\n",
      "step 1080, loss = 4.102398, perp: 51.160, dev_prep: 49.366, (0.153 sec/step)\n",
      "step 1100, loss = 4.095388, perp: 48.160, dev_prep: 47.519, (0.179 sec/step)\n",
      "step 1120, loss = 4.040977, perp: 48.008, dev_prep: 46.709, (0.116 sec/step)\n",
      "step 1140, loss = 4.096312, perp: 50.893, dev_prep: 42.803, (0.179 sec/step)\n",
      "step 1160, loss = 3.904175, perp: 42.237, dev_prep: 60.174, (0.115 sec/step)\n",
      "step 1180, loss = 4.026818, perp: 47.326, dev_prep: 53.808, (0.157 sec/step)\n",
      "step 1200, loss = 3.871540, perp: 40.812, dev_prep: 45.325, (0.133 sec/step)\n",
      "step 1220, loss = 4.053095, perp: 48.898, dev_prep: 46.184, (0.139 sec/step)\n",
      "step 1240, loss = 3.886315, perp: 41.717, dev_prep: 52.489, (0.113 sec/step)\n",
      "step 1260, loss = 3.947883, perp: 44.034, dev_prep: 55.098, (0.197 sec/step)\n",
      "step 1280, loss = 3.848665, perp: 40.593, dev_prep: 42.191, (0.153 sec/step)\n",
      "step 1300, loss = 3.917185, perp: 43.991, dev_prep: 44.462, (0.139 sec/step)\n",
      "step 1320, loss = 3.851715, perp: 40.607, dev_prep: 45.294, (0.128 sec/step)\n",
      "step 1340, loss = 4.013631, perp: 46.035, dev_prep: 40.776, (0.128 sec/step)\n",
      "step 1360, loss = 3.899570, perp: 40.855, dev_prep: 44.350, (0.168 sec/step)\n",
      "step 1380, loss = 3.881748, perp: 41.380, dev_prep: 45.005, (0.128 sec/step)\n",
      "step 1400, loss = 3.959418, perp: 43.950, dev_prep: 42.757, (0.152 sec/step)\n",
      "step 1420, loss = 3.862342, perp: 41.435, dev_prep: 52.430, (0.178 sec/step)\n",
      "step 1440, loss = 3.795844, perp: 37.387, dev_prep: 40.548, (0.160 sec/step)\n",
      "step 1460, loss = 3.890837, perp: 41.741, dev_prep: 44.859, (0.109 sec/step)\n",
      "step 1480, loss = 3.795985, perp: 37.489, dev_prep: 39.143, (0.136 sec/step)\n",
      "step 1500, loss = 3.953565, perp: 44.554, dev_prep: 45.371, (0.112 sec/step)\n",
      "step 1520, loss = 3.908072, perp: 41.332, dev_prep: 45.914, (0.104 sec/step)\n",
      "step 1540, loss = 3.899162, perp: 42.378, dev_prep: 44.566, (0.141 sec/step)\n",
      "step 1560, loss = 3.842764, perp: 37.469, dev_prep: 40.103, (0.213 sec/step)\n",
      "step 1580, loss = 3.934090, perp: 42.362, dev_prep: 43.674, (0.165 sec/step)\n",
      "step 1600, loss = 3.770914, perp: 36.867, dev_prep: 41.286, (0.130 sec/step)\n",
      "step 1620, loss = 3.717126, perp: 34.712, dev_prep: 41.108, (0.146 sec/step)\n",
      "step 1640, loss = 3.902526, perp: 41.664, dev_prep: 36.271, (0.116 sec/step)\n",
      "step 1660, loss = 3.793515, perp: 37.211, dev_prep: 39.607, (0.165 sec/step)\n",
      "step 1680, loss = 3.822739, perp: 37.770, dev_prep: 39.677, (0.179 sec/step)\n",
      "step 1700, loss = 3.767973, perp: 36.518, dev_prep: 39.690, (0.130 sec/step)\n",
      "step 1720, loss = 3.819821, perp: 37.660, dev_prep: 38.348, (0.114 sec/step)\n",
      "step 1740, loss = 3.852336, perp: 39.162, dev_prep: 42.193, (0.191 sec/step)\n",
      "step 1760, loss = 3.862222, perp: 39.059, dev_prep: 39.088, (0.183 sec/step)\n",
      "step 1780, loss = 3.812925, perp: 38.072, dev_prep: 39.237, (0.158 sec/step)\n",
      "step 1800, loss = 3.780943, perp: 36.591, dev_prep: 36.255, (0.104 sec/step)\n",
      "step 1820, loss = 3.821132, perp: 38.721, dev_prep: 36.270, (0.138 sec/step)\n",
      "step 1840, loss = 3.767241, perp: 36.005, dev_prep: 34.847, (0.158 sec/step)\n",
      "step 1860, loss = 3.811955, perp: 37.923, dev_prep: 41.738, (0.100 sec/step)\n",
      "step 1880, loss = 3.882755, perp: 41.323, dev_prep: 37.102, (0.141 sec/step)\n",
      "step 1900, loss = 3.809468, perp: 37.790, dev_prep: 34.800, (0.149 sec/step)\n",
      "step 1920, loss = 3.665507, perp: 33.297, dev_prep: 37.769, (0.108 sec/step)\n",
      "step 1940, loss = 3.757852, perp: 35.592, dev_prep: 37.534, (0.120 sec/step)\n",
      "step 1960, loss = 3.668220, perp: 33.735, dev_prep: 39.531, (0.144 sec/step)\n",
      "step 1980, loss = 3.784302, perp: 36.265, dev_prep: 34.962, (0.168 sec/step)\n",
      "step 2000, loss = 3.705191, perp: 34.617, dev_prep: 37.801, (0.155 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 2020, loss = 3.783960, perp: 37.315, dev_prep: 37.927, (0.119 sec/step)\n",
      "step 2040, loss = 3.688097, perp: 32.940, dev_prep: 32.475, (0.127 sec/step)\n",
      "step 2060, loss = 3.645330, perp: 32.803, dev_prep: 37.918, (0.163 sec/step)\n",
      "step 2080, loss = 3.677071, perp: 33.027, dev_prep: 41.375, (0.159 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.870945, perp: 39.108, dev_prep: 38.147, (0.108 sec/step)\n",
      "step 2120, loss = 3.699479, perp: 33.001, dev_prep: 34.940, (0.116 sec/step)\n",
      "step 2140, loss = 3.546679, perp: 29.765, dev_prep: 41.696, (0.132 sec/step)\n",
      "step 2160, loss = 3.688624, perp: 32.998, dev_prep: 35.430, (0.150 sec/step)\n",
      "step 2180, loss = 3.653970, perp: 32.147, dev_prep: 35.292, (0.124 sec/step)\n",
      "step 2200, loss = 3.675508, perp: 33.649, dev_prep: 36.875, (0.184 sec/step)\n",
      "step 2220, loss = 3.661255, perp: 33.026, dev_prep: 38.315, (0.145 sec/step)\n",
      "step 2240, loss = 3.772642, perp: 36.513, dev_prep: 38.826, (0.186 sec/step)\n",
      "step 2260, loss = 3.770366, perp: 36.174, dev_prep: 42.257, (0.138 sec/step)\n",
      "step 2280, loss = 3.685226, perp: 32.979, dev_prep: 34.889, (0.192 sec/step)\n",
      "step 2300, loss = 3.710427, perp: 34.000, dev_prep: 34.165, (0.148 sec/step)\n",
      "step 2320, loss = 3.571474, perp: 30.001, dev_prep: 30.798, (0.139 sec/step)\n",
      "step 2340, loss = 3.680650, perp: 32.148, dev_prep: 38.162, (0.170 sec/step)\n",
      "step 2360, loss = 3.730306, perp: 35.162, dev_prep: 35.553, (0.116 sec/step)\n",
      "step 2380, loss = 3.687488, perp: 34.538, dev_prep: 34.995, (0.145 sec/step)\n",
      "step 2400, loss = 3.717639, perp: 35.702, dev_prep: 33.575, (0.157 sec/step)\n",
      "step 2420, loss = 3.662410, perp: 33.393, dev_prep: 39.896, (0.186 sec/step)\n",
      "step 2440, loss = 3.571920, perp: 29.963, dev_prep: 34.300, (0.135 sec/step)\n",
      "step 2460, loss = 3.490467, perp: 28.455, dev_prep: 32.044, (0.200 sec/step)\n",
      "step 2480, loss = 3.494628, perp: 28.386, dev_prep: 30.142, (0.143 sec/step)\n",
      "step 2500, loss = 3.692569, perp: 32.827, dev_prep: 28.673, (0.156 sec/step)\n",
      "step 2520, loss = 3.612405, perp: 31.525, dev_prep: 36.923, (0.151 sec/step)\n",
      "step 2540, loss = 3.603898, perp: 30.988, dev_prep: 30.768, (0.190 sec/step)\n",
      "step 2560, loss = 3.599064, perp: 31.022, dev_prep: 36.190, (0.112 sec/step)\n",
      "step 2580, loss = 3.660842, perp: 33.426, dev_prep: 33.197, (0.125 sec/step)\n",
      "step 2600, loss = 3.593538, perp: 30.684, dev_prep: 36.587, (0.209 sec/step)\n",
      "step 2620, loss = 3.656710, perp: 33.087, dev_prep: 36.242, (0.145 sec/step)\n",
      "step 2640, loss = 3.660941, perp: 32.539, dev_prep: 34.591, (0.087 sec/step)\n",
      "step 2660, loss = 3.636778, perp: 32.504, dev_prep: 33.034, (0.124 sec/step)\n",
      "step 2680, loss = 3.590771, perp: 31.597, dev_prep: 35.220, (0.156 sec/step)\n",
      "step 2700, loss = 3.557564, perp: 29.701, dev_prep: 30.372, (0.163 sec/step)\n",
      "step 2720, loss = 3.495646, perp: 27.959, dev_prep: 31.277, (0.158 sec/step)\n",
      "step 2740, loss = 3.575610, perp: 30.344, dev_prep: 34.090, (0.142 sec/step)\n",
      "step 2760, loss = 3.543175, perp: 28.998, dev_prep: 29.573, (0.211 sec/step)\n",
      "step 2780, loss = 3.539986, perp: 28.759, dev_prep: 32.414, (0.117 sec/step)\n",
      "step 2800, loss = 3.634820, perp: 33.119, dev_prep: 33.049, (0.109 sec/step)\n",
      "step 2820, loss = 3.683141, perp: 32.730, dev_prep: 29.249, (0.148 sec/step)\n",
      "step 2840, loss = 3.526898, perp: 29.838, dev_prep: 34.777, (0.156 sec/step)\n",
      "step 2860, loss = 3.590281, perp: 30.810, dev_prep: 30.885, (0.183 sec/step)\n",
      "step 2880, loss = 3.615052, perp: 31.506, dev_prep: 31.212, (0.150 sec/step)\n",
      "step 2900, loss = 3.741636, perp: 36.021, dev_prep: 30.888, (0.182 sec/step)\n",
      "step 2920, loss = 3.569084, perp: 29.663, dev_prep: 29.487, (0.191 sec/step)\n",
      "step 2940, loss = 3.523495, perp: 28.874, dev_prep: 31.762, (0.178 sec/step)\n",
      "step 2960, loss = 3.530619, perp: 30.105, dev_prep: 29.036, (0.163 sec/step)\n",
      "step 2980, loss = 3.478936, perp: 26.558, dev_prep: 34.158, (0.158 sec/step)\n",
      "step 3000, loss = 3.643614, perp: 31.504, dev_prep: 30.263, (0.126 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 3020, loss = 3.559291, perp: 29.277, dev_prep: 36.290, (0.176 sec/step)\n",
      "step 3040, loss = 3.545905, perp: 28.357, dev_prep: 27.294, (0.183 sec/step)\n",
      "step 3060, loss = 3.623517, perp: 31.385, dev_prep: 27.628, (0.116 sec/step)\n",
      "step 3080, loss = 3.351971, perp: 24.287, dev_prep: 29.555, (0.179 sec/step)\n",
      "step 3100, loss = 3.495818, perp: 28.041, dev_prep: 30.092, (0.120 sec/step)\n",
      "step 3120, loss = 3.471572, perp: 26.657, dev_prep: 26.233, (0.137 sec/step)\n",
      "step 3140, loss = 3.315949, perp: 23.749, dev_prep: 31.060, (0.185 sec/step)\n",
      "step 3160, loss = 3.553158, perp: 30.006, dev_prep: 29.066, (0.170 sec/step)\n",
      "step 3180, loss = 3.495111, perp: 27.971, dev_prep: 29.652, (0.206 sec/step)\n",
      "step 3200, loss = 3.409018, perp: 27.111, dev_prep: 30.015, (0.123 sec/step)\n",
      "step 3220, loss = 3.422822, perp: 25.841, dev_prep: 28.722, (0.146 sec/step)\n",
      "step 3240, loss = 3.358055, perp: 24.884, dev_prep: 28.099, (0.135 sec/step)\n",
      "step 3260, loss = 3.440890, perp: 26.642, dev_prep: 28.338, (0.161 sec/step)\n",
      "step 3280, loss = 3.560953, perp: 28.545, dev_prep: 27.415, (0.219 sec/step)\n",
      "step 3300, loss = 3.513951, perp: 29.651, dev_prep: 28.605, (0.134 sec/step)\n",
      "step 3320, loss = 3.556237, perp: 29.262, dev_prep: 30.353, (0.149 sec/step)\n",
      "step 3340, loss = 3.583241, perp: 30.608, dev_prep: 27.780, (0.208 sec/step)\n",
      "step 3360, loss = 3.462825, perp: 27.635, dev_prep: 30.327, (0.148 sec/step)\n",
      "step 3380, loss = 3.416420, perp: 26.004, dev_prep: 27.349, (0.134 sec/step)\n",
      "step 3400, loss = 3.479966, perp: 26.046, dev_prep: 26.741, (0.186 sec/step)\n",
      "step 3420, loss = 3.386009, perp: 24.762, dev_prep: 28.001, (0.176 sec/step)\n",
      "step 3440, loss = 3.364197, perp: 25.294, dev_prep: 27.112, (0.120 sec/step)\n",
      "step 3460, loss = 3.523543, perp: 28.280, dev_prep: 27.879, (0.144 sec/step)\n",
      "step 3480, loss = 3.332986, perp: 24.082, dev_prep: 28.132, (0.143 sec/step)\n",
      "step 3500, loss = 3.429308, perp: 26.788, dev_prep: 24.875, (0.188 sec/step)\n",
      "step 3520, loss = 3.406567, perp: 25.595, dev_prep: 26.734, (0.111 sec/step)\n",
      "step 3540, loss = 3.494677, perp: 27.505, dev_prep: 27.962, (0.155 sec/step)\n",
      "step 3560, loss = 3.502362, perp: 28.080, dev_prep: 26.270, (0.124 sec/step)\n",
      "step 3580, loss = 3.316983, perp: 23.467, dev_prep: 30.670, (0.198 sec/step)\n",
      "step 3600, loss = 3.384675, perp: 25.124, dev_prep: 27.070, (0.174 sec/step)\n",
      "step 3620, loss = 3.342047, perp: 24.359, dev_prep: 29.011, (0.151 sec/step)\n",
      "step 3640, loss = 3.366817, perp: 25.084, dev_prep: 27.067, (0.175 sec/step)\n",
      "step 3660, loss = 3.336539, perp: 24.070, dev_prep: 27.871, (0.139 sec/step)\n",
      "step 3680, loss = 3.345669, perp: 24.358, dev_prep: 26.236, (0.159 sec/step)\n",
      "step 3700, loss = 3.349782, perp: 24.711, dev_prep: 29.270, (0.148 sec/step)\n",
      "step 3720, loss = 3.327799, perp: 24.117, dev_prep: 30.066, (0.120 sec/step)\n",
      "step 3740, loss = 3.364646, perp: 25.141, dev_prep: 24.926, (0.109 sec/step)\n",
      "step 3760, loss = 3.288863, perp: 23.942, dev_prep: 24.203, (0.172 sec/step)\n",
      "step 3780, loss = 3.406192, perp: 25.605, dev_prep: 26.118, (0.134 sec/step)\n",
      "step 3800, loss = 3.205035, perp: 20.436, dev_prep: 27.206, (0.112 sec/step)\n",
      "step 3820, loss = 3.275164, perp: 24.639, dev_prep: 26.001, (0.152 sec/step)\n",
      "step 3840, loss = 3.310462, perp: 24.272, dev_prep: 27.151, (0.143 sec/step)\n",
      "step 3860, loss = 3.249839, perp: 22.304, dev_prep: 23.740, (0.189 sec/step)\n",
      "step 3880, loss = 3.347553, perp: 23.901, dev_prep: 26.190, (0.159 sec/step)\n",
      "step 3900, loss = 3.293625, perp: 23.168, dev_prep: 25.126, (0.179 sec/step)\n",
      "step 3920, loss = 3.381026, perp: 25.851, dev_prep: 25.704, (0.188 sec/step)\n",
      "step 3940, loss = 3.214682, perp: 21.054, dev_prep: 24.334, (0.151 sec/step)\n",
      "step 3960, loss = 3.344072, perp: 24.782, dev_prep: 23.203, (0.143 sec/step)\n",
      "step 3980, loss = 3.256413, perp: 21.878, dev_prep: 22.542, (0.147 sec/step)\n",
      "step 4000, loss = 3.224111, perp: 21.701, dev_prep: 24.107, (0.119 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n",
      "step 4020, loss = 3.185019, perp: 20.840, dev_prep: 26.642, (0.188 sec/step)\n",
      "step 4040, loss = 3.254172, perp: 22.726, dev_prep: 25.835, (0.160 sec/step)\n",
      "step 4060, loss = 3.287714, perp: 23.376, dev_prep: 24.521, (0.170 sec/step)\n",
      "step 4080, loss = 3.265814, perp: 22.777, dev_prep: 25.148, (0.145 sec/step)\n",
      "step 4100, loss = 3.269825, perp: 22.972, dev_prep: 25.228, (0.198 sec/step)\n",
      "step 4120, loss = 3.212846, perp: 21.433, dev_prep: 23.397, (0.162 sec/step)\n",
      "step 4140, loss = 3.287128, perp: 22.890, dev_prep: 24.808, (0.139 sec/step)\n",
      "step 4160, loss = 3.175178, perp: 21.334, dev_prep: 25.425, (0.178 sec/step)\n",
      "step 4180, loss = 3.311111, perp: 22.774, dev_prep: 21.677, (0.209 sec/step)\n",
      "step 4200, loss = 3.144721, perp: 20.039, dev_prep: 19.768, (0.144 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.245541, perp: 21.472, dev_prep: 23.731, (0.113 sec/step)\n",
      "step 4240, loss = 3.219932, perp: 21.743, dev_prep: 23.509, (0.154 sec/step)\n",
      "step 4260, loss = 3.108898, perp: 19.896, dev_prep: 22.387, (0.143 sec/step)\n",
      "step 4280, loss = 3.131690, perp: 20.354, dev_prep: 22.129, (0.168 sec/step)\n",
      "step 4300, loss = 3.205822, perp: 21.086, dev_prep: 25.032, (0.181 sec/step)\n",
      "step 4320, loss = 3.126025, perp: 19.267, dev_prep: 21.967, (0.124 sec/step)\n",
      "step 4340, loss = 3.173150, perp: 20.922, dev_prep: 21.225, (0.172 sec/step)\n",
      "step 4360, loss = 3.270586, perp: 22.512, dev_prep: 22.000, (0.169 sec/step)\n",
      "step 4380, loss = 3.084371, perp: 18.856, dev_prep: 22.714, (0.158 sec/step)\n",
      "step 4400, loss = 3.104808, perp: 20.260, dev_prep: 24.818, (0.154 sec/step)\n",
      "step 4420, loss = 3.080061, perp: 18.427, dev_prep: 20.812, (0.113 sec/step)\n",
      "step 4440, loss = 3.200763, perp: 21.624, dev_prep: 23.672, (0.091 sec/step)\n",
      "step 4460, loss = 3.175321, perp: 20.647, dev_prep: 24.249, (0.151 sec/step)\n",
      "step 4480, loss = 3.163862, perp: 20.939, dev_prep: 20.073, (0.214 sec/step)\n",
      "step 4500, loss = 3.282511, perp: 23.134, dev_prep: 21.966, (0.163 sec/step)\n",
      "step 4520, loss = 3.125983, perp: 20.098, dev_prep: 20.063, (0.156 sec/step)\n",
      "step 4540, loss = 3.022717, perp: 18.894, dev_prep: 22.171, (0.152 sec/step)\n",
      "step 4560, loss = 3.248476, perp: 22.875, dev_prep: 21.062, (0.159 sec/step)\n",
      "step 4580, loss = 3.042267, perp: 18.713, dev_prep: 21.030, (0.117 sec/step)\n",
      "step 4600, loss = 3.042590, perp: 18.400, dev_prep: 20.802, (0.172 sec/step)\n",
      "step 4620, loss = 3.115970, perp: 19.664, dev_prep: 20.895, (0.128 sec/step)\n",
      "step 4640, loss = 3.081037, perp: 19.347, dev_prep: 21.656, (0.107 sec/step)\n",
      "step 4660, loss = 3.121806, perp: 20.045, dev_prep: 18.967, (0.118 sec/step)\n",
      "step 4680, loss = 3.011758, perp: 17.721, dev_prep: 20.593, (0.153 sec/step)\n",
      "step 4700, loss = 3.099985, perp: 19.465, dev_prep: 20.889, (0.134 sec/step)\n",
      "step 4720, loss = 3.059587, perp: 19.720, dev_prep: 19.528, (0.163 sec/step)\n",
      "step 4740, loss = 3.024031, perp: 17.605, dev_prep: 21.918, (0.108 sec/step)\n",
      "step 4760, loss = 3.009457, perp: 17.930, dev_prep: 18.879, (0.135 sec/step)\n",
      "step 4780, loss = 3.094502, perp: 19.454, dev_prep: 20.972, (0.190 sec/step)\n",
      "step 4800, loss = 3.140090, perp: 20.639, dev_prep: 19.962, (0.168 sec/step)\n",
      "step 4820, loss = 3.056027, perp: 18.357, dev_prep: 20.166, (0.104 sec/step)\n",
      "step 4840, loss = 3.102206, perp: 20.122, dev_prep: 20.900, (0.175 sec/step)\n",
      "step 4860, loss = 3.057509, perp: 19.028, dev_prep: 19.564, (0.165 sec/step)\n",
      "step 4880, loss = 3.070925, perp: 18.817, dev_prep: 18.645, (0.121 sec/step)\n",
      "step 4900, loss = 3.023117, perp: 18.235, dev_prep: 16.943, (0.132 sec/step)\n",
      "step 4920, loss = 3.017339, perp: 18.440, dev_prep: 20.477, (0.163 sec/step)\n",
      "step 4940, loss = 2.877388, perp: 15.566, dev_prep: 19.628, (0.111 sec/step)\n",
      "step 4960, loss = 2.848274, perp: 15.526, dev_prep: 19.136, (0.160 sec/step)\n",
      "step 4980, loss = 3.001612, perp: 18.083, dev_prep: 19.226, (0.127 sec/step)\n",
      "Storing checkpoint to ./log_ECM/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        choice_batch = choice_data[rand_indexes]\n",
    "        emotions = category_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            choice_qs: choice_batch,\n",
    "            emo_cat: emotions,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    choice_qs: dev_choice_data[dev_inds],\n",
    "                    emo_cat: dev_category_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Total loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"Perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check trained ECM model: internal memory and external choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norms of final-state internal memory:\n",
      " [1.70996273e-07 8.38531378e-08 3.36476774e-07 1.00261396e-07\n",
      " 7.53778764e-08 4.75606384e-07 8.84938061e-08 1.68322856e-07\n",
      " 2.69895480e-07 1.36910259e-07 1.45936724e-07 8.54624034e-07\n",
      " 5.21416439e-07 9.20925416e-08 1.08271102e-07 1.41317628e-06\n",
      " 5.27229993e-08 3.23120133e-07 3.55641632e-08 2.61158686e-08\n",
      " 3.51025591e-07 6.90607251e-08 1.19021529e-07 1.16351302e-07\n",
      " 2.47459781e-07 8.44490629e-08 2.65242164e-07 8.12769727e-08\n",
      " 4.90236005e-07 9.36149362e-08 7.04381193e-08 2.46980846e-07\n",
      " 8.85340370e-08 4.09272104e-07 5.48042999e-06 5.88493485e-08\n",
      " 2.02280177e-07 1.47491619e-06 6.39969784e-08 1.70224567e-07\n",
      " 7.54834986e-08 1.94278044e-07 1.70826738e-07 4.47285515e-07\n",
      " 6.81168444e-07 2.57398540e-07 3.98460287e-07 7.55414860e-08\n",
      " 2.06590315e-07 1.15204237e-07 6.39717683e-08 6.76679193e-08\n",
      " 2.74721401e-07 1.37139523e-06 1.61020822e-07 5.70227826e-07\n",
      " 5.96142584e-08 4.64516830e-07 3.43496289e-08 3.29081686e-08\n",
      " 1.34587424e-07 6.30477146e-08 6.57599557e-08 2.07110617e-07]\n"
     ]
    }
   ],
   "source": [
    "(cell, train_log_probs, alphas, int_M_emo) = train_outs\n",
    "\n",
    "M_norms = sess.run(tf.norm(int_M_emo, axis=1), feed_dict)\n",
    "print(\"Norms of final-state internal memory:\\n\", M_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x265.846 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA60AAAERCAYAAACQMkH4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAF1FJREFUeJzt3Xm0ZWV5J+DfaxWCiENUNDIoJkHFIZRaQdPSGocWtI2YdIu40tFMoh0TzYrdJsF0gyYOnY5j2okoiwwqMRqVqN0lEic0IiCIA5GmEYPMg4gjQ/n2H+cUXK5VcE7VrXP2Pfd51rrr7vl7b9VXh/vj2/vb1d0BAACAIbrdvAsAAACAbRFaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAAaiql5dVafv4DV2q6quqqeuVF0AME9CKwALZRzYbu3r+DnWdnhVfaKqvl1V362qs6vqZVV1j5Vqo7t/mOTeSU5aqWsCwDwJrQAsmnsv+XruVra9aGsnVdUuO7OoqnpNkncl+XySJyd5cJLfT3JAkt9cyba6+9Luvm4lrwkA8yK0ArBQxoHt0u6+NMk1y7d197er6oHjUddnVNUnq+qHSZ5TVc+vqiuXXq+qDh0fu8eSbY+pqlOq6gdVdWFV/cXS/ctV1WMyCqgv7O6XdPdnu/sb3f2x7j48yduWHf/sqvp6VV1bVe+tqp9Ysm9dVb28qr5ZVddV1VlV9ZQl+3/s9uCq2reqTqiqq6rq+1V1RlUdvGT/L1fVmVX1w6o6v6qOWRriq+qZVfXl8c97dVV9vKruPtVfDABsJ6EVgLXs1Ulel9Fo50cmOaGqHpHkfyd5T5KHJjk8yaOSvPVWTvuVJN/KsnC6RXdfs2T1AUl+cfz1lCQ/n+SYJftfkuSFSV6c5GeTbErywao6YBv13jnJp5P8ZJKnjc95VZIa739akuMy+nN4UJIjk/xqkqPH+++b5J3jn++AJI9NcsKt/KwAsKLWz7sAAJij13b3B7asVNUk5/xBkuO7+43j9fOq6neT/HNV/XZ3X7uVc/ZP8n+7e/OEdf1ad39vXNNxSX5pyb7/kuSV3f13W+qpqsdmFGJ/ayvXek6SuyZ5+pJwfN6S/X+c5BXd/dfj9fOr6qVJ3jzet3dG/5P7vePR6yT50oQ/BwDsMKEVgLVse2bqfUSSfarqOUu2bUm7P53kzK2cM1EaHjt/S2AduzjJPZOkqu6Z5G5JPrPsnFOS/JttXO9hSc5YNpqb8fVqvP+hVXX0kl23S3KH8W3Jp2U0Uvu1qvpoRhM8va+7r5riZwKA7Sa0ArCWfW/Z+o/y4wFz+QRNt0vypoxGIpe7cBvtnJvkGVW1boLR1huWrXcme5ynJzhmuRpf+4+TfHAr+6/t7s1V9biMblN+UpL/nOTVVfXo7j5nO9oEgKl4phUAbnZFkrtW1W5Ltm1YdswXkjy4u8/byte2Zux9V5KfSPK8re2sqrtOUlx3X57kqiSPXrbr4CRf3cZpZyZ5+Nba6O4fJTkryf238fNs3nJcd3+mu4/OaKT5W0meMUnNALCjjLQCwM0+m+T6JK+qqjdlFNCeu+yYVyb5bFW9MaMJjL6X0QRFh3T3C7Z20e7+5Pj4N4wnNvpARrf9/nRGEx+dmeR/TFjjnyc5qqq+nuSLSX5jXOevb+P4v07yX5N8YPys6iVJDkxyZXd/OsnLkryvqi5K8r6MRpsfmmRDdx9VVf82o1B8UpLLk/xcRq8O2lZIBoAVZaQVAMa6+7Ikz85o5t4vjZf/+7JjzshoBt0DMnqW9Mwkf5rk0tyK7n7R+Ho/n9GMv19N8oYkX0vyl1OU+T+TvDHJ65N8OaN3vj59W7fqdve3kzwmyZUZzZD8pSQvzSicprtPTHJYkkMzesb3cxlN9vSN8SWuSfIL43PPzWjm4Zd293unqBkAtlt1b88jMMNXVYdm9MvAuiRv7+5Xz7kk2C5VdUGS7yTZnOTG7t4434rgto1nvH1qksu7+yHjbXdL8ndJ9ktyQZLDu/tb86oRJrGNvnxMRiPwV4wPO6q7J3plEsxDVe2b0V0X98ro+fdju/sNPpdZLRZypLWq1mU0ScaTM3rn3LOq6kHzrQp2yOO6e4PAyipyfEYjd0v9YZKTu3v/JCeP12Hojs+P9+Uked34c3mDwMoqcGOSF3f3gzJ6r/QLxr8b+1xmVVjI0JrkoCTndff53X19Ri9BP2zONQGsGd39qSRXL9t8WJK/Gi//VZKnz7Qo2A7b6MuwqnT3Jd39hfHyd5Kck9E7mH0usyosamjdO7d87cA3x9tgNeokH62qM6rqyHkXAzvgXt19yXj50oxuU4PV6neq6uyqOm78PltYFapqv4zez3xqfC6zSixqaIVFcnB3Pzyj291fUFWPmXdBsKN6NKHCYk6qwFrwloxmft6Q0WzMr5lvOTCZqtojo1nCf6+7r126z+cyQ7aoofWiJPsuWd9nvA1Wne6+aPz98iTvz+j2d1iNLquqeyfJ+Pvlc64Htkt3X9bdm8fvuf3L+FxmFaiqXTIKrO/s7n8Yb/a5zKqwqKH1tCT7V9X9qur2SY5IcuKca4KpVdUdq+pOW5aTPCmjV1zAanRikueMl5+T5INzrAW225Zf8sd+KT6XGbiqqiTvSHJOd792yS6fy6wKi/zKm6dk9A67dUmO6+5XzLkkmFpV/VRGo6tJsj7Ju/RlVoOqendG7/a8R5LLkhyd5ANJ3pPkPhm9A/Tw7jbBDYO2jb78CxndGtwZvSbkeUueC4TBqaqDk3w6o/c0/2i8+aiMnmv1uczgLWxoBQAAYPVb1NuDAQAAWABCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWAsdWqvqyHnXACtBX2ZR6MssAv2YRaEvs1osdGhN4h8ii0JfZlHoyywC/ZhFoS+zKix6aAUAAGAVq+6edw1bdfvatXfLHXfoGjfkuuySXVeoItg+9//Z7+/wNa64anP2vPu6Hb7OuWfvvsPXgB3hc5lFoB+zKPRl5umH+V6u7+tqkmPX7+xittduuWMeWU+YdxmwwzZtOmveJdzkkL02zLsEAADIqX3yxMe6PRgAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABismYXWqjq0qr5WVedV1R/Oql0AAABWr5mE1qpal+RNSZ6c5EFJnlVVD5pF2wAAAKxesxppPSjJed19fndfn+SEJIfNqG0AAABWqVmF1r2TXLhk/ZvjbbdQVUdW1elVdfoNuW5GpQEAADBUg5qIqbuP7e6N3b1xl+w673IAAACYs1mF1ouS7LtkfZ/xNgAAANimWYXW05LsX1X3q6rbJzkiyYkzahsAAIBVav0sGunuG6vqd5JsSrIuyXHd/ZVZtA0AAMDqNZPQmiTd/ZEkH5lVewAAAKx+g5qICQAAAJYSWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGKzq7nnXsFUbD9ytP79p33mXcZND9tow7xIA4DZtuviseZdwC/77CcDWnNon59q+uiY51kgrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgzWT0FpVx1XV5VX15Vm0BwAAwGKY1Ujr8UkOnVFbAAAALIiZhNbu/lSSq2fRFgAAAIvDM60AAAAM1qBCa1UdWVWnV9XpV1y1ed7lAAAAMGeDCq3dfWx3b+zujXvefd28ywEAAGDOBhVaAQAAYKlZvfLm3Un+OckDquqbVfWbs2gXAACA1W39LBrp7mfNoh0AAAAWi9uDAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGKzq7nnXsFV3rrv1I+sJ8y6DCWy6+Kx5l3ALh+y1Yd4lADuZzx0AWN1O7ZNzbV9dkxxrpBUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABisiUNrVT2rqg4YLz+gqj5VVR+vqgfuvPIAAABYy6YZaf3TJFePl/88yeeTfDLJm2/rxKradxxwv1pVX6mqF01fKgAAAGvN+imO3bO7L6uq3ZIcnOQ/JrkhyZUTnHtjkhd39xeq6k5Jzqiqk7r7q9OXDAAAwFoxTWi9oqp+JslDk5zW3ddV1e5J6rZO7O5LklwyXv5OVZ2TZO8kQisAAADbNE1o/ZMkZyTZnOSZ421PTPLFaRqsqv2SPCzJqdOcBwAAwNozcWjt7uOr6j3j5e+PN38uyRGTXqOq9kjyviS/193XbmX/kUmOTJLdsvuklwUAAGBBTfvKmzsk+Q9V9ZLx+vpMGHyrapeMAus7u/sftnZMdx/b3Ru7e+Mu2XXK0gAAAFg007zy5rFJvpbkV5L8t/Hm/ZO8ZYJzK8k7kpzT3a/djjoBAABYg6YZaX19kmd296EZzQacjJ5LPWiCcx+d5FeTPL6qzhp/PWW6UgEAAFhrppmIab/uPnm83OPv109yje4+JRPMMgwAAABLTTPS+tWqOmTZticm+dIK1gMAAAA3mWak9cVJPlRVH05yh6p6W5JfTHLYTqkMAACANW/ikdbu/lySA5N8JclxSb6e5KDuPm0n1QYAAMAaN81Ia7r7oiR/tpNqAQAAgFu41dBaVX+Tmydd2qbufvaKVQQAAABjtzXSet5MqgAAAICtuNXQ2t0vm1UhAAAAsNxUz7RW1eOTPCvJXkkuTnLCkne3AgAAwIqaePbgqnpxkhOSXJ3kw0muSvKu8XYAAABYcdOMtP5+ksd395e3bBhP1HRSktesdGEAAAAw8Ujr2PKJmc7PBLMLAwAAwPaYJrQek+QdVbV/Vd2hqu6f5NgkR1fV7bZ87ZQqAQAAWJOqe7KB0qr60ZLVTlJbWe/uXrcShd257taPrCesxKUWzqaLz5p3CbdwyF4b5l0CAGP+GwHAanBqn5xr++q67SOne6b1fttZDwAAAGyXiUNrd39jZxYCAAAAy00cWqvqLklemORhSfZYuq+7n7TCdQEAAMBUtwf/fZJ1Sd6f5Ac7pxwAAAC42TSh9VFJ7tHd1++sYgAAAGCpaV5Rc0qSB+6sQgAAAGC5aUZafy3JR6rq1CSXLd3R3S9fyaIAAAAgmS60viLJvkkuSHLnJdsne9ErAAAATGma0HpEkvt39yU7qxgAAABYappnWs9PcsPOKgQAAACWm2ak9W+SnFhVf5Eff6b1n1a0KgAAAMh0ofUF4++vXLa9k/zUypQDAAAAN5s4tHb3/XZmIQAAALDcNM+0AgAAwExNPNJaVXdOckySxya5R5Lasq+773Mb5+6W5FNJdh23+d7uPno76gUAAGANmWak9c1JHp7k5UnuluR3k/xrktdNcO51SR7f3Qcm2ZDk0Kp61JS1AgAAsMZMMxHTk5Ic0N1XVdXm7v5gVZ2e5B9zG8G1uzvJd8eru4y/ensKBgAAYO2YZqT1dkm+PV7+blXdJcklSX5mkpOral1VnZXk8iQndfepU1UKAADAmjNNaP1iRs+zJskpGd0u/JYk505ycndv7u4NSfZJclBVPWT5MVV1ZFWdXlWn35DrpigNAACARTRNaH1ukgvGyy9M8oMkd0ny7Gka7O5rknw8yaFb2Xdsd2/s7o27ZNdpLgsAAMACus3QWlWPqKqHdPf53f3/qmrPjJ5hPSij24X/dYJr7FlVdx0v3yHJv0vyLztWOgAAAItukpHW1yf5ySXrb09y/yRvS/LgJH82wTXuneTjVXV2ktMyeqb1Q1PWCgAAwBozyezBByT5dJKMR0ufnOQh3X1uVZ2Y5LNJfvvWLtDdZyd52A7WCgAAwBozyUjr+iTXj5cfleTS7j43Sbr7wiR33Um1AQAAsMZNElq/kuQZ4+Ujknxsy46q2js3vwYHAAAAVtQktwf/QZJ/rKq3Jtmc5OAl+56Z5DM7ozAAAAC4zdDa3adU1X0ymnzp3O7+zpLdH05yws4qDgAAgLVtkpHWjIPqGVvZ/rUVrwgAAADGJnmmFQAAAOZCaAUAAGCwhFYAAAAGS2gFAABgsIRWAAAABktoBQAAYLCEVgAAAAarunveNWzVxgN3689v2nfeZdzkkL02zLsEAGAHbbr4rHmXcBO/WwBr2al9cq7tq2uSY420AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIM109BaVeuq6syq+tAs2wUAAGB1mvVI64uSnDPjNgEAAFilZhZaq2qfJP8+ydtn1SYAAACr2yxHWl+f5CVJfrStA6rqyKo6vapOv+KqzbOrDAAAgEGaSWitqqcmuby7z7i147r72O7e2N0b97z7ulmUBgAAwIDNaqT10UmeVlUXJDkhyeOr6m9n1DYAAACr1ExCa3f/UXfv0937JTkiyT9193+aRdsAAACsXt7TCgAAwGCtn3WD3f2JJJ+YdbsAAACsPkZaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwVo/7wK25dyzd88he22Ydxk32XTxWfMu4SZD+nNJhvVnM0RD+/uC7eHf+a3z73zbhtZ3/F2xvYbUl4fWj4f0ZzM0Q/u7Wq2MtAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBY62fVUFVdkOQ7STYnubG7N86qbQAAAFanmYXWscd195UzbhMAAIBVyu3BAAAADNYsQ2sn+WhVnVFVR86wXQAAAFapWd4efHB3X1RV90xyUlX9S3d/aukB4zB7ZJLslt1nWBoAAABDNLOR1u6+aPz98iTvT3LQVo45trs3dvfGXbLrrEoDAABgoGYSWqvqjlV1py3LSZ6U5MuzaBsAAIDVa1a3B98ryfurakub7+ru/zOjtgEAAFilZhJau/v8JAfOoi0AAAAWh1feAAAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFhCKwAAAIMltAIAADBYQisAAACDJbQCAAAwWEIrAAAAgyW0AgAAMFjV3fOuYauq6ook39jBy9wjyZUrUA7Mm77MotCXWQT6MYtCX2ae7tvde05y4GBD60qoqtO7e+O864AdpS+zKPRlFoF+zKLQl1kt3B4MAADAYAmtAAAADNaih9Zj510ArBB9mUWxpvtyVR1VVW+fdx3ssDXdj1ko+jKrwkI/0woAs1RV312yunuS65JsHq8/r7vfOfuqAGB1E1oBYCeoqguS/FZ3f2zetQDAarbotwcDwGBU1TFV9bfj5f2qqqvq16vqwqr6VlU9v6p+rqrOrqprqup/LTv/N6rqnPGxm6rqvvP5SQBgdoRWAJivRybZP8kzk7w+yUuTPDHJg5McXlWPTZKqOizJUUl+OcmeST6d5N3zKBgAZkloBYD5+pPu/mF3fzTJ95K8u7sv7+6LMgqmDxsf9/wkr+ruc7r7xiSvTLLBaCsAi05oBYD5umzJ8g+2sr7HePm+Sd4wvm34miRXJ6kke8+kSgCYk/XzLgAAmMiFSV5hBmIA1hojrQCwOrw1yR9V1YOTpKruUlXPmHNNALDTGWkFgFWgu99fVXskOWH8HOu3k5yU5O/nWxkA7Fze0woAAMBguT0YAACAwRJaAQAAGCyhFQAAgMESWgEAABgsoRUAAIDBEloBAAAYLKEVAACAwRJaAQAAGCyhFQAAgMH6/1WWlVkGrF82AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x276.48 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# alphas (predicted choice) and true choices\n",
    "rand_indexes = np.random.choice(n_data, 6)\n",
    "source_batch = source_data[rand_indexes]\n",
    "target_batch = target_data[rand_indexes]\n",
    "emotions = category_data[rand_indexes]\n",
    "\n",
    "choice_preds = sess.run(alphas,\n",
    "                        feed_dict={\n",
    "                            source_ids: source_batch,\n",
    "                            target_ids: target_batch,\n",
    "                            emo_cat: emotions})\n",
    "choice_batch = choice_data[rand_indexes]\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(choice_preds)\n",
    "plt.title(\"Predicted Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.savefig(\"./predict_choice\")\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(choice_batch)\n",
    "plt.title(\"True Choices\", fontsize=14)\n",
    "plt.xlabel(\"Time\", fontsize=12)\n",
    "plt.ylabel(\"Samples\", fontsize=12)\n",
    "plt.savefig(\"./true_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "c_filename = config[\"inference\"][\"infer_category_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "category_data = pd.read_csv(\n",
    "    c_filename, header=None, index_col=None, dtype=int)[0].values\n",
    "\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "category_data = np.concatenate((category_data, np.zeros(n_pad)))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "    batch_cat = category_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs,\n",
    "                      feed_dict={source_ids: batch, emo_cat: batch_cat})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "choice_pred = (final_result >= vocab_size).astype(np.int)\n",
    "final_result[final_result >= vocab_size] -= (vocab_size + embed_shift)\n",
    "\n",
    "# transform to output format\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "choice_pred = choice_pred.astype(str).tolist()\n",
    "choice_pred = list(map(lambda t: \" \".join(t), choice_pred))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "\n",
    "cdf = pd.DataFrame(data={\"0\": choice_pred})\n",
    "cdf.to_csv(config[\"inference\"][\"choice_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.577753985556704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "q = dev_choice_data[random_indexes]\n",
    "c = dev_category_data[random_indexes]\n",
    "\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    choice_qs: q,\n",
    "    emo_cat: c,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
