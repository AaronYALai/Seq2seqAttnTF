{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        # transpose to [batch_size, num_units, max_time]\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        \"\"\"\n",
    "        if self._dec_init_states is None:\n",
    "            attn_state_0 = None\n",
    "\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "            h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "            context_0 = self._compute_context(h_0)\n",
    "            h_0 = context_0 * 0\n",
    "            attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECMWrapper(RNNCell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, time_major=False, dtype=tf.float32,\n",
    "                  forget_bias=1.0, name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        mode: train or infer\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            # need batch-major\n",
    "            if time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=False,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099916, 0.00099655, 0.00099818, ..., 0.00099683,\n",
       "         0.00099601, 0.00099499],\n",
       "        [0.00099954, 0.00099693, 0.0009984 , ..., 0.00099637,\n",
       "         0.00099578, 0.00099362],\n",
       "        [0.00099893, 0.00099769, 0.00099811, ..., 0.00099595,\n",
       "         0.00099601, 0.00099298],\n",
       "        [0.00099781, 0.00099862, 0.00099754, ..., 0.00099569,\n",
       "         0.00099649, 0.00099301]],\n",
       "\n",
       "       [[0.00099916, 0.00099655, 0.00099818, ..., 0.00099683,\n",
       "         0.00099601, 0.00099499],\n",
       "        [0.00099956, 0.00099695, 0.00099837, ..., 0.00099637,\n",
       "         0.00099581, 0.00099363],\n",
       "        [0.00099957, 0.00099771, 0.00099874, ..., 0.00099582,\n",
       "         0.00099662, 0.00099286],\n",
       "        [0.00099955, 0.00099741, 0.00099847, ..., 0.00099578,\n",
       "         0.00099733, 0.00099251]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=True, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-2.81072280e-04, -3.39810690e-03, -1.50790578e-03, ...,\n",
       "          -2.27342849e-03,  5.24411816e-03,  1.72508752e-03],\n",
       "         [-2.81072280e-04, -3.39810690e-03, -1.50790578e-03, ...,\n",
       "          -2.27342849e-03,  5.24411816e-03,  1.72508752e-03],\n",
       "         [-2.81072280e-04, -3.39810690e-03, -1.50790578e-03, ...,\n",
       "          -2.27342849e-03,  5.24411816e-03,  1.72508752e-03],\n",
       "         [-2.81072280e-04, -3.39810690e-03, -1.50790578e-03, ...,\n",
       "          -2.27342849e-03,  5.24411816e-03,  1.72508752e-03],\n",
       "         [-2.81072280e-04, -3.39810690e-03, -1.50790578e-03, ...,\n",
       "          -2.27342849e-03,  5.24411816e-03,  1.72508752e-03]],\n",
       "\n",
       "        [[ 3.72944458e-04, -3.60391173e-03, -8.79623112e-04, ...,\n",
       "          -2.81932205e-03,  5.51768020e-03,  5.25486015e-04],\n",
       "         [ 3.72944458e-04, -3.60391173e-03, -8.79623112e-04, ...,\n",
       "          -2.81932205e-03,  5.51768020e-03,  5.25486015e-04],\n",
       "         [-3.53647687e-04, -3.74791259e-03, -6.64695166e-04, ...,\n",
       "          -2.40269187e-03,  5.36056515e-03,  1.39933091e-03],\n",
       "         [ 1.53288041e-04, -2.97727948e-03, -7.35462643e-04, ...,\n",
       "          -2.31781928e-03,  5.99017181e-03,  6.59490004e-04],\n",
       "         [ 1.53288041e-04, -2.97727948e-03, -7.35462643e-04, ...,\n",
       "          -2.31781928e-03,  5.99017181e-03,  6.59490004e-04]],\n",
       "\n",
       "        [[ 6.09590323e-04, -3.01492144e-03, -4.79898765e-04, ...,\n",
       "          -2.86762277e-03,  5.98819181e-03, -3.98342527e-04],\n",
       "         [-4.77309397e-04, -3.81242996e-03, -6.00772444e-04, ...,\n",
       "          -2.43241549e-03,  4.92884079e-03,  1.01644057e-03],\n",
       "         [-1.29155349e-04, -4.11279965e-03, -4.03439888e-04, ...,\n",
       "          -2.93316972e-03,  5.20439073e-03,  7.03768805e-04],\n",
       "         [ 6.09590323e-04, -3.01492144e-03, -4.79898765e-04, ...,\n",
       "          -2.86762277e-03,  5.98819181e-03, -3.98342527e-04],\n",
       "         [-1.29155349e-04, -4.11279965e-03, -4.03439888e-04, ...,\n",
       "          -2.93316972e-03,  5.20439073e-03,  7.03768805e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.13547596e-03, -9.36729251e-04,  4.92493680e-04, ...,\n",
       "          -5.85885532e-03,  7.25302706e-03,  3.26761510e-05],\n",
       "         [ 4.13547596e-03, -9.36729251e-04,  4.92493680e-04, ...,\n",
       "          -5.85885532e-03,  7.25302706e-03,  3.26761510e-05],\n",
       "         [ 4.13547596e-03, -9.36729251e-04,  4.92493680e-04, ...,\n",
       "          -5.85885532e-03,  7.25302706e-03,  3.26761510e-05],\n",
       "         [ 4.18362115e-03, -9.48000001e-04,  3.36221303e-04, ...,\n",
       "          -5.72309457e-03,  6.49188552e-03, -9.34004085e-04],\n",
       "         [ 4.13547596e-03, -9.36729251e-04,  4.92493680e-04, ...,\n",
       "          -5.85885532e-03,  7.25302706e-03,  3.26761510e-05]],\n",
       "\n",
       "        [[ 4.69437614e-03, -7.53732864e-04,  6.83376333e-04, ...,\n",
       "          -6.15916494e-03,  7.60109024e-03,  1.38671370e-04],\n",
       "         [ 4.69437614e-03, -7.53732864e-04,  6.83376333e-04, ...,\n",
       "          -6.15916494e-03,  7.60109024e-03,  1.38671370e-04],\n",
       "         [ 4.69437614e-03, -7.53732864e-04,  6.83376333e-04, ...,\n",
       "          -6.15916494e-03,  7.60109024e-03,  1.38671370e-04],\n",
       "         [ 4.69437614e-03, -7.53732864e-04,  6.83376333e-04, ...,\n",
       "          -6.15916494e-03,  7.60109024e-03,  1.38671370e-04],\n",
       "         [ 4.69437614e-03, -7.53732864e-04,  6.83376333e-04, ...,\n",
       "          -6.15916494e-03,  7.60109024e-03,  1.38671370e-04]],\n",
       "\n",
       "        [[ 5.18915942e-03, -6.86099404e-04,  8.84604407e-04, ...,\n",
       "          -6.44474104e-03,  7.79978000e-03,  1.45103200e-04],\n",
       "         [ 5.23823919e-03, -6.92289846e-04,  7.34150060e-04, ...,\n",
       "          -6.31013606e-03,  7.03472178e-03, -8.18992034e-04],\n",
       "         [ 4.92712855e-03, -1.03621813e-03,  9.05154797e-04, ...,\n",
       "          -6.41240040e-03,  7.00771529e-03, -2.51791673e-04],\n",
       "         [ 5.22351218e-03, -6.16984093e-04,  5.69186232e-04, ...,\n",
       "          -6.20434526e-03,  6.61888393e-03, -1.22040114e-03],\n",
       "         [ 4.27658577e-03, -1.01756665e-03,  5.77329483e-04, ...,\n",
       "          -6.31376309e-03,  6.59698667e-03, -9.87194944e-05]]],\n",
       "\n",
       "\n",
       "       [[[ 7.60902301e-04, -1.18476230e-04, -7.37661263e-04, ...,\n",
       "          -1.58489449e-03,  1.28345098e-03, -5.89154777e-04],\n",
       "         [ 7.60902301e-04, -1.18476230e-04, -7.37661263e-04, ...,\n",
       "          -1.58489449e-03,  1.28345098e-03, -5.89154777e-04],\n",
       "         [ 7.60902301e-04, -1.18476230e-04, -7.37661263e-04, ...,\n",
       "          -1.58489449e-03,  1.28345098e-03, -5.89154777e-04],\n",
       "         [ 7.60902301e-04, -1.18476230e-04, -7.37661263e-04, ...,\n",
       "          -1.58489449e-03,  1.28345098e-03, -5.89154777e-04],\n",
       "         [ 7.60902301e-04, -1.18476230e-04, -7.37661263e-04, ...,\n",
       "          -1.58489449e-03,  1.28345098e-03, -5.89154777e-04]],\n",
       "\n",
       "        [[ 8.40906054e-04, -6.26523164e-04, -8.04675161e-04, ...,\n",
       "          -1.79084309e-03,  2.04386981e-03, -1.73545664e-03],\n",
       "         [ 1.15059817e-03, -2.96468381e-04, -6.37254794e-04, ...,\n",
       "          -1.44960685e-03,  2.21451605e-03, -1.72710128e-03],\n",
       "         [ 1.18498551e-03, -9.27656074e-04, -6.10955118e-04, ...,\n",
       "          -2.28061737e-03,  2.33657542e-03, -2.04420066e-03],\n",
       "         [ 6.00228435e-04,  2.19689973e-04, -1.82661926e-04, ...,\n",
       "          -1.42469164e-03,  2.06816709e-03, -1.79370225e-03],\n",
       "         [ 8.40906054e-04, -6.26523164e-04, -8.04675161e-04, ...,\n",
       "          -1.79084309e-03,  2.04386981e-03, -1.73545664e-03]],\n",
       "\n",
       "        [[ 1.44013192e-03,  5.26393706e-05, -6.58306177e-04, ...,\n",
       "          -1.02049846e-03,  2.56296014e-03, -2.09895638e-03],\n",
       "         [ 1.63841201e-03,  1.08195236e-04, -5.27916127e-04, ...,\n",
       "          -1.25049357e-03,  2.89317826e-03, -1.71488873e-03],\n",
       "         [ 1.04050187e-03, -4.74175235e-04, -8.98293802e-04, ...,\n",
       "          -1.65970065e-03,  1.83875998e-03, -2.73862947e-03],\n",
       "         [ 1.60187250e-03, -7.04982784e-04, -9.95023525e-04, ...,\n",
       "          -1.24551088e-03,  2.28657830e-03, -2.78081628e-03],\n",
       "         [ 1.44013192e-03,  5.26393706e-05, -6.58306177e-04, ...,\n",
       "          -1.02049846e-03,  2.56296014e-03, -2.09895638e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.67794731e-03, -2.30550976e-03, -1.89215899e-03, ...,\n",
       "          -3.29351658e-03,  1.46517716e-03, -3.84654594e-03],\n",
       "         [ 4.89418954e-03, -2.51522986e-03, -1.97787955e-03, ...,\n",
       "          -3.29177082e-03,  1.52503094e-03, -3.80487274e-03],\n",
       "         [ 4.30967845e-03, -2.65833293e-03, -1.92338089e-03, ...,\n",
       "          -3.25607811e-03,  1.55031192e-03, -3.70893674e-03],\n",
       "         [ 4.89418954e-03, -2.51522986e-03, -1.97787955e-03, ...,\n",
       "          -3.29177082e-03,  1.52503094e-03, -3.80487274e-03],\n",
       "         [ 5.09900972e-03, -2.44911038e-03, -1.83792145e-03, ...,\n",
       "          -3.25543503e-03,  1.50407618e-03, -3.74441803e-03]],\n",
       "\n",
       "        [[ 5.62877115e-03, -3.09715606e-03, -1.65159418e-03, ...,\n",
       "          -3.88936186e-03,  2.03962135e-03, -3.52228805e-03],\n",
       "         [ 6.39728457e-03, -2.86298688e-03, -1.38837937e-03, ...,\n",
       "          -3.82918073e-03,  1.97928376e-03, -3.46554560e-03],\n",
       "         [ 4.84673074e-03, -3.30263795e-03, -1.73265941e-03, ...,\n",
       "          -3.88922077e-03,  2.08470458e-03, -3.48244328e-03],\n",
       "         [ 6.39728457e-03, -2.86298688e-03, -1.38837937e-03, ...,\n",
       "          -3.82918073e-03,  1.97928376e-03, -3.46554560e-03],\n",
       "         [ 5.62877115e-03, -3.09715606e-03, -1.65159418e-03, ...,\n",
       "          -3.88936186e-03,  2.03962135e-03, -3.52228805e-03]],\n",
       "\n",
       "        [[ 6.23733224e-03, -3.66759999e-03, -1.22297253e-03, ...,\n",
       "          -4.27705934e-03,  2.43186392e-03, -3.24209034e-03],\n",
       "         [ 6.88542798e-03, -3.42866126e-03, -8.05391930e-04, ...,\n",
       "          -4.15612338e-03,  2.38774158e-03, -3.09379678e-03],\n",
       "         [ 5.47061069e-03, -3.89647670e-03, -1.48128590e-03, ...,\n",
       "          -4.33705607e-03,  2.47845659e-03, -3.29981116e-03],\n",
       "         [ 6.81108423e-03, -3.17027187e-03, -1.07979262e-03, ...,\n",
       "          -4.19082679e-03,  1.92314025e-03, -3.22778267e-03],\n",
       "         [ 6.12228783e-03, -3.66089283e-03, -1.06492511e-03, ...,\n",
       "          -4.22046240e-03,  2.44404213e-03, -3.14473850e-03]]],\n",
       "\n",
       "\n",
       "       [[[-3.54377832e-03, -1.26343174e-03,  1.86504982e-03, ...,\n",
       "          -8.46705632e-04,  4.91619203e-03,  3.92157963e-04],\n",
       "         [-3.54377832e-03, -1.26343174e-03,  1.86504982e-03, ...,\n",
       "          -8.46705632e-04,  4.91619203e-03,  3.92157963e-04],\n",
       "         [-3.54377832e-03, -1.26343174e-03,  1.86504982e-03, ...,\n",
       "          -8.46705632e-04,  4.91619203e-03,  3.92157963e-04],\n",
       "         [-3.54377832e-03, -1.26343174e-03,  1.86504982e-03, ...,\n",
       "          -8.46705632e-04,  4.91619203e-03,  3.92157963e-04],\n",
       "         [-3.54377832e-03, -1.26343174e-03,  1.86504982e-03, ...,\n",
       "          -8.46705632e-04,  4.91619203e-03,  3.92157963e-04]],\n",
       "\n",
       "        [[-4.53494536e-03, -2.14120024e-03,  7.37491995e-04, ...,\n",
       "          -8.66019574e-04,  4.59905341e-03, -3.58462043e-04],\n",
       "         [-4.53494536e-03, -2.14120024e-03,  7.37491995e-04, ...,\n",
       "          -8.66019574e-04,  4.59905341e-03, -3.58462043e-04],\n",
       "         [-4.34515858e-03, -1.72830420e-03,  7.00796256e-04, ...,\n",
       "          -1.11049949e-03,  5.08421008e-03,  4.35428490e-04],\n",
       "         [-4.82209725e-03, -2.24934518e-03,  7.06511317e-04, ...,\n",
       "          -1.01343333e-03,  5.42250462e-03,  4.13671776e-04],\n",
       "         [-4.51537780e-03, -2.08500028e-03,  8.92448239e-04, ...,\n",
       "          -6.30431809e-04,  4.97451657e-03, -3.86408065e-04]],\n",
       "\n",
       "        [[-6.07878901e-03, -2.81597022e-03, -7.24813435e-05, ...,\n",
       "          -9.58111486e-04,  5.17007476e-03,  5.77376573e-04],\n",
       "         [-6.07878901e-03, -2.81597022e-03, -7.24813435e-05, ...,\n",
       "          -9.58111486e-04,  5.17007476e-03,  5.77376573e-04],\n",
       "         [-5.74634038e-03, -2.45849346e-03, -3.52502102e-04, ...,\n",
       "          -9.65530751e-04,  4.79496550e-03,  3.61876795e-04],\n",
       "         [-6.07878901e-03, -2.81597022e-03, -7.24813435e-05, ...,\n",
       "          -9.58111486e-04,  5.17007476e-03,  5.77376573e-04],\n",
       "         [-5.74634038e-03, -2.45849346e-03, -3.52502102e-04, ...,\n",
       "          -9.65530751e-04,  4.79496550e-03,  3.61876795e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.09967070e-03,  3.20883119e-03,  6.19408675e-05, ...,\n",
       "          -1.30183878e-03, -1.68739166e-03, -1.38606899e-03],\n",
       "         [-2.09967070e-03,  3.20883119e-03,  6.19408675e-05, ...,\n",
       "          -1.30183878e-03, -1.68739166e-03, -1.38606899e-03],\n",
       "         [-2.09967070e-03,  3.20883119e-03,  6.19408675e-05, ...,\n",
       "          -1.30183878e-03, -1.68739166e-03, -1.38606899e-03],\n",
       "         [-2.09967070e-03,  3.20883119e-03,  6.19408675e-05, ...,\n",
       "          -1.30183878e-03, -1.68739166e-03, -1.38606899e-03],\n",
       "         [-2.09967070e-03,  3.20883119e-03,  6.19408675e-05, ...,\n",
       "          -1.30183878e-03, -1.68739166e-03, -1.38606899e-03]],\n",
       "\n",
       "        [[-1.83064071e-03,  2.15622666e-03, -8.00435082e-04, ...,\n",
       "          -1.81930745e-03, -2.79454701e-03, -3.04601999e-04],\n",
       "         [-2.39014276e-03,  2.47005792e-03, -1.10649120e-03, ...,\n",
       "          -9.25718807e-04, -3.33613250e-03,  2.11204941e-04],\n",
       "         [-2.39014276e-03,  2.47005792e-03, -1.10649120e-03, ...,\n",
       "          -9.25718807e-04, -3.33613250e-03,  2.11204941e-04],\n",
       "         [-2.22161412e-03,  3.31269554e-03, -9.38144687e-04, ...,\n",
       "          -8.96121259e-04, -2.42724456e-03, -2.49560340e-04],\n",
       "         [-1.83467788e-03,  3.45571525e-03, -1.87480822e-04, ...,\n",
       "          -1.08427345e-03, -2.33512069e-03, -6.35309552e-04]],\n",
       "\n",
       "        [[-1.10358896e-03,  3.14544048e-03, -1.65047939e-04, ...,\n",
       "          -1.32509519e-03, -3.28578567e-03, -6.98571210e-04],\n",
       "         [-5.47459349e-04,  4.13366221e-03,  7.54247711e-04, ...,\n",
       "          -1.48303725e-03, -2.28835316e-03, -1.54280826e-03],\n",
       "         [-7.89802638e-04,  3.98797821e-03,  5.36029111e-04, ...,\n",
       "          -9.21144034e-04, -2.85727787e-03, -1.50079292e-03],\n",
       "         [-1.34824822e-03,  2.74195755e-03, -4.83689568e-04, ...,\n",
       "          -1.44180842e-03, -2.45512952e-03, -1.12280925e-03],\n",
       "         [-1.25915546e-03,  3.32992757e-03, -2.08766316e-04, ...,\n",
       "          -1.23898056e-03, -2.71725445e-03, -6.47965004e-04]]]],\n",
       "      dtype=float32), ids=array([[[165, 104, 165, 671, 671],\n",
       "        [671, 671, 671, 881, 881],\n",
       "        [671, 671, 881, 671, 881],\n",
       "        [806, 458, 881, 458, 881],\n",
       "        [806, 458, 806, 458, 458],\n",
       "        [806, 806, 806, 702, 806],\n",
       "        [806, 806, 806, 702, 147],\n",
       "        [806, 806, 806, 702, 147],\n",
       "        [806, 806, 806, 147, 806],\n",
       "        [806, 806, 806, 806, 806],\n",
       "        [806, 147, 293, 702,  64]],\n",
       "\n",
       "       [[371, 881, 371, 371, 324],\n",
       "        [371, 324, 888, 371, 371],\n",
       "        [888, 324, 888, 888, 888],\n",
       "        [888, 888, 324, 888, 888],\n",
       "        [888, 888, 888, 888, 888],\n",
       "        [888, 888, 123, 123, 475],\n",
       "        [475, 123, 123, 475, 475],\n",
       "        [475, 475, 123, 123, 123],\n",
       "        [475, 475, 475, 475, 475],\n",
       "        [475, 475, 475, 475, 475],\n",
       "        [475, 475, 475, 888, 133]],\n",
       "\n",
       "       [[415, 415, 611, 415, 611],\n",
       "        [671, 671, 201, 201, 415],\n",
       "        [611, 611, 481, 611, 481],\n",
       "        [611, 611, 611, 481, 611],\n",
       "        [611, 581, 581, 611, 671],\n",
       "        [581, 671, 581, 581, 611],\n",
       "        [581, 611, 581, 581, 581],\n",
       "        [581, 581, 611, 611, 581],\n",
       "        [581, 581, 581, 581, 581],\n",
       "        [518, 581, 581, 503, 375],\n",
       "        [518, 518, 903, 903, 903]]], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[165, 104, 165, 671, 671],\n",
       "        [671, 671, 671, 881, 881],\n",
       "        [671, 671, 881, 671, 881],\n",
       "        [806, 458, 881, 458, 881],\n",
       "        [806, 458, 806, 458, 458],\n",
       "        [806, 806, 806, 702, 806],\n",
       "        [806, 806, 806, 702, 147],\n",
       "        [806, 806, 806, 702, 147],\n",
       "        [806, 806, 806, 147, 806],\n",
       "        [806, 806, 806, 806, 806],\n",
       "        [806, 147, 293, 702,  64]],\n",
       "\n",
       "       [[371, 881, 371, 371, 324],\n",
       "        [371, 324, 888, 371, 371],\n",
       "        [888, 324, 888, 888, 888],\n",
       "        [888, 888, 324, 888, 888],\n",
       "        [888, 888, 888, 888, 888],\n",
       "        [888, 888, 123, 123, 475],\n",
       "        [475, 123, 123, 475, 475],\n",
       "        [475, 475, 123, 123, 123],\n",
       "        [475, 475, 475, 475, 475],\n",
       "        [475, 475, 475, 475, 475],\n",
       "        [475, 475, 475, 888, 133]],\n",
       "\n",
       "       [[415, 415, 611, 415, 611],\n",
       "        [671, 671, 201, 201, 415],\n",
       "        [611, 611, 481, 611, 481],\n",
       "        [611, 611, 611, 481, 611],\n",
       "        [611, 581, 581, 611, 671],\n",
       "        [581, 671, 581, 581, 611],\n",
       "        [581, 611, 581, 581, 581],\n",
       "        [581, 581, 611, 611, 581],\n",
       "        [581, 581, 581, 581, 581],\n",
       "        [518, 581, 581, 503, 375],\n",
       "        [518, 518, 903, 903, 903]]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 10,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": ECMWrapper,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.910902, perp: 999.369, dev_prep: 998.714, (0.405 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 5.605354, perp: 273.761, dev_prep: 329.932, (0.069 sec/step)\n",
      "step 40, loss = 7.399301, perp: 1509.882, dev_prep: 1527.500, (0.071 sec/step)\n",
      "step 60, loss = 5.440578, perp: 247.508, dev_prep: 240.896, (0.073 sec/step)\n",
      "step 80, loss = 5.206920, perp: 180.957, dev_prep: 192.151, (0.071 sec/step)\n",
      "step 100, loss = 5.185344, perp: 176.673, dev_prep: 163.548, (0.069 sec/step)\n",
      "step 120, loss = 4.926706, perp: 136.486, dev_prep: 147.021, (0.071 sec/step)\n",
      "step 140, loss = 4.867050, perp: 127.226, dev_prep: 139.108, (0.074 sec/step)\n",
      "step 160, loss = 4.960773, perp: 141.561, dev_prep: 135.956, (0.070 sec/step)\n",
      "step 180, loss = 4.844193, perp: 125.581, dev_prep: 134.816, (0.073 sec/step)\n",
      "step 200, loss = 5.041557, perp: 149.277, dev_prep: 132.492, (0.069 sec/step)\n",
      "step 220, loss = 4.839423, perp: 126.328, dev_prep: 135.981, (0.071 sec/step)\n",
      "step 240, loss = 5.047417, perp: 148.635, dev_prep: 130.384, (0.076 sec/step)\n",
      "step 260, loss = 4.878618, perp: 130.094, dev_prep: 128.164, (0.069 sec/step)\n",
      "step 280, loss = 4.776291, perp: 117.075, dev_prep: 124.891, (0.075 sec/step)\n",
      "step 300, loss = 4.776600, perp: 117.364, dev_prep: 121.847, (0.071 sec/step)\n",
      "step 320, loss = 4.729349, perp: 112.142, dev_prep: 119.023, (0.069 sec/step)\n",
      "step 340, loss = 4.956083, perp: 140.809, dev_prep: 118.136, (0.074 sec/step)\n",
      "step 360, loss = 4.786687, perp: 117.228, dev_prep: 116.342, (0.070 sec/step)\n",
      "step 380, loss = 4.955359, perp: 140.933, dev_prep: 133.824, (0.073 sec/step)\n",
      "step 400, loss = 4.743935, perp: 113.787, dev_prep: 116.400, (0.071 sec/step)\n",
      "step 420, loss = 4.644773, perp: 101.290, dev_prep: 109.144, (0.070 sec/step)\n",
      "step 440, loss = 4.540239, perp: 93.564, dev_prep: 101.311, (0.071 sec/step)\n",
      "step 460, loss = 4.609219, perp: 97.953, dev_prep: 95.860, (0.071 sec/step)\n",
      "step 480, loss = 4.600354, perp: 96.252, dev_prep: 93.761, (0.070 sec/step)\n",
      "step 500, loss = 4.558183, perp: 94.244, dev_prep: 90.211, (0.071 sec/step)\n",
      "step 520, loss = 4.585708, perp: 96.219, dev_prep: 90.178, (0.069 sec/step)\n",
      "step 540, loss = 4.561693, perp: 93.407, dev_prep: 86.509, (0.074 sec/step)\n",
      "step 560, loss = 4.369624, perp: 78.456, dev_prep: 83.201, (0.070 sec/step)\n",
      "step 580, loss = 4.456933, perp: 83.691, dev_prep: 79.056, (0.073 sec/step)\n",
      "step 600, loss = 4.443285, perp: 83.903, dev_prep: 78.694, (0.072 sec/step)\n",
      "step 620, loss = 4.154479, perp: 62.908, dev_prep: 76.566, (0.071 sec/step)\n",
      "step 640, loss = 4.353038, perp: 76.104, dev_prep: 75.050, (0.070 sec/step)\n",
      "step 660, loss = 4.381294, perp: 80.649, dev_prep: 75.254, (0.070 sec/step)\n",
      "step 680, loss = 4.200270, perp: 65.854, dev_prep: 71.662, (0.072 sec/step)\n",
      "step 700, loss = 4.324090, perp: 73.328, dev_prep: 72.004, (0.070 sec/step)\n",
      "step 720, loss = 4.381404, perp: 79.421, dev_prep: 70.649, (0.070 sec/step)\n",
      "step 740, loss = 4.341184, perp: 74.003, dev_prep: 68.873, (0.076 sec/step)\n",
      "step 760, loss = 4.277703, perp: 69.613, dev_prep: 67.914, (0.070 sec/step)\n",
      "step 780, loss = 4.151629, perp: 61.805, dev_prep: 65.373, (0.071 sec/step)\n",
      "step 800, loss = 4.243585, perp: 71.381, dev_prep: 70.846, (0.071 sec/step)\n",
      "step 820, loss = 4.038928, perp: 57.634, dev_prep: 66.780, (0.072 sec/step)\n",
      "step 840, loss = 4.068301, perp: 57.660, dev_prep: 63.413, (0.077 sec/step)\n",
      "step 860, loss = 4.192465, perp: 64.842, dev_prep: 61.658, (0.071 sec/step)\n",
      "step 880, loss = 4.278340, perp: 71.313, dev_prep: 60.928, (0.070 sec/step)\n",
      "step 900, loss = 4.177391, perp: 60.866, dev_prep: 61.105, (0.069 sec/step)\n",
      "step 920, loss = 4.130468, perp: 62.360, dev_prep: 60.165, (0.069 sec/step)\n",
      "step 940, loss = 3.997930, perp: 53.004, dev_prep: 58.059, (0.073 sec/step)\n",
      "step 960, loss = 3.936697, perp: 51.230, dev_prep: 58.982, (0.070 sec/step)\n",
      "step 980, loss = 4.368863, perp: 71.884, dev_prep: 62.788, (0.070 sec/step)\n",
      "step 1000, loss = 4.075142, perp: 58.282, dev_prep: 61.428, (0.073 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 4.065232, perp: 57.014, dev_prep: 61.725, (0.071 sec/step)\n",
      "step 1040, loss = 4.124703, perp: 61.810, dev_prep: 59.007, (0.071 sec/step)\n",
      "step 1060, loss = 4.222405, perp: 66.876, dev_prep: 57.608, (0.071 sec/step)\n",
      "step 1080, loss = 4.125537, perp: 60.063, dev_prep: 55.910, (0.070 sec/step)\n",
      "step 1100, loss = 4.044335, perp: 54.781, dev_prep: 54.999, (0.072 sec/step)\n",
      "step 1120, loss = 4.031745, perp: 55.745, dev_prep: 55.006, (0.072 sec/step)\n",
      "step 1140, loss = 3.961428, perp: 51.673, dev_prep: 54.330, (0.071 sec/step)\n",
      "step 1160, loss = 3.983302, perp: 54.065, dev_prep: 55.336, (0.073 sec/step)\n",
      "step 1180, loss = 3.970628, perp: 52.378, dev_prep: 54.044, (0.070 sec/step)\n",
      "step 1200, loss = 3.924171, perp: 50.485, dev_prep: 54.339, (0.075 sec/step)\n",
      "step 1220, loss = 4.059187, perp: 57.367, dev_prep: 52.918, (0.072 sec/step)\n",
      "step 1240, loss = 4.007725, perp: 54.125, dev_prep: 52.625, (0.071 sec/step)\n",
      "step 1260, loss = 4.082938, perp: 57.965, dev_prep: 52.122, (0.072 sec/step)\n",
      "step 1280, loss = 4.083807, perp: 58.455, dev_prep: 52.204, (0.070 sec/step)\n",
      "step 1300, loss = 3.882420, perp: 48.213, dev_prep: 52.058, (0.069 sec/step)\n",
      "step 1320, loss = 3.925433, perp: 50.475, dev_prep: 53.896, (0.070 sec/step)\n",
      "step 1340, loss = 3.892623, perp: 47.721, dev_prep: 53.968, (0.072 sec/step)\n",
      "step 1360, loss = 3.954584, perp: 51.270, dev_prep: 51.961, (0.070 sec/step)\n",
      "step 1380, loss = 3.831801, perp: 45.828, dev_prep: 50.612, (0.070 sec/step)\n",
      "step 1400, loss = 3.919581, perp: 50.010, dev_prep: 50.318, (0.073 sec/step)\n",
      "step 1420, loss = 3.897168, perp: 48.273, dev_prep: 49.726, (0.070 sec/step)\n",
      "step 1440, loss = 3.956857, perp: 51.403, dev_prep: 51.272, (0.073 sec/step)\n",
      "step 1460, loss = 3.881648, perp: 47.426, dev_prep: 48.270, (0.075 sec/step)\n",
      "step 1480, loss = 3.833053, perp: 45.032, dev_prep: 48.083, (0.071 sec/step)\n",
      "step 1500, loss = 3.944880, perp: 48.162, dev_prep: 47.826, (0.070 sec/step)\n",
      "step 1520, loss = 3.949462, perp: 51.307, dev_prep: 47.525, (0.074 sec/step)\n",
      "step 1540, loss = 3.907138, perp: 49.134, dev_prep: 46.848, (0.069 sec/step)\n",
      "step 1560, loss = 3.939313, perp: 50.806, dev_prep: 46.876, (0.070 sec/step)\n",
      "step 1580, loss = 3.872001, perp: 44.635, dev_prep: 46.856, (0.071 sec/step)\n",
      "step 1600, loss = 3.894406, perp: 48.564, dev_prep: 48.373, (0.071 sec/step)\n",
      "step 1620, loss = 3.855366, perp: 47.125, dev_prep: 46.914, (0.069 sec/step)\n",
      "step 1640, loss = 3.752065, perp: 42.178, dev_prep: 46.048, (0.071 sec/step)\n",
      "step 1660, loss = 3.662525, perp: 38.320, dev_prep: 45.787, (0.071 sec/step)\n",
      "step 1680, loss = 3.925754, perp: 50.082, dev_prep: 45.518, (0.070 sec/step)\n",
      "step 1700, loss = 3.792207, perp: 43.859, dev_prep: 45.151, (0.070 sec/step)\n",
      "step 1720, loss = 3.663723, perp: 38.652, dev_prep: 45.015, (0.070 sec/step)\n",
      "step 1740, loss = 3.840122, perp: 45.929, dev_prep: 44.877, (0.070 sec/step)\n",
      "step 1760, loss = 3.698975, perp: 40.130, dev_prep: 44.720, (0.073 sec/step)\n",
      "step 1780, loss = 3.839444, perp: 45.785, dev_prep: 46.364, (0.070 sec/step)\n",
      "step 1800, loss = 3.774385, perp: 42.350, dev_prep: 44.303, (0.070 sec/step)\n",
      "step 1820, loss = 3.713885, perp: 41.308, dev_prep: 45.413, (0.070 sec/step)\n",
      "step 1840, loss = 3.883810, perp: 46.256, dev_prep: 43.683, (0.071 sec/step)\n",
      "step 1860, loss = 3.816628, perp: 44.509, dev_prep: 43.499, (0.072 sec/step)\n",
      "step 1880, loss = 3.842601, perp: 46.060, dev_prep: 43.280, (0.070 sec/step)\n",
      "step 1900, loss = 3.799551, perp: 43.554, dev_prep: 42.453, (0.069 sec/step)\n",
      "step 1920, loss = 3.801486, perp: 43.956, dev_prep: 42.144, (0.072 sec/step)\n",
      "step 1940, loss = 3.742278, perp: 41.362, dev_prep: 42.074, (0.072 sec/step)\n",
      "step 1960, loss = 3.820739, perp: 44.980, dev_prep: 41.662, (0.072 sec/step)\n",
      "step 1980, loss = 3.655812, perp: 38.024, dev_prep: 41.753, (0.070 sec/step)\n",
      "step 2000, loss = 3.770970, perp: 43.029, dev_prep: 42.316, (0.070 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.742246, perp: 41.206, dev_prep: 41.398, (0.072 sec/step)\n",
      "step 2040, loss = 3.760526, perp: 41.795, dev_prep: 40.951, (0.071 sec/step)\n",
      "step 2060, loss = 3.758214, perp: 41.961, dev_prep: 40.670, (0.073 sec/step)\n",
      "step 2080, loss = 3.707711, perp: 39.718, dev_prep: 40.866, (0.072 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.625684, perp: 37.260, dev_prep: 42.685, (0.072 sec/step)\n",
      "step 2120, loss = 3.576061, perp: 35.571, dev_prep: 41.274, (0.070 sec/step)\n",
      "step 2140, loss = 3.658810, perp: 38.130, dev_prep: 40.699, (0.070 sec/step)\n",
      "step 2160, loss = 3.719200, perp: 40.668, dev_prep: 40.409, (0.072 sec/step)\n",
      "step 2180, loss = 3.650526, perp: 37.853, dev_prep: 40.253, (0.069 sec/step)\n",
      "step 2200, loss = 3.686821, perp: 39.371, dev_prep: 40.492, (0.072 sec/step)\n",
      "step 2220, loss = 3.678053, perp: 39.049, dev_prep: 39.836, (0.072 sec/step)\n",
      "step 2240, loss = 3.683729, perp: 39.101, dev_prep: 39.938, (0.072 sec/step)\n",
      "step 2260, loss = 3.596235, perp: 35.717, dev_prep: 39.923, (0.071 sec/step)\n",
      "step 2280, loss = 3.555161, perp: 34.141, dev_prep: 39.761, (0.070 sec/step)\n",
      "step 2300, loss = 3.696428, perp: 39.658, dev_prep: 39.909, (0.069 sec/step)\n",
      "step 2320, loss = 3.838975, perp: 45.353, dev_prep: 42.217, (0.070 sec/step)\n",
      "step 2340, loss = 3.638969, perp: 37.763, dev_prep: 40.312, (0.072 sec/step)\n",
      "step 2360, loss = 3.710324, perp: 40.405, dev_prep: 39.372, (0.070 sec/step)\n",
      "step 2380, loss = 3.736472, perp: 40.865, dev_prep: 39.038, (0.071 sec/step)\n",
      "step 2400, loss = 3.701274, perp: 39.612, dev_prep: 39.013, (0.071 sec/step)\n",
      "step 2420, loss = 3.689000, perp: 38.426, dev_prep: 39.004, (0.071 sec/step)\n",
      "step 2440, loss = 3.551364, perp: 34.541, dev_prep: 39.147, (0.072 sec/step)\n",
      "step 2460, loss = 3.537655, perp: 33.856, dev_prep: 38.722, (0.073 sec/step)\n",
      "step 2480, loss = 3.848805, perp: 43.733, dev_prep: 38.548, (0.070 sec/step)\n",
      "step 2500, loss = 3.728374, perp: 39.803, dev_prep: 38.895, (0.070 sec/step)\n",
      "step 2520, loss = 3.695752, perp: 39.027, dev_prep: 39.169, (0.071 sec/step)\n",
      "step 2540, loss = 3.749673, perp: 41.875, dev_prep: 38.280, (0.072 sec/step)\n",
      "step 2560, loss = 3.521586, perp: 32.965, dev_prep: 37.752, (0.072 sec/step)\n",
      "step 2580, loss = 3.568039, perp: 35.302, dev_prep: 37.937, (0.070 sec/step)\n",
      "step 2600, loss = 3.504629, perp: 32.516, dev_prep: 37.595, (0.074 sec/step)\n",
      "step 2620, loss = 3.499726, perp: 32.189, dev_prep: 37.210, (0.072 sec/step)\n",
      "step 2640, loss = 3.488965, perp: 32.447, dev_prep: 38.128, (0.073 sec/step)\n",
      "step 2660, loss = 3.716847, perp: 40.215, dev_prep: 38.088, (0.074 sec/step)\n",
      "step 2680, loss = 3.533857, perp: 33.703, dev_prep: 37.505, (0.073 sec/step)\n",
      "step 2700, loss = 3.416281, perp: 29.619, dev_prep: 36.681, (0.072 sec/step)\n",
      "step 2720, loss = 3.567573, perp: 35.227, dev_prep: 37.923, (0.070 sec/step)\n",
      "step 2740, loss = 3.614697, perp: 36.074, dev_prep: 37.002, (0.072 sec/step)\n",
      "step 2760, loss = 3.712035, perp: 37.597, dev_prep: 37.050, (0.072 sec/step)\n",
      "step 2780, loss = 3.598764, perp: 35.112, dev_prep: 36.983, (0.071 sec/step)\n",
      "step 2800, loss = 3.664244, perp: 38.671, dev_prep: 37.505, (0.072 sec/step)\n",
      "step 2820, loss = 3.561500, perp: 34.556, dev_prep: 36.996, (0.071 sec/step)\n",
      "step 2840, loss = 3.664698, perp: 38.287, dev_prep: 36.847, (0.072 sec/step)\n",
      "step 2860, loss = 3.639018, perp: 36.821, dev_prep: 36.024, (0.073 sec/step)\n",
      "step 2880, loss = 3.650944, perp: 38.541, dev_prep: 36.408, (0.071 sec/step)\n",
      "step 2900, loss = 3.528595, perp: 33.189, dev_prep: 35.556, (0.073 sec/step)\n",
      "step 2920, loss = 3.568807, perp: 34.808, dev_prep: 35.160, (0.072 sec/step)\n",
      "step 2940, loss = 3.568302, perp: 35.310, dev_prep: 35.386, (0.071 sec/step)\n",
      "step 2960, loss = 3.461721, perp: 31.094, dev_prep: 35.359, (0.072 sec/step)\n",
      "step 2980, loss = 3.548717, perp: 33.428, dev_prep: 34.940, (0.072 sec/step)\n",
      "step 3000, loss = 3.539834, perp: 32.739, dev_prep: 34.554, (0.073 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.574154, perp: 35.419, dev_prep: 36.861, (0.073 sec/step)\n",
      "step 3040, loss = 3.583356, perp: 35.998, dev_prep: 36.335, (0.071 sec/step)\n",
      "step 3060, loss = 3.529458, perp: 33.768, dev_prep: 34.947, (0.071 sec/step)\n",
      "step 3080, loss = 3.530738, perp: 33.455, dev_prep: 34.219, (0.071 sec/step)\n",
      "step 3100, loss = 3.458615, perp: 30.764, dev_prep: 34.019, (0.071 sec/step)\n",
      "step 3120, loss = 3.457100, perp: 31.093, dev_prep: 33.568, (0.072 sec/step)\n",
      "step 3140, loss = 3.451244, perp: 30.114, dev_prep: 33.707, (0.070 sec/step)\n",
      "step 3160, loss = 3.597228, perp: 36.565, dev_prep: 34.243, (0.070 sec/step)\n",
      "step 3180, loss = 3.538454, perp: 33.770, dev_prep: 33.652, (0.071 sec/step)\n",
      "step 3200, loss = 3.655934, perp: 37.868, dev_prep: 34.343, (0.069 sec/step)\n",
      "step 3220, loss = 3.661268, perp: 37.825, dev_prep: 33.142, (0.069 sec/step)\n",
      "step 3240, loss = 3.725676, perp: 38.974, dev_prep: 33.586, (0.071 sec/step)\n",
      "step 3260, loss = 3.441275, perp: 30.241, dev_prep: 33.698, (0.073 sec/step)\n",
      "step 3280, loss = 3.381052, perp: 28.746, dev_prep: 33.853, (0.072 sec/step)\n",
      "step 3300, loss = 3.451030, perp: 31.055, dev_prep: 33.173, (0.071 sec/step)\n",
      "step 3320, loss = 3.567737, perp: 34.043, dev_prep: 32.705, (0.070 sec/step)\n",
      "step 3340, loss = 3.425805, perp: 30.194, dev_prep: 32.886, (0.071 sec/step)\n",
      "step 3360, loss = 3.448611, perp: 30.295, dev_prep: 32.482, (0.071 sec/step)\n",
      "step 3380, loss = 3.417829, perp: 29.969, dev_prep: 32.536, (0.071 sec/step)\n",
      "step 3400, loss = 3.515557, perp: 31.736, dev_prep: 33.034, (0.071 sec/step)\n",
      "step 3420, loss = 3.442711, perp: 31.013, dev_prep: 32.770, (0.070 sec/step)\n",
      "step 3440, loss = 3.498101, perp: 33.230, dev_prep: 33.926, (0.070 sec/step)\n",
      "step 3460, loss = 3.507931, perp: 32.563, dev_prep: 33.318, (0.076 sec/step)\n",
      "step 3480, loss = 3.531163, perp: 33.511, dev_prep: 32.659, (0.072 sec/step)\n",
      "step 3500, loss = 3.396942, perp: 29.409, dev_prep: 32.474, (0.070 sec/step)\n",
      "step 3520, loss = 3.333297, perp: 27.223, dev_prep: 32.036, (0.070 sec/step)\n",
      "step 3540, loss = 3.548615, perp: 33.675, dev_prep: 32.667, (0.073 sec/step)\n",
      "step 3560, loss = 3.544209, perp: 33.570, dev_prep: 33.224, (0.074 sec/step)\n",
      "step 3580, loss = 3.336835, perp: 27.994, dev_prep: 32.836, (0.072 sec/step)\n",
      "step 3600, loss = 3.471409, perp: 31.137, dev_prep: 32.079, (0.070 sec/step)\n",
      "step 3620, loss = 3.658719, perp: 37.656, dev_prep: 32.071, (0.070 sec/step)\n",
      "step 3640, loss = 3.498554, perp: 31.932, dev_prep: 31.937, (0.073 sec/step)\n",
      "step 3660, loss = 3.433473, perp: 30.077, dev_prep: 31.986, (0.071 sec/step)\n",
      "step 3680, loss = 3.462601, perp: 31.338, dev_prep: 31.536, (0.071 sec/step)\n",
      "step 3700, loss = 3.450775, perp: 31.002, dev_prep: 31.553, (0.069 sec/step)\n",
      "step 3720, loss = 3.322394, perp: 26.742, dev_prep: 31.670, (0.072 sec/step)\n",
      "step 3740, loss = 3.465195, perp: 30.929, dev_prep: 31.826, (0.069 sec/step)\n",
      "step 3760, loss = 3.233061, perp: 25.381, dev_prep: 32.348, (0.075 sec/step)\n",
      "step 3780, loss = 3.406665, perp: 29.325, dev_prep: 31.804, (0.072 sec/step)\n",
      "step 3800, loss = 3.476219, perp: 31.620, dev_prep: 31.288, (0.071 sec/step)\n",
      "step 3820, loss = 3.299165, perp: 26.271, dev_prep: 30.619, (0.072 sec/step)\n",
      "step 3840, loss = 3.375589, perp: 28.889, dev_prep: 31.036, (0.071 sec/step)\n",
      "step 3860, loss = 3.420021, perp: 30.139, dev_prep: 30.004, (0.071 sec/step)\n",
      "step 3880, loss = 3.453494, perp: 30.653, dev_prep: 29.759, (0.070 sec/step)\n",
      "step 3900, loss = 3.395121, perp: 31.562, dev_prep: 32.834, (0.071 sec/step)\n",
      "step 3920, loss = 3.453546, perp: 30.955, dev_prep: 30.604, (0.070 sec/step)\n",
      "step 3940, loss = 3.391636, perp: 28.865, dev_prep: 29.661, (0.071 sec/step)\n",
      "step 3960, loss = 3.465467, perp: 31.493, dev_prep: 30.100, (0.074 sec/step)\n",
      "step 3980, loss = 3.311939, perp: 26.847, dev_prep: 29.895, (0.070 sec/step)\n",
      "step 4000, loss = 3.352694, perp: 27.704, dev_prep: 29.233, (0.075 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.200924, perp: 24.069, dev_prep: 29.162, (0.071 sec/step)\n",
      "step 4040, loss = 3.386494, perp: 28.514, dev_prep: 28.522, (0.073 sec/step)\n",
      "step 4060, loss = 3.391263, perp: 29.369, dev_prep: 29.828, (0.070 sec/step)\n",
      "step 4080, loss = 3.354326, perp: 27.855, dev_prep: 28.672, (0.074 sec/step)\n",
      "step 4100, loss = 3.317263, perp: 27.101, dev_prep: 28.497, (0.071 sec/step)\n",
      "step 4120, loss = 3.355657, perp: 27.943, dev_prep: 28.022, (0.071 sec/step)\n",
      "step 4140, loss = 3.347084, perp: 26.974, dev_prep: 27.884, (0.070 sec/step)\n",
      "step 4160, loss = 3.333354, perp: 27.625, dev_prep: 29.273, (0.070 sec/step)\n",
      "step 4180, loss = 3.300296, perp: 26.503, dev_prep: 28.288, (0.072 sec/step)\n",
      "step 4200, loss = 3.284117, perp: 26.327, dev_prep: 27.892, (0.071 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.286234, perp: 25.779, dev_prep: 27.518, (0.071 sec/step)\n",
      "step 4240, loss = 3.214027, perp: 24.617, dev_prep: 27.423, (0.070 sec/step)\n",
      "step 4260, loss = 3.206983, perp: 24.421, dev_prep: 27.180, (0.070 sec/step)\n",
      "step 4280, loss = 3.282256, perp: 26.507, dev_prep: 26.971, (0.073 sec/step)\n",
      "step 4300, loss = 3.337909, perp: 27.511, dev_prep: 26.716, (0.073 sec/step)\n",
      "step 4320, loss = 3.197356, perp: 24.027, dev_prep: 26.706, (0.071 sec/step)\n",
      "step 4340, loss = 3.263339, perp: 25.311, dev_prep: 26.544, (0.071 sec/step)\n",
      "step 4360, loss = 3.234272, perp: 24.923, dev_prep: 26.781, (0.071 sec/step)\n",
      "step 4380, loss = 3.228511, perp: 24.516, dev_prep: 26.305, (0.072 sec/step)\n",
      "step 4400, loss = 3.320543, perp: 26.550, dev_prep: 26.128, (0.072 sec/step)\n",
      "step 4420, loss = 3.289860, perp: 26.275, dev_prep: 26.001, (0.072 sec/step)\n",
      "step 4440, loss = 3.205961, perp: 24.387, dev_prep: 25.959, (0.070 sec/step)\n",
      "step 4460, loss = 3.142467, perp: 22.345, dev_prep: 25.321, (0.072 sec/step)\n",
      "step 4480, loss = 3.090601, perp: 21.442, dev_prep: 25.465, (0.071 sec/step)\n",
      "step 4500, loss = 3.201521, perp: 24.630, dev_prep: 25.763, (0.071 sec/step)\n",
      "step 4520, loss = 3.243412, perp: 25.507, dev_prep: 26.014, (0.071 sec/step)\n",
      "step 4540, loss = 3.185467, perp: 23.541, dev_prep: 25.546, (0.076 sec/step)\n",
      "step 4560, loss = 3.004222, perp: 19.843, dev_prep: 25.386, (0.071 sec/step)\n",
      "step 4580, loss = 3.226988, perp: 24.726, dev_prep: 24.969, (0.070 sec/step)\n",
      "step 4600, loss = 3.183999, perp: 23.851, dev_prep: 25.112, (0.069 sec/step)\n",
      "step 4620, loss = 3.247636, perp: 25.550, dev_prep: 25.917, (0.070 sec/step)\n",
      "step 4640, loss = 3.217560, perp: 24.164, dev_prep: 24.648, (0.071 sec/step)\n",
      "step 4660, loss = 3.123170, perp: 22.531, dev_prep: 24.400, (0.074 sec/step)\n",
      "step 4680, loss = 3.089023, perp: 21.188, dev_prep: 23.930, (0.071 sec/step)\n",
      "step 4700, loss = 3.130671, perp: 22.438, dev_prep: 23.708, (0.071 sec/step)\n",
      "step 4720, loss = 3.204725, perp: 24.200, dev_prep: 23.538, (0.069 sec/step)\n",
      "step 4740, loss = 3.014986, perp: 20.023, dev_prep: 23.397, (0.071 sec/step)\n",
      "step 4760, loss = 3.061589, perp: 21.147, dev_prep: 23.286, (0.069 sec/step)\n",
      "step 4780, loss = 3.257006, perp: 24.916, dev_prep: 23.367, (0.070 sec/step)\n",
      "step 4800, loss = 3.045120, perp: 20.402, dev_prep: 23.103, (0.071 sec/step)\n",
      "step 4820, loss = 3.070227, perp: 20.957, dev_prep: 22.998, (0.071 sec/step)\n",
      "step 4840, loss = 3.060432, perp: 20.867, dev_prep: 23.517, (0.071 sec/step)\n",
      "step 4860, loss = 3.068023, perp: 21.263, dev_prep: 23.113, (0.070 sec/step)\n",
      "step 4880, loss = 3.071074, perp: 20.977, dev_prep: 22.738, (0.070 sec/step)\n",
      "step 4900, loss = 2.962165, perp: 18.877, dev_prep: 22.051, (0.070 sec/step)\n",
      "step 4920, loss = 3.100713, perp: 20.810, dev_prep: 22.145, (0.071 sec/step)\n",
      "step 4940, loss = 3.077859, perp: 21.006, dev_prep: 22.197, (0.072 sec/step)\n",
      "step 4960, loss = 2.977974, perp: 19.274, dev_prep: 21.848, (0.070 sec/step)\n",
      "step 4980, loss = 3.106134, perp: 21.494, dev_prep: 21.771, (0.070 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data,\n",
    "                    target_ids: dev_target_data,\n",
    "                    sequence_mask: dev_masks,\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks, dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "# plt.savefig(loss_fig)\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps, perps, label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps, dev_perps, label=\"dev\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(perp_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.random.choice(1000, infer_batch_size)\n",
    "infer_inputs = dev_source_data[inds]\n",
    "infer_results = sess.run(infer_outputs, feed_dict={source_ids: infer_inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,   2,  80, 455, 190, 748, 281, 573, 654,  65, 937, 369, 459,\n",
       "        184, 320, 159, 344, 947, 536, 884, 194, 848, 674, 804, 891],\n",
       "       [  2,   2,   2,   2,   2,   2, 448, 913, 481,   5, 821, 301, 717,\n",
       "        417, 992, 352, 747, 525, 752, 292, 955, 590, 293, 439, 866],\n",
       "       [396, 431, 601, 611, 490, 499, 814, 246, 476, 975, 491, 904, 696,\n",
       "         88, 361, 991, 584, 171, 239, 637, 756,  20,  10, 298, 377],\n",
       "       [344, 221, 867, 540, 130, 496, 706, 328, 132, 549, 965, 419, 477,\n",
       "        717, 743, 341, 366, 206, 193, 365, 748, 511,  23, 579, 239],\n",
       "       [  2,   2,   2,   2,   2,   2, 829, 434, 251, 620, 330, 939, 368,\n",
       "        277, 579, 956, 793,  76, 770, 485, 139, 658, 750, 767, 798],\n",
       "       [  2,   2,   2,   2,   2,   2,   2,   2,   2,   2, 109, 688, 832,\n",
       "        729, 522, 112, 691, 577, 722, 526,  67, 913,  16, 488, 793],\n",
       "       [  2,   2,   2,   2,   2,   2,   2,   2,   2, 329, 352, 542, 479,\n",
       "        692, 934,  78, 950,  90, 247, 737, 233, 468, 923, 156, 118],\n",
       "       [  2,   2,   2,   2,   2,   2,   2,   2,   2, 526, 307, 438, 208,\n",
       "        664, 922, 619, 954, 496, 226, 272, 474, 794, 275, 581, 929],\n",
       "       [  2,   2,   2,   2,   2,   2,   2, 237, 921, 587, 849, 681, 451,\n",
       "        536, 247, 512, 686, 101, 529, 941, 314, 178, 300, 700, 369],\n",
       "       [  2,   2, 401, 330, 324, 504, 977, 114,  47, 557, 974, 108,  70,\n",
       "        594, 228, 148, 398, 734, 382, 181, 724, 284, 617, 713, 280]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  22,   72,  160,  180,  190,  190,  271,  309,  345,  367,  444,\n",
       "         475,  512,  596,  624,  690,  763,  820,  847,  877,  906],\n",
       "       [   7,  278,  297,  314,  367,  418,  446,  457,  474,  544,  620,\n",
       "         721,  744,  777,  808,  862,  899,  932, 1001,    1,    1],\n",
       "       [  15,   42,   72,  169,  239,  263,  297,  345,  372,  410,  434,\n",
       "         454,  485,  512,  544,  576,  596,  623,  655,  703,  744],\n",
       "       [  92,  121,  153,  181,  201,  222,  244,  304,  314,  348,  367,\n",
       "         387,  410,  480,  504,  516,  535,  544,  596,  691,  721],\n",
       "       [  59,  132,  244,  295,  314,  367,  430,  512,  592,  610,  681,\n",
       "         744,  754,  765,  788,  806,  842,  920,  957,    1,    1],\n",
       "       [   5,   27,   63,  130,  506,  517,  544,  591,  682,  704,  721,\n",
       "         744,  784,  847,  899,    1,    1,    1,    1,    1,    1],\n",
       "       [  63,   92,  121,  138,  235,  244,  314,  367,  454,  505,  544,\n",
       "         708,  728,  900,  932,  957,    1,    1,    1,    1,    1],\n",
       "       [ 201,  239,  244,  280,  304,  434,  472,  512,  566,  592,  624,\n",
       "         655,  823,  877,  919,  975,    1,    1,    1,    1,    1],\n",
       "       [  95,  181,  222,  263,  292,  314,  385,  446,  512,  535,  566,\n",
       "         596,  653,  690,  719,  884,  917,  957,    1,    1,    1],\n",
       "       [  42,   63,   97,  121,  152,  181,  222,  252,  292,  314,  354,\n",
       "         378,  387,  410,  516,  544,  596,  653,  691,  719,  753]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_results.ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 65,  80, 159, 184, 190, 194, 281, 320, 344, 369, 455, 459, 536,\n",
       "        573, 654, 674, 748, 804, 848, 884, 891, 937, 947,   1,   1],\n",
       "       [  5, 292, 293, 301, 352, 417, 439, 448, 481, 525, 590, 717, 747,\n",
       "        752, 821, 866, 913, 955, 992,   1,   1,   1,   1,   1,   1],\n",
       "       [ 10,  20,  88, 171, 239, 246, 298, 361, 377, 396, 431, 476, 490,\n",
       "        491, 499, 584, 601, 611, 637, 696, 756, 814, 904, 975, 991],\n",
       "       [ 23, 130, 132, 193, 206, 221, 239, 328, 341, 344, 365, 366, 419,\n",
       "        477, 496, 511, 540, 549, 579, 706, 717, 743, 748, 867, 965],\n",
       "       [ 76, 139, 251, 277, 330, 368, 434, 485, 579, 620, 658, 750, 767,\n",
       "        770, 793, 798, 829, 939, 956,   1,   1,   1,   1,   1,   1],\n",
       "       [ 16,  67, 109, 112, 488, 522, 526, 577, 688, 691, 722, 729, 793,\n",
       "        832, 913,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       [ 78,  90, 118, 156, 233, 247, 329, 352, 468, 479, 542, 692, 737,\n",
       "        923, 934, 950,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       [208, 226, 272, 275, 307, 438, 474, 496, 526, 581, 619, 664, 794,\n",
       "        922, 929, 954,   1,   1,   1,   1,   1,   1,   1,   1,   1],\n",
       "       [101, 178, 237, 247, 300, 314, 369, 451, 512, 529, 536, 587, 681,\n",
       "        686, 700, 849, 921, 941,   1,   1,   1,   1,   1,   1,   1],\n",
       "       [ 47,  70, 108, 114, 148, 181, 228, 280, 284, 324, 330, 382, 398,\n",
       "        401, 504, 557, 594, 617, 713, 724, 734, 974, 977,   1,   1]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_target_data[inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.52708425227248"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(1000, infer_batch_size)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, dev_masks, dev_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
