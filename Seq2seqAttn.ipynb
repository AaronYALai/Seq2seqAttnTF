{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ECM wrapper\n",
    "# ECMState = collections.namedtuple(\n",
    "#     \"ECMState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "# class ECMWrapper(RNNCell):\n",
    "#     \"\"\"\n",
    "#     Emotion Chatting Machine: H. Zhou, et al. AAAI 2018\n",
    "#     (https://arxiv.org/abs/1704.01074)\n",
    "#     Emotion Category Embedding, Internal and External Memory Modules\n",
    "#         cell: vanilla multi-layer RNNCell\n",
    "#         memory: [batch_size, max_time, num_units]\n",
    "#     \"\"\"\n",
    "#     def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "#                  num_units, dtype, emo_embed, emo_cat, num_emo,\n",
    "#                  num_emo_units, emo_init=None):\n",
    "#         self._cell = cell\n",
    "#         self._memory = memory\n",
    "#         self.num_hidden = num_hidden\n",
    "\n",
    "#         self._dec_init_states = dec_init_states\n",
    "#         self._state_size = ECMState(self._cell.state_size,\n",
    "#                                     num_units, memory.shape[-1].value)\n",
    "#         self._num_units = num_units\n",
    "#         self._dtype = dtype\n",
    "\n",
    "#         # ECM hyperparameters\n",
    "#         self._emo_embed = emo_embed\n",
    "#         self._emo_cat = emo_cat\n",
    "#         self._num_emo_units = num_emo_units\n",
    "\n",
    "#         # internal memory\n",
    "#         if emo_init is None:\n",
    "#             initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "#         self.int_memory = tf.Variable(\n",
    "#             initializer(shape=(num_emo, num_emo_units)),\n",
    "#             name=\"emo_memory\", dtype=dtype)\n",
    "#         self.emo_int_memory = tf.gather(self.int_memory, emo_cat)\n",
    "\n",
    "#     @property\n",
    "#     def state_size(self):\n",
    "#         return self._state_size\n",
    "\n",
    "#     @property\n",
    "#     def output_size(self):\n",
    "#         return self._num_units\n",
    "\n",
    "#     def initial_state(self):\n",
    "#         \"\"\"\n",
    "#         Generate initial state for attn wrapped rnn cell\n",
    "#             dec_init_states: None (no states pass), or encoder final states\n",
    "#             num_units: decoder's num of cell units\n",
    "#         \"\"\"\n",
    "#         h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "#         context_0 = self._compute_context(h_0)\n",
    "#         h_0 = context_0 * 0\n",
    "\n",
    "#         if self._dec_init_states is None:\n",
    "#             batch_size = tf.shape(self._memory)[0]\n",
    "#             cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "#         else:\n",
    "#             cell_states = self._dec_init_states\n",
    "\n",
    "#         attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "#         return attn_state_0\n",
    "\n",
    "#     def _compute_context(self, query):\n",
    "#         \"\"\"\n",
    "#         Compute attn scores and weighted sum of memory as the context\n",
    "#             query: [batch_size, num_units]\n",
    "#         Returns:\n",
    "#             context: [batch_size, num_units]\n",
    "#         \"\"\"\n",
    "#         query = tf.expand_dims(query, -2)\n",
    "#         Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "#         Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "#         e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "#         attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "#         context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "#         return context\n",
    "\n",
    "#     def __call__(self, inputs, attn_states):\n",
    "#         \"\"\"\n",
    "#             inputs: emebeddings of previous word\n",
    "#             states: (cell_states, outputs) at each step\n",
    "#         \"\"\"\n",
    "#         prev_cell_states, h, context = attn_states\n",
    "\n",
    "#         x = tf.concat([inputs, h, context], axis=-1)\n",
    "#         new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "#         new_context = self._compute_context(new_h)\n",
    "#         new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "#         return (new_h, new_attn_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, time_major=False, dtype=tf.float32,\n",
    "                  forget_bias=1.0, name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        mode: train or infer\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            # need batch-major\n",
    "            if time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=True,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099376, 0.00099645, 0.00099146, ..., 0.00100074,\n",
       "         0.00099693, 0.00099725],\n",
       "        [0.00099183, 0.00099612, 0.00099237, ..., 0.00099916,\n",
       "         0.00099589, 0.00099665],\n",
       "        [0.00099046, 0.00099617, 0.00099328, ..., 0.00099728,\n",
       "         0.00099525, 0.00099633],\n",
       "        [0.00098959, 0.00099639, 0.00099407, ..., 0.00099539,\n",
       "         0.00099494, 0.00099622]],\n",
       "\n",
       "       [[0.00099548, 0.00100049, 0.00099475, ..., 0.0009981 ,\n",
       "         0.000997  , 0.00099507],\n",
       "        [0.00099426, 0.00100053, 0.00099505, ..., 0.00099784,\n",
       "         0.00099697, 0.00099506],\n",
       "        [0.00099379, 0.00100026, 0.00099523, ..., 0.00099786,\n",
       "         0.0009964 , 0.00099538],\n",
       "        [0.00099353, 0.00100006, 0.00099603, ..., 0.00099753,\n",
       "         0.00099582, 0.00099596]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=False, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-7.31002889e-04,  2.04341835e-04, -2.67840096e-06, ...,\n",
       "          -1.47821684e-03,  1.34075154e-03,  7.83172844e-04],\n",
       "         [-7.31002889e-04,  2.04341835e-04, -2.67840096e-06, ...,\n",
       "          -1.47821684e-03,  1.34075154e-03,  7.83172844e-04],\n",
       "         [-7.31002889e-04,  2.04341835e-04, -2.67840096e-06, ...,\n",
       "          -1.47821684e-03,  1.34075154e-03,  7.83172844e-04],\n",
       "         [-7.31002889e-04,  2.04341835e-04, -2.67840096e-06, ...,\n",
       "          -1.47821684e-03,  1.34075154e-03,  7.83172844e-04],\n",
       "         [-7.31002889e-04,  2.04341835e-04, -2.67840096e-06, ...,\n",
       "          -1.47821684e-03,  1.34075154e-03,  7.83172844e-04]],\n",
       "\n",
       "        [[-1.61618227e-05,  5.99595020e-04,  4.00797173e-04, ...,\n",
       "          -1.80941552e-03,  1.80929783e-03,  1.03675388e-03],\n",
       "         [-3.76146869e-04, -3.27974209e-04, -1.38137679e-04, ...,\n",
       "          -1.24983187e-03,  2.15268717e-03,  1.46182254e-03],\n",
       "         [-1.61618227e-05,  5.99595020e-04,  4.00797173e-04, ...,\n",
       "          -1.80941552e-03,  1.80929783e-03,  1.03675388e-03],\n",
       "         [-5.23921335e-04,  2.09292863e-04,  6.00540443e-05, ...,\n",
       "          -1.44268165e-03,  2.47712480e-03,  1.21238152e-03],\n",
       "         [-5.23921335e-04,  2.09292863e-04,  6.00540443e-05, ...,\n",
       "          -1.44268165e-03,  2.47712480e-03,  1.21238152e-03]],\n",
       "\n",
       "        [[ 7.34432600e-04, -9.62118676e-04, -5.71382989e-04, ...,\n",
       "          -2.51290447e-04,  2.23921752e-03,  1.50108535e-03],\n",
       "         [ 1.23296538e-03,  2.77666957e-04,  4.36171773e-04, ...,\n",
       "          -1.18378177e-03,  1.75210217e-03,  1.05309626e-03],\n",
       "         [ 7.77718145e-04, -1.17029296e-04,  4.34859321e-05, ...,\n",
       "          -6.57497614e-04,  2.38169706e-03,  1.41715328e-03],\n",
       "         [ 3.08332848e-04, -3.74093652e-05,  1.39796539e-04, ...,\n",
       "          -1.85301842e-03,  1.64614851e-03,  1.30920822e-03],\n",
       "         [ 7.77718145e-04, -1.17029296e-04,  4.34859321e-05, ...,\n",
       "          -6.57497614e-04,  2.38169706e-03,  1.41715328e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 3.03054671e-03,  8.61360226e-04,  1.80507265e-03, ...,\n",
       "           1.82533404e-03,  2.43464601e-05, -1.81217561e-04],\n",
       "         [ 3.88503564e-03,  9.63277766e-04,  1.97643205e-03, ...,\n",
       "           1.66636345e-03,  1.06607750e-03,  1.87030528e-05],\n",
       "         [ 3.03054671e-03,  8.61360226e-04,  1.80507265e-03, ...,\n",
       "           1.82533404e-03,  2.43464601e-05, -1.81217561e-04],\n",
       "         [ 3.50394938e-03,  1.56536931e-03,  2.76708882e-03, ...,\n",
       "           2.13139481e-03,  1.02435115e-04, -1.54024456e-05],\n",
       "         [ 3.88503564e-03,  9.63277766e-04,  1.97643205e-03, ...,\n",
       "           1.66636345e-03,  1.06607750e-03,  1.87030528e-05]],\n",
       "\n",
       "        [[ 2.72587361e-03,  1.25659932e-03,  2.47146003e-03, ...,\n",
       "           1.62339211e-03, -4.20671015e-04, -4.86196630e-04],\n",
       "         [ 3.00445501e-03,  1.44398713e-03,  2.86500482e-03, ...,\n",
       "           2.32541421e-03, -7.10499473e-04, -1.32654677e-05],\n",
       "         [ 3.00445501e-03,  1.44398713e-03,  2.86500482e-03, ...,\n",
       "           2.32541421e-03, -7.10499473e-04, -1.32654677e-05],\n",
       "         [ 3.00445501e-03,  1.44398713e-03,  2.86500482e-03, ...,\n",
       "           2.32541421e-03, -7.10499473e-04, -1.32654677e-05],\n",
       "         [ 2.72587361e-03,  1.25659932e-03,  2.47146003e-03, ...,\n",
       "           1.62339211e-03, -4.20671015e-04, -4.86196630e-04]],\n",
       "\n",
       "        [[ 2.47721933e-03,  1.99937588e-03,  3.29830241e-03, ...,\n",
       "           1.22249301e-03, -6.53197174e-04, -8.98860104e-04],\n",
       "         [ 2.01426703e-03,  9.39675490e-04,  2.81903846e-03, ...,\n",
       "           1.80036877e-03, -1.11450325e-03, -4.59871197e-04],\n",
       "         [ 2.97291670e-03,  1.43157062e-03,  3.09175113e-03, ...,\n",
       "           1.44387397e-03,  7.88486213e-05, -3.72943439e-04],\n",
       "         [ 2.78898748e-03,  1.89143640e-03,  3.34942923e-03, ...,\n",
       "           2.08265195e-03, -1.16370094e-03, -3.46917805e-04],\n",
       "         [ 2.50645354e-03,  3.73609364e-04,  2.61302060e-03, ...,\n",
       "           2.01798836e-03, -3.75401753e-04,  7.05819693e-05]]],\n",
       "\n",
       "\n",
       "       [[[-7.74389075e-04,  2.16587214e-04, -1.94987915e-05, ...,\n",
       "          -1.48681062e-03,  1.32372894e-03,  7.93928048e-04],\n",
       "         [-7.74389075e-04,  2.16587214e-04, -1.94987915e-05, ...,\n",
       "          -1.48681062e-03,  1.32372894e-03,  7.93928048e-04],\n",
       "         [-7.74389075e-04,  2.16587214e-04, -1.94987915e-05, ...,\n",
       "          -1.48681062e-03,  1.32372894e-03,  7.93928048e-04],\n",
       "         [-7.74389075e-04,  2.16587214e-04, -1.94987915e-05, ...,\n",
       "          -1.48681062e-03,  1.32372894e-03,  7.93928048e-04],\n",
       "         [-7.74389075e-04,  2.16587214e-04, -1.94987915e-05, ...,\n",
       "          -1.48681062e-03,  1.32372894e-03,  7.93928048e-04]],\n",
       "\n",
       "        [[-1.23111764e-04,  6.24446140e-04,  3.63319676e-04, ...,\n",
       "          -1.84384908e-03,  1.76535931e-03,  1.06916507e-03],\n",
       "         [-4.82740579e-04, -3.03707202e-04, -1.75589928e-04, ...,\n",
       "          -1.28453481e-03,  2.10888032e-03,  1.49412593e-03],\n",
       "         [-1.23111764e-04,  6.24446140e-04,  3.63319676e-04, ...,\n",
       "          -1.84384908e-03,  1.76535931e-03,  1.06916507e-03],\n",
       "         [-6.31897361e-04,  2.33977509e-04,  2.25443764e-05, ...,\n",
       "          -1.47738925e-03,  2.43417942e-03,  1.24507351e-03],\n",
       "         [-6.31897361e-04,  2.33977509e-04,  2.25443764e-05, ...,\n",
       "          -1.47738925e-03,  2.43417942e-03,  1.24507351e-03]],\n",
       "\n",
       "        [[ 5.59652224e-04, -9.27878078e-04, -6.25260000e-04, ...,\n",
       "          -3.28220602e-04,  2.16690334e-03,  1.56448747e-03],\n",
       "         [ 6.03418564e-04, -8.26107571e-05, -1.11256959e-05, ...,\n",
       "          -7.33942667e-04,  2.30847532e-03,  1.48023304e-03],\n",
       "         [ 1.05753494e-03,  3.13439057e-04,  3.82227940e-04, ...,\n",
       "          -1.26053742e-03,  1.67919509e-03,  1.11681398e-03],\n",
       "         [ 1.33219059e-04, -2.53273174e-06,  8.63583118e-05, ...,\n",
       "          -1.92850200e-03,  1.57195411e-03,  1.37124374e-03],\n",
       "         [-2.11447477e-05, -8.76338105e-04, -6.27808622e-04, ...,\n",
       "          -8.85829329e-04,  2.17668666e-03,  1.77383388e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.66527617e-03,  9.10384697e-04,  1.75640255e-03, ...,\n",
       "           1.41826051e-03, -1.82572112e-04,  1.13998773e-04],\n",
       "         [ 3.51918163e-03,  1.01440796e-03,  1.92892493e-03, ...,\n",
       "           1.25771505e-03,  8.60779081e-04,  3.14829493e-04],\n",
       "         [ 3.51918163e-03,  1.01440796e-03,  1.92892493e-03, ...,\n",
       "           1.25771505e-03,  8.60779081e-04,  3.14829493e-04],\n",
       "         [ 2.66527617e-03,  9.10384697e-04,  1.75640255e-03, ...,\n",
       "           1.41826051e-03, -1.82572112e-04,  1.13998773e-04],\n",
       "         [ 2.66527617e-03,  9.10384697e-04,  1.75640255e-03, ...,\n",
       "           1.41826051e-03, -1.82572112e-04,  1.13998773e-04]],\n",
       "\n",
       "        [[ 2.86261784e-03,  7.40229501e-04,  2.22864398e-03, ...,\n",
       "           1.39528466e-03,  9.71339177e-05,  3.67405999e-04],\n",
       "         [ 2.37103552e-03,  1.30456593e-03,  2.43452424e-03, ...,\n",
       "           1.17579114e-03, -6.38472731e-04, -1.61456206e-04],\n",
       "         [ 2.86261784e-03,  7.40229501e-04,  2.22864398e-03, ...,\n",
       "           1.39528466e-03,  9.71339177e-05,  3.67405999e-04],\n",
       "         [ 2.86261784e-03,  7.40229501e-04,  2.22864398e-03, ...,\n",
       "           1.39528466e-03,  9.71339177e-05,  3.67405999e-04],\n",
       "         [ 2.37103552e-03,  1.30456593e-03,  2.43452424e-03, ...,\n",
       "           1.17579114e-03, -6.38472731e-04, -1.61456206e-04]],\n",
       "\n",
       "        [[ 2.14146334e-03,  2.04625726e-03,  3.27343680e-03, ...,\n",
       "           7.41224620e-04, -8.79700994e-04, -5.48289856e-04],\n",
       "         [ 1.67861197e-03,  9.85995750e-04,  2.79297656e-03, ...,\n",
       "           1.32134766e-03, -1.34188868e-03, -1.09709072e-04],\n",
       "         [ 2.17060558e-03,  4.21507750e-04,  2.58660666e-03, ...,\n",
       "           1.53903163e-03, -6.02631131e-04,  4.21743316e-04],\n",
       "         [ 2.31717201e-03,  1.86345307e-03,  2.92366650e-03, ...,\n",
       "           1.40649930e-03, -8.41741799e-04, -1.53832909e-04],\n",
       "         [ 2.63695046e-03,  1.48006249e-03,  3.06647900e-03, ...,\n",
       "           9.62642895e-04, -1.47478073e-04, -2.13795283e-05]]],\n",
       "\n",
       "\n",
       "       [[[-7.85355398e-04,  2.88226118e-04, -2.41390762e-06, ...,\n",
       "          -1.51382375e-03,  1.34638906e-03,  7.71100167e-04],\n",
       "         [-7.85355398e-04,  2.88226118e-04, -2.41390762e-06, ...,\n",
       "          -1.51382375e-03,  1.34638906e-03,  7.71100167e-04],\n",
       "         [-7.85355398e-04,  2.88226118e-04, -2.41390762e-06, ...,\n",
       "          -1.51382375e-03,  1.34638906e-03,  7.71100167e-04],\n",
       "         [-7.85355398e-04,  2.88226118e-04, -2.41390762e-06, ...,\n",
       "          -1.51382375e-03,  1.34638906e-03,  7.71100167e-04],\n",
       "         [-7.85355398e-04,  2.88226118e-04, -2.41390762e-06, ...,\n",
       "          -1.51382375e-03,  1.34638906e-03,  7.71100167e-04]],\n",
       "\n",
       "        [[-1.97955291e-04, -4.34076646e-05, -2.14672880e-04, ...,\n",
       "          -1.49703119e-03,  1.67936308e-03,  1.08121103e-03],\n",
       "         [-1.52771594e-04,  8.01885966e-04,  3.99718178e-04, ...,\n",
       "          -1.90747413e-03,  1.81584037e-03,  1.00504584e-03],\n",
       "         [-1.52771594e-04,  8.01885966e-04,  3.99718178e-04, ...,\n",
       "          -1.90747413e-03,  1.81584037e-03,  1.00504584e-03],\n",
       "         [-6.61273953e-04,  4.13097441e-04,  5.88680414e-05, ...,\n",
       "          -1.54001231e-03,  2.48604640e-03,  1.17957289e-03],\n",
       "         [-6.61273953e-04,  4.13097441e-04,  5.88680414e-05, ...,\n",
       "          -1.54001231e-03,  2.48604640e-03,  1.17957289e-03]],\n",
       "\n",
       "        [[ 9.99423093e-04,  6.08547824e-04,  4.31456487e-04, ...,\n",
       "          -1.35558273e-03,  1.75439101e-03,  9.96350660e-04],\n",
       "         [ 5.44777955e-04,  2.13350402e-04,  3.92643706e-05, ...,\n",
       "          -8.31018726e-04,  2.38471618e-03,  1.36031141e-03],\n",
       "         [ 7.52459746e-05,  2.94055440e-04,  1.34226488e-04, ...,\n",
       "          -2.02444987e-03,  1.64809322e-03,  1.25116366e-03],\n",
       "         [-8.10369384e-05, -5.77624422e-04, -5.77511208e-04, ...,\n",
       "          -9.81394318e-04,  2.25468492e-03,  1.65185228e-03],\n",
       "         [ 5.00231632e-04, -6.30936236e-04, -5.74256119e-04, ...,\n",
       "          -4.22971265e-04,  2.24432885e-03,  1.44274835e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.82507827e-03,  1.49856461e-03,  1.46962202e-03, ...,\n",
       "           1.39374589e-03, -6.17093814e-04, -6.57800236e-04],\n",
       "         [ 1.46444328e-03,  8.29873374e-04,  1.09147420e-03, ...,\n",
       "           1.77805824e-03, -9.29387228e-04, -3.36447731e-04],\n",
       "         [ 1.82507827e-03,  1.49856461e-03,  1.46962202e-03, ...,\n",
       "           1.39374589e-03, -6.17093814e-04, -6.57800236e-04],\n",
       "         [ 2.28935829e-03,  2.56061484e-03,  1.94129930e-03, ...,\n",
       "           8.11845879e-04, -1.55848655e-04, -1.09905587e-03],\n",
       "         [ 1.01507455e-03,  1.44282181e-03,  8.32686608e-04, ...,\n",
       "           2.05312017e-03, -3.68226960e-04, -4.89095808e-04]],\n",
       "\n",
       "        [[ 1.10816781e-03,  1.38352613e-03,  1.81537005e-03, ...,\n",
       "           1.54275307e-03, -1.36469817e-03, -7.97900779e-04],\n",
       "         [ 1.57145679e-03,  2.44518905e-03,  2.29054713e-03, ...,\n",
       "           9.60960228e-04, -9.02965199e-04, -1.23608473e-03],\n",
       "         [ 1.57145679e-03,  2.44518905e-03,  2.29054713e-03, ...,\n",
       "           9.60960228e-04, -9.02965199e-04, -1.23608473e-03],\n",
       "         [ 1.57145679e-03,  2.44518905e-03,  2.29054713e-03, ...,\n",
       "           9.60960228e-04, -9.02965199e-04, -1.23608473e-03],\n",
       "         [ 2.03877338e-03,  3.71226878e-03,  2.71694269e-03, ...,\n",
       "           2.67476804e-04, -4.13772272e-04, -1.69144897e-03]],\n",
       "\n",
       "        [[ 1.32515235e-03,  3.56511166e-03,  3.14312149e-03, ...,\n",
       "           4.18215786e-04, -1.04990741e-03, -1.77627499e-03],\n",
       "         [ 8.57684296e-04,  2.29861354e-03,  2.71379063e-03, ...,\n",
       "           1.11086539e-03, -1.54016516e-03, -1.32380321e-03],\n",
       "         [ 5.23802242e-04,  3.51392175e-03,  2.50709290e-03, ...,\n",
       "           1.08102639e-03, -8.11106875e-04, -1.60984905e-03],\n",
       "         [ 1.37202698e-03,  3.69865471e-03,  3.16111231e-03, ...,\n",
       "           3.85905325e-04, -1.05226808e-03, -1.81130064e-03],\n",
       "         [ 2.10690487e-04,  3.46958940e-03,  2.08441238e-03, ...,\n",
       "           1.46393734e-03, -6.25002896e-04, -1.61703303e-03]]]],\n",
       "      dtype=float32), ids=array([[[ 86,  86, 245, 169,  86],\n",
       "        [169, 495, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 169],\n",
       "        [169, 169, 169, 495, 169],\n",
       "        [510, 169, 242, 169, 169],\n",
       "        [510, 510, 104, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510,  34],\n",
       "        [ 34, 510,  34, 510, 510],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34,  98, 657,  98]],\n",
       "\n",
       "       [[ 86,  86, 245, 169,  86],\n",
       "        [169, 495, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 495],\n",
       "        [169, 169, 495, 169, 169],\n",
       "        [510, 169, 169, 495, 169],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [ 34, 510, 510,  34,  34],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34, 657,  98, 657]],\n",
       "\n",
       "       [[ 86,  86, 169, 245,  86],\n",
       "        [495, 169, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 495],\n",
       "        [495, 169, 169, 169, 169],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510,  34, 510, 510],\n",
       "        [ 34, 510,  34,  34,  34],\n",
       "        [ 34,  34,  34,  34, 311],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34, 837, 311,  34]]], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 86,  86, 245, 169,  86],\n",
       "        [169, 495, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 169],\n",
       "        [169, 169, 169, 495, 169],\n",
       "        [510, 169, 242, 169, 169],\n",
       "        [510, 510, 104, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510,  34],\n",
       "        [ 34, 510,  34, 510, 510],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34,  98, 657,  98]],\n",
       "\n",
       "       [[ 86,  86, 245, 169,  86],\n",
       "        [169, 495, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 495],\n",
       "        [169, 169, 495, 169, 169],\n",
       "        [510, 169, 169, 495, 169],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [ 34, 510, 510,  34,  34],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34, 657,  98, 657]],\n",
       "\n",
       "       [[ 86,  86, 169, 245,  86],\n",
       "        [495, 169, 169, 245, 245],\n",
       "        [169, 169, 169, 495, 495],\n",
       "        [495, 169, 169, 169, 169],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510, 510, 510, 510],\n",
       "        [510, 510,  34, 510, 510],\n",
       "        [ 34, 510,  34,  34,  34],\n",
       "        [ 34,  34,  34,  34, 311],\n",
       "        [ 34,  34,  34,  34,  34],\n",
       "        [ 34,  34, 837, 311,  34]]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": None,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.909417, perp: 997.652, dev_prep: 997.115, (0.550 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 5.794305, perp: 318.350, dev_prep: 309.229, (0.148 sec/step)\n",
      "step 40, loss = 5.805670, perp: 331.517, dev_prep: 279.219, (0.120 sec/step)\n",
      "step 60, loss = 5.322953, perp: 201.430, dev_prep: 239.833, (0.138 sec/step)\n",
      "step 80, loss = 5.293991, perp: 197.184, dev_prep: 224.721, (0.121 sec/step)\n",
      "step 100, loss = 5.069460, perp: 156.294, dev_prep: 174.812, (0.101 sec/step)\n",
      "step 120, loss = 4.902556, perp: 132.322, dev_prep: 123.866, (0.127 sec/step)\n",
      "step 140, loss = 5.067151, perp: 156.738, dev_prep: 137.191, (0.111 sec/step)\n",
      "step 160, loss = 4.777549, perp: 117.008, dev_prep: 106.300, (0.091 sec/step)\n",
      "step 180, loss = 4.720104, perp: 108.221, dev_prep: 114.713, (0.132 sec/step)\n",
      "step 200, loss = 4.678106, perp: 101.960, dev_prep: 132.185, (0.106 sec/step)\n",
      "step 220, loss = 4.806243, perp: 121.340, dev_prep: 122.345, (0.188 sec/step)\n",
      "step 240, loss = 4.876970, perp: 129.360, dev_prep: 113.929, (0.129 sec/step)\n",
      "step 260, loss = 4.745822, perp: 114.183, dev_prep: 113.330, (0.155 sec/step)\n",
      "step 280, loss = 4.810163, perp: 122.051, dev_prep: 124.920, (0.082 sec/step)\n",
      "step 300, loss = 4.810503, perp: 120.454, dev_prep: 106.244, (0.112 sec/step)\n",
      "step 320, loss = 4.763212, perp: 116.067, dev_prep: 121.589, (0.144 sec/step)\n",
      "step 340, loss = 4.628365, perp: 101.290, dev_prep: 141.803, (0.127 sec/step)\n",
      "step 360, loss = 4.821845, perp: 122.925, dev_prep: 103.229, (0.106 sec/step)\n",
      "step 380, loss = 4.624307, perp: 100.900, dev_prep: 115.354, (0.095 sec/step)\n",
      "step 400, loss = 4.766724, perp: 115.536, dev_prep: 89.892, (0.112 sec/step)\n",
      "step 420, loss = 4.563806, perp: 93.708, dev_prep: 96.354, (0.122 sec/step)\n",
      "step 440, loss = 4.653395, perp: 97.918, dev_prep: 89.897, (0.134 sec/step)\n",
      "step 460, loss = 4.619597, perp: 100.760, dev_prep: 110.265, (0.151 sec/step)\n",
      "step 480, loss = 4.451239, perp: 84.264, dev_prep: 89.995, (0.151 sec/step)\n",
      "step 500, loss = 4.566277, perp: 95.856, dev_prep: 95.549, (0.116 sec/step)\n",
      "step 520, loss = 4.350877, perp: 76.823, dev_prep: 85.598, (0.117 sec/step)\n",
      "step 540, loss = 4.356247, perp: 76.548, dev_prep: 81.200, (0.089 sec/step)\n",
      "step 560, loss = 4.399222, perp: 80.962, dev_prep: 73.561, (0.097 sec/step)\n",
      "step 580, loss = 4.289224, perp: 71.911, dev_prep: 69.210, (0.117 sec/step)\n",
      "step 600, loss = 4.316860, perp: 73.929, dev_prep: 73.834, (0.112 sec/step)\n",
      "step 620, loss = 4.053160, perp: 56.941, dev_prep: 73.025, (0.119 sec/step)\n",
      "step 640, loss = 4.433748, perp: 82.836, dev_prep: 61.540, (0.103 sec/step)\n",
      "step 660, loss = 4.277940, perp: 71.220, dev_prep: 67.112, (0.161 sec/step)\n",
      "step 680, loss = 4.275095, perp: 70.494, dev_prep: 67.545, (0.091 sec/step)\n",
      "step 700, loss = 4.285184, perp: 71.703, dev_prep: 70.407, (0.103 sec/step)\n",
      "step 720, loss = 4.104914, perp: 59.883, dev_prep: 70.735, (0.135 sec/step)\n",
      "step 740, loss = 4.213318, perp: 66.725, dev_prep: 70.892, (0.141 sec/step)\n",
      "step 760, loss = 4.010458, perp: 54.280, dev_prep: 65.564, (0.146 sec/step)\n",
      "step 780, loss = 4.099258, perp: 58.920, dev_prep: 56.048, (0.154 sec/step)\n",
      "step 800, loss = 4.200101, perp: 64.812, dev_prep: 60.412, (0.157 sec/step)\n",
      "step 820, loss = 4.144338, perp: 62.441, dev_prep: 60.831, (0.113 sec/step)\n",
      "step 840, loss = 4.119390, perp: 60.709, dev_prep: 60.896, (0.116 sec/step)\n",
      "step 860, loss = 4.073042, perp: 57.708, dev_prep: 48.623, (0.107 sec/step)\n",
      "step 880, loss = 4.040085, perp: 55.378, dev_prep: 57.227, (0.102 sec/step)\n",
      "step 900, loss = 4.062515, perp: 57.129, dev_prep: 50.088, (0.128 sec/step)\n",
      "step 920, loss = 3.993901, perp: 53.044, dev_prep: 55.081, (0.118 sec/step)\n",
      "step 940, loss = 3.967203, perp: 52.068, dev_prep: 60.236, (0.112 sec/step)\n",
      "step 960, loss = 4.057008, perp: 55.645, dev_prep: 54.897, (0.101 sec/step)\n",
      "step 980, loss = 4.065835, perp: 57.269, dev_prep: 48.872, (0.133 sec/step)\n",
      "step 1000, loss = 3.901037, perp: 48.450, dev_prep: 46.778, (0.126 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 3.923760, perp: 50.617, dev_prep: 50.020, (0.114 sec/step)\n",
      "step 1040, loss = 3.853324, perp: 47.709, dev_prep: 52.003, (0.108 sec/step)\n",
      "step 1060, loss = 3.880513, perp: 47.881, dev_prep: 51.542, (0.117 sec/step)\n",
      "step 1080, loss = 3.885079, perp: 48.982, dev_prep: 53.402, (0.092 sec/step)\n",
      "step 1100, loss = 4.110652, perp: 59.317, dev_prep: 52.958, (0.152 sec/step)\n",
      "step 1120, loss = 3.854394, perp: 46.588, dev_prep: 56.197, (0.095 sec/step)\n",
      "step 1140, loss = 3.830283, perp: 44.879, dev_prep: 60.050, (0.115 sec/step)\n",
      "step 1160, loss = 3.959951, perp: 49.567, dev_prep: 45.088, (0.155 sec/step)\n",
      "step 1180, loss = 3.867873, perp: 47.150, dev_prep: 49.704, (0.095 sec/step)\n",
      "step 1200, loss = 3.845112, perp: 45.789, dev_prep: 55.964, (0.140 sec/step)\n",
      "step 1220, loss = 3.967147, perp: 50.747, dev_prep: 56.404, (0.138 sec/step)\n",
      "step 1240, loss = 3.899945, perp: 48.909, dev_prep: 48.177, (0.100 sec/step)\n",
      "step 1260, loss = 3.874472, perp: 47.972, dev_prep: 51.369, (0.123 sec/step)\n",
      "step 1280, loss = 3.844050, perp: 46.929, dev_prep: 53.832, (0.134 sec/step)\n",
      "step 1300, loss = 3.721169, perp: 40.727, dev_prep: 49.727, (0.092 sec/step)\n",
      "step 1320, loss = 3.988045, perp: 52.938, dev_prep: 49.328, (0.095 sec/step)\n",
      "step 1340, loss = 3.801526, perp: 44.034, dev_prep: 46.942, (0.098 sec/step)\n",
      "step 1360, loss = 3.836159, perp: 44.941, dev_prep: 46.228, (0.150 sec/step)\n",
      "step 1380, loss = 3.889573, perp: 47.336, dev_prep: 42.860, (0.130 sec/step)\n",
      "step 1400, loss = 3.901113, perp: 48.688, dev_prep: 52.740, (0.134 sec/step)\n",
      "step 1420, loss = 3.898068, perp: 48.318, dev_prep: 47.601, (0.135 sec/step)\n",
      "step 1440, loss = 3.916115, perp: 49.972, dev_prep: 45.795, (0.182 sec/step)\n",
      "step 1460, loss = 3.785841, perp: 43.119, dev_prep: 47.385, (0.110 sec/step)\n",
      "step 1480, loss = 3.779644, perp: 42.898, dev_prep: 51.195, (0.120 sec/step)\n",
      "step 1500, loss = 3.887855, perp: 47.840, dev_prep: 45.100, (0.091 sec/step)\n",
      "step 1520, loss = 3.654229, perp: 37.635, dev_prep: 41.312, (0.102 sec/step)\n",
      "step 1540, loss = 3.817368, perp: 44.457, dev_prep: 49.360, (0.114 sec/step)\n",
      "step 1560, loss = 3.693489, perp: 39.395, dev_prep: 45.090, (0.080 sec/step)\n",
      "step 1580, loss = 3.764172, perp: 41.527, dev_prep: 41.316, (0.159 sec/step)\n",
      "step 1600, loss = 3.811559, perp: 44.881, dev_prep: 41.033, (0.145 sec/step)\n",
      "step 1620, loss = 3.682194, perp: 39.032, dev_prep: 43.660, (0.093 sec/step)\n",
      "step 1640, loss = 3.790248, perp: 44.298, dev_prep: 43.417, (0.113 sec/step)\n",
      "step 1660, loss = 3.756977, perp: 40.553, dev_prep: 46.834, (0.106 sec/step)\n",
      "step 1680, loss = 3.792495, perp: 43.779, dev_prep: 39.777, (0.098 sec/step)\n",
      "step 1700, loss = 3.812443, perp: 43.765, dev_prep: 41.677, (0.133 sec/step)\n",
      "step 1720, loss = 3.600577, perp: 36.113, dev_prep: 39.552, (0.169 sec/step)\n",
      "step 1740, loss = 3.824756, perp: 45.017, dev_prep: 39.134, (0.100 sec/step)\n",
      "step 1760, loss = 3.681454, perp: 38.308, dev_prep: 45.260, (0.092 sec/step)\n",
      "step 1780, loss = 3.657481, perp: 37.969, dev_prep: 43.688, (0.141 sec/step)\n",
      "step 1800, loss = 3.706694, perp: 39.104, dev_prep: 40.691, (0.125 sec/step)\n",
      "step 1820, loss = 3.682507, perp: 39.015, dev_prep: 42.135, (0.138 sec/step)\n",
      "step 1840, loss = 3.814725, perp: 41.437, dev_prep: 38.810, (0.136 sec/step)\n",
      "step 1860, loss = 3.673049, perp: 38.222, dev_prep: 41.463, (0.144 sec/step)\n",
      "step 1880, loss = 3.572978, perp: 34.881, dev_prep: 41.414, (0.104 sec/step)\n",
      "step 1900, loss = 3.621989, perp: 36.094, dev_prep: 42.044, (0.113 sec/step)\n",
      "step 1920, loss = 3.741251, perp: 41.337, dev_prep: 37.734, (0.110 sec/step)\n",
      "step 1940, loss = 3.644481, perp: 37.950, dev_prep: 36.280, (0.102 sec/step)\n",
      "step 1960, loss = 3.715479, perp: 40.384, dev_prep: 38.577, (0.121 sec/step)\n",
      "step 1980, loss = 3.647747, perp: 37.134, dev_prep: 40.196, (0.107 sec/step)\n",
      "step 2000, loss = 3.630505, perp: 36.602, dev_prep: 38.934, (0.112 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.600776, perp: 35.810, dev_prep: 37.602, (0.116 sec/step)\n",
      "step 2040, loss = 3.723135, perp: 40.871, dev_prep: 38.046, (0.124 sec/step)\n",
      "step 2060, loss = 3.661517, perp: 38.244, dev_prep: 33.795, (0.131 sec/step)\n",
      "step 2080, loss = 3.573232, perp: 34.736, dev_prep: 43.588, (0.107 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.624309, perp: 37.306, dev_prep: 36.123, (0.106 sec/step)\n",
      "step 2120, loss = 3.498974, perp: 32.748, dev_prep: 33.258, (0.114 sec/step)\n",
      "step 2140, loss = 3.588278, perp: 36.269, dev_prep: 36.895, (0.109 sec/step)\n",
      "step 2160, loss = 3.683581, perp: 38.898, dev_prep: 37.732, (0.131 sec/step)\n",
      "step 2180, loss = 3.644855, perp: 36.771, dev_prep: 34.358, (0.091 sec/step)\n",
      "step 2200, loss = 3.524454, perp: 33.264, dev_prep: 42.002, (0.143 sec/step)\n",
      "step 2220, loss = 3.582700, perp: 35.522, dev_prep: 38.908, (0.093 sec/step)\n",
      "step 2240, loss = 3.474727, perp: 31.798, dev_prep: 33.005, (0.115 sec/step)\n",
      "step 2260, loss = 3.550777, perp: 33.932, dev_prep: 32.322, (0.074 sec/step)\n",
      "step 2280, loss = 3.607729, perp: 36.200, dev_prep: 40.142, (0.135 sec/step)\n",
      "step 2300, loss = 3.527058, perp: 33.666, dev_prep: 32.484, (0.131 sec/step)\n",
      "step 2320, loss = 3.533964, perp: 33.855, dev_prep: 34.540, (0.125 sec/step)\n",
      "step 2340, loss = 3.502652, perp: 32.364, dev_prep: 36.838, (0.127 sec/step)\n",
      "step 2360, loss = 3.528821, perp: 33.919, dev_prep: 37.804, (0.159 sec/step)\n",
      "step 2380, loss = 3.448335, perp: 30.332, dev_prep: 35.133, (0.094 sec/step)\n",
      "step 2400, loss = 3.526238, perp: 32.604, dev_prep: 37.885, (0.136 sec/step)\n",
      "step 2420, loss = 3.536680, perp: 34.755, dev_prep: 35.033, (0.154 sec/step)\n",
      "step 2440, loss = 3.477669, perp: 32.071, dev_prep: 34.759, (0.118 sec/step)\n",
      "step 2460, loss = 3.442074, perp: 30.295, dev_prep: 29.628, (0.111 sec/step)\n",
      "step 2480, loss = 3.613874, perp: 36.139, dev_prep: 40.069, (0.112 sec/step)\n",
      "step 2500, loss = 3.660649, perp: 37.783, dev_prep: 34.345, (0.112 sec/step)\n",
      "step 2520, loss = 3.524449, perp: 33.514, dev_prep: 32.708, (0.141 sec/step)\n",
      "step 2540, loss = 3.561606, perp: 33.058, dev_prep: 35.847, (0.134 sec/step)\n",
      "step 2560, loss = 3.411387, perp: 29.806, dev_prep: 35.148, (0.103 sec/step)\n",
      "step 2580, loss = 3.456617, perp: 31.102, dev_prep: 32.817, (0.111 sec/step)\n",
      "step 2600, loss = 3.459834, perp: 30.928, dev_prep: 31.869, (0.123 sec/step)\n",
      "step 2620, loss = 3.468817, perp: 31.245, dev_prep: 31.160, (0.114 sec/step)\n",
      "step 2640, loss = 3.497629, perp: 32.301, dev_prep: 29.371, (0.107 sec/step)\n",
      "step 2660, loss = 3.550122, perp: 33.788, dev_prep: 32.925, (0.127 sec/step)\n",
      "step 2680, loss = 3.274528, perp: 26.117, dev_prep: 31.117, (0.134 sec/step)\n",
      "step 2700, loss = 3.462940, perp: 30.961, dev_prep: 31.840, (0.120 sec/step)\n",
      "step 2720, loss = 3.592341, perp: 34.812, dev_prep: 34.039, (0.096 sec/step)\n",
      "step 2740, loss = 3.327205, perp: 27.381, dev_prep: 30.091, (0.119 sec/step)\n",
      "step 2760, loss = 3.426009, perp: 30.292, dev_prep: 32.230, (0.158 sec/step)\n",
      "step 2780, loss = 3.480503, perp: 31.964, dev_prep: 32.970, (0.167 sec/step)\n",
      "step 2800, loss = 3.456561, perp: 30.662, dev_prep: 33.730, (0.166 sec/step)\n",
      "step 2820, loss = 3.388034, perp: 29.378, dev_prep: 29.050, (0.151 sec/step)\n",
      "step 2840, loss = 3.450834, perp: 29.915, dev_prep: 30.606, (0.130 sec/step)\n",
      "step 2860, loss = 3.555372, perp: 34.100, dev_prep: 31.446, (0.112 sec/step)\n",
      "step 2880, loss = 3.440923, perp: 29.930, dev_prep: 35.760, (0.105 sec/step)\n",
      "step 2900, loss = 3.514012, perp: 32.599, dev_prep: 33.609, (0.182 sec/step)\n",
      "step 2920, loss = 3.591434, perp: 35.256, dev_prep: 31.742, (0.117 sec/step)\n",
      "step 2940, loss = 3.522312, perp: 32.766, dev_prep: 30.182, (0.136 sec/step)\n",
      "step 2960, loss = 3.422447, perp: 29.847, dev_prep: 32.006, (0.133 sec/step)\n",
      "step 2980, loss = 3.325579, perp: 27.365, dev_prep: 27.954, (0.144 sec/step)\n",
      "step 3000, loss = 3.419391, perp: 29.876, dev_prep: 29.004, (0.160 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.405795, perp: 29.326, dev_prep: 29.685, (0.120 sec/step)\n",
      "step 3040, loss = 3.422117, perp: 29.386, dev_prep: 31.868, (0.121 sec/step)\n",
      "step 3060, loss = 3.415816, perp: 29.653, dev_prep: 32.313, (0.186 sec/step)\n",
      "step 3080, loss = 3.390990, perp: 28.119, dev_prep: 33.371, (0.141 sec/step)\n",
      "step 3100, loss = 3.497313, perp: 32.456, dev_prep: 31.794, (0.111 sec/step)\n",
      "step 3120, loss = 3.349836, perp: 28.038, dev_prep: 30.167, (0.133 sec/step)\n",
      "step 3140, loss = 3.370212, perp: 28.713, dev_prep: 27.499, (0.164 sec/step)\n",
      "step 3160, loss = 3.390161, perp: 27.665, dev_prep: 27.830, (0.129 sec/step)\n",
      "step 3180, loss = 3.360482, perp: 27.992, dev_prep: 30.792, (0.122 sec/step)\n",
      "step 3200, loss = 3.269872, perp: 25.686, dev_prep: 27.794, (0.173 sec/step)\n",
      "step 3220, loss = 3.307123, perp: 26.663, dev_prep: 32.404, (0.171 sec/step)\n",
      "step 3240, loss = 3.236712, perp: 24.811, dev_prep: 29.149, (0.121 sec/step)\n",
      "step 3260, loss = 3.318569, perp: 27.427, dev_prep: 28.115, (0.137 sec/step)\n",
      "step 3280, loss = 3.320203, perp: 26.856, dev_prep: 29.364, (0.144 sec/step)\n",
      "step 3300, loss = 3.365480, perp: 27.768, dev_prep: 28.865, (0.093 sec/step)\n",
      "step 3320, loss = 3.288035, perp: 25.331, dev_prep: 30.007, (0.098 sec/step)\n",
      "step 3340, loss = 3.353956, perp: 28.262, dev_prep: 32.097, (0.178 sec/step)\n",
      "step 3360, loss = 3.326618, perp: 27.338, dev_prep: 29.052, (0.094 sec/step)\n",
      "step 3380, loss = 3.330586, perp: 27.306, dev_prep: 28.808, (0.141 sec/step)\n",
      "step 3400, loss = 3.421497, perp: 29.179, dev_prep: 27.300, (0.086 sec/step)\n",
      "step 3420, loss = 3.251067, perp: 25.303, dev_prep: 28.471, (0.097 sec/step)\n",
      "step 3440, loss = 3.245112, perp: 25.478, dev_prep: 26.800, (0.132 sec/step)\n",
      "step 3460, loss = 3.316263, perp: 26.843, dev_prep: 26.752, (0.119 sec/step)\n",
      "step 3480, loss = 3.252242, perp: 25.304, dev_prep: 24.955, (0.154 sec/step)\n",
      "step 3500, loss = 3.392045, perp: 29.236, dev_prep: 28.744, (0.157 sec/step)\n",
      "step 3520, loss = 3.185883, perp: 23.630, dev_prep: 30.322, (0.168 sec/step)\n",
      "step 3540, loss = 3.272281, perp: 25.884, dev_prep: 29.224, (0.116 sec/step)\n",
      "step 3560, loss = 3.141896, perp: 22.411, dev_prep: 25.096, (0.113 sec/step)\n",
      "step 3580, loss = 3.204093, perp: 24.164, dev_prep: 26.943, (0.112 sec/step)\n",
      "step 3600, loss = 3.274183, perp: 25.795, dev_prep: 26.595, (0.136 sec/step)\n",
      "step 3620, loss = 3.379921, perp: 28.363, dev_prep: 29.425, (0.114 sec/step)\n",
      "step 3640, loss = 3.334909, perp: 27.211, dev_prep: 25.939, (0.186 sec/step)\n",
      "step 3660, loss = 3.164442, perp: 23.047, dev_prep: 24.516, (0.155 sec/step)\n",
      "step 3680, loss = 3.285362, perp: 25.892, dev_prep: 27.180, (0.138 sec/step)\n",
      "step 3700, loss = 3.138245, perp: 22.844, dev_prep: 25.689, (0.169 sec/step)\n",
      "step 3720, loss = 3.132313, perp: 22.218, dev_prep: 23.945, (0.139 sec/step)\n",
      "step 3740, loss = 3.243266, perp: 24.897, dev_prep: 25.487, (0.148 sec/step)\n",
      "step 3760, loss = 3.227435, perp: 24.303, dev_prep: 24.156, (0.127 sec/step)\n",
      "step 3780, loss = 3.201022, perp: 24.221, dev_prep: 24.817, (0.097 sec/step)\n",
      "step 3800, loss = 3.168287, perp: 23.192, dev_prep: 26.201, (0.130 sec/step)\n",
      "step 3820, loss = 3.133632, perp: 22.363, dev_prep: 23.899, (0.110 sec/step)\n",
      "step 3840, loss = 3.237612, perp: 24.794, dev_prep: 28.115, (0.136 sec/step)\n",
      "step 3860, loss = 3.285178, perp: 26.264, dev_prep: 21.924, (0.106 sec/step)\n",
      "step 3880, loss = 3.224459, perp: 25.417, dev_prep: 24.506, (0.099 sec/step)\n",
      "step 3900, loss = 3.223991, perp: 23.198, dev_prep: 28.008, (0.094 sec/step)\n",
      "step 3920, loss = 3.192141, perp: 24.101, dev_prep: 26.364, (0.137 sec/step)\n",
      "step 3940, loss = 3.190255, perp: 24.196, dev_prep: 26.061, (0.141 sec/step)\n",
      "step 3960, loss = 3.235976, perp: 24.914, dev_prep: 26.409, (0.095 sec/step)\n",
      "step 3980, loss = 3.240722, perp: 25.368, dev_prep: 21.930, (0.123 sec/step)\n",
      "step 4000, loss = 3.145388, perp: 22.866, dev_prep: 25.140, (0.098 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.138034, perp: 22.447, dev_prep: 26.334, (0.089 sec/step)\n",
      "step 4040, loss = 3.147613, perp: 22.775, dev_prep: 23.199, (0.131 sec/step)\n",
      "step 4060, loss = 3.031439, perp: 20.279, dev_prep: 24.999, (0.137 sec/step)\n",
      "step 4080, loss = 3.070774, perp: 21.266, dev_prep: 22.408, (0.135 sec/step)\n",
      "step 4100, loss = 3.203770, perp: 23.465, dev_prep: 22.012, (0.095 sec/step)\n",
      "step 4120, loss = 3.117721, perp: 22.074, dev_prep: 23.535, (0.117 sec/step)\n",
      "step 4140, loss = 3.172398, perp: 23.274, dev_prep: 22.386, (0.114 sec/step)\n",
      "step 4160, loss = 3.064231, perp: 20.949, dev_prep: 25.640, (0.126 sec/step)\n",
      "step 4180, loss = 3.112333, perp: 22.083, dev_prep: 24.536, (0.124 sec/step)\n",
      "step 4200, loss = 3.118182, perp: 22.129, dev_prep: 23.898, (0.123 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.129116, perp: 22.512, dev_prep: 22.499, (0.111 sec/step)\n",
      "step 4240, loss = 3.115620, perp: 22.187, dev_prep: 24.689, (0.158 sec/step)\n",
      "step 4260, loss = 2.975064, perp: 19.324, dev_prep: 21.346, (0.196 sec/step)\n",
      "step 4280, loss = 3.095112, perp: 21.542, dev_prep: 24.923, (0.105 sec/step)\n",
      "step 4300, loss = 3.225891, perp: 24.621, dev_prep: 24.694, (0.128 sec/step)\n",
      "step 4320, loss = 3.140077, perp: 22.623, dev_prep: 24.645, (0.118 sec/step)\n",
      "step 4340, loss = 3.048257, perp: 20.519, dev_prep: 22.940, (0.112 sec/step)\n",
      "step 4360, loss = 2.919043, perp: 18.084, dev_prep: 22.680, (0.111 sec/step)\n",
      "step 4380, loss = 3.133595, perp: 22.643, dev_prep: 21.406, (0.118 sec/step)\n",
      "step 4400, loss = 3.052035, perp: 20.595, dev_prep: 22.598, (0.127 sec/step)\n",
      "step 4420, loss = 3.010403, perp: 19.574, dev_prep: 23.259, (0.117 sec/step)\n",
      "step 4440, loss = 2.953592, perp: 18.735, dev_prep: 22.867, (0.113 sec/step)\n",
      "step 4460, loss = 3.051817, perp: 20.903, dev_prep: 22.008, (0.107 sec/step)\n",
      "step 4480, loss = 3.090476, perp: 20.891, dev_prep: 23.335, (0.143 sec/step)\n",
      "step 4500, loss = 3.038911, perp: 20.184, dev_prep: 21.167, (0.133 sec/step)\n",
      "step 4520, loss = 2.910966, perp: 18.075, dev_prep: 20.238, (0.132 sec/step)\n",
      "step 4540, loss = 2.976638, perp: 20.132, dev_prep: 23.466, (0.107 sec/step)\n",
      "step 4560, loss = 2.945545, perp: 18.353, dev_prep: 21.284, (0.124 sec/step)\n",
      "step 4580, loss = 3.037273, perp: 20.247, dev_prep: 23.070, (0.146 sec/step)\n",
      "step 4600, loss = 3.055167, perp: 20.825, dev_prep: 21.389, (0.139 sec/step)\n",
      "step 4620, loss = 2.945091, perp: 18.372, dev_prep: 20.343, (0.104 sec/step)\n",
      "step 4640, loss = 3.069815, perp: 21.163, dev_prep: 22.527, (0.125 sec/step)\n",
      "step 4660, loss = 3.052437, perp: 20.549, dev_prep: 20.761, (0.148 sec/step)\n",
      "step 4680, loss = 2.907744, perp: 17.785, dev_prep: 20.021, (0.145 sec/step)\n",
      "step 4700, loss = 3.074524, perp: 21.055, dev_prep: 17.339, (0.123 sec/step)\n",
      "step 4720, loss = 2.944858, perp: 17.924, dev_prep: 19.656, (0.117 sec/step)\n",
      "step 4740, loss = 2.939816, perp: 18.584, dev_prep: 20.154, (0.135 sec/step)\n",
      "step 4760, loss = 3.044372, perp: 19.767, dev_prep: 19.013, (0.156 sec/step)\n",
      "step 4780, loss = 2.982720, perp: 18.910, dev_prep: 19.985, (0.124 sec/step)\n",
      "step 4800, loss = 2.957863, perp: 19.006, dev_prep: 21.882, (0.085 sec/step)\n",
      "step 4820, loss = 2.930435, perp: 18.228, dev_prep: 18.731, (0.155 sec/step)\n",
      "step 4840, loss = 2.948590, perp: 18.846, dev_prep: 18.324, (0.092 sec/step)\n",
      "step 4860, loss = 2.917758, perp: 18.350, dev_prep: 19.084, (0.129 sec/step)\n",
      "step 4880, loss = 2.898045, perp: 17.989, dev_prep: 18.604, (0.128 sec/step)\n",
      "step 4900, loss = 2.876081, perp: 17.893, dev_prep: 20.956, (0.113 sec/step)\n",
      "step 4920, loss = 2.741016, perp: 15.196, dev_prep: 19.672, (0.186 sec/step)\n",
      "step 4940, loss = 2.813561, perp: 16.465, dev_prep: 17.398, (0.120 sec/step)\n",
      "step 4960, loss = 2.962161, perp: 18.876, dev_prep: 19.364, (0.152 sec/step)\n",
      "step 4980, loss = 2.885315, perp: 17.545, dev_prep: 17.274, (0.090 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"CE loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.097691962492878"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
