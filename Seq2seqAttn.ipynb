{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        Returns:\n",
    "            h_0: [batch_size, num_units]\n",
    "            context_0: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, dtype=tf.float32, forget_bias=1.0,\n",
    "                  name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            memory = encoder_outputs\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=True,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099895, 0.00099375, 0.00100089, ..., 0.00099835,\n",
       "         0.0009961 , 0.00099775],\n",
       "        [0.00099943, 0.00099426, 0.00100015, ..., 0.00099751,\n",
       "         0.00099748, 0.00099536],\n",
       "        [0.00099922, 0.00099476, 0.00099993, ..., 0.00099658,\n",
       "         0.00099852, 0.00099408],\n",
       "        [0.00099872, 0.00099532, 0.00100003, ..., 0.00099566,\n",
       "         0.0009992 , 0.00099349]],\n",
       "\n",
       "       [[0.00099596, 0.00099735, 0.00099903, ..., 0.00099552,\n",
       "         0.00099743, 0.00099682],\n",
       "        [0.00099635, 0.00099655, 0.00100005, ..., 0.00099543,\n",
       "         0.00099774, 0.0009962 ],\n",
       "        [0.00099629, 0.00099666, 0.00100072, ..., 0.0009954 ,\n",
       "         0.00099781, 0.00099628],\n",
       "        [0.00099603, 0.0009973 , 0.00100115, ..., 0.00099583,\n",
       "         0.00099758, 0.00099657]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=False, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[ 8.72506062e-04, -3.96022369e-06,  2.87310104e-04, ...,\n",
       "           7.98192632e-04, -9.98562900e-04, -2.15223082e-03],\n",
       "         [ 8.72506062e-04, -3.96022369e-06,  2.87310104e-04, ...,\n",
       "           7.98192632e-04, -9.98562900e-04, -2.15223082e-03],\n",
       "         [ 8.72506062e-04, -3.96022369e-06,  2.87310104e-04, ...,\n",
       "           7.98192632e-04, -9.98562900e-04, -2.15223082e-03],\n",
       "         [ 8.72506062e-04, -3.96022369e-06,  2.87310104e-04, ...,\n",
       "           7.98192632e-04, -9.98562900e-04, -2.15223082e-03],\n",
       "         [ 8.72506062e-04, -3.96022369e-06,  2.87310104e-04, ...,\n",
       "           7.98192632e-04, -9.98562900e-04, -2.15223082e-03]],\n",
       "\n",
       "        [[ 7.20189302e-04, -3.46703804e-04,  2.73300364e-04, ...,\n",
       "           1.59756735e-03, -1.48663565e-03, -2.88707134e-03],\n",
       "         [ 7.20189302e-04, -3.46703804e-04,  2.73300364e-04, ...,\n",
       "           1.59756735e-03, -1.48663565e-03, -2.88707134e-03],\n",
       "         [ 1.72360835e-03,  1.25209976e-04,  4.34149144e-04, ...,\n",
       "           9.94603499e-04, -2.07391474e-03, -2.38280720e-03],\n",
       "         [ 7.20189302e-04, -3.46703804e-04,  2.73300364e-04, ...,\n",
       "           1.59756735e-03, -1.48663565e-03, -2.88707134e-03],\n",
       "         [ 1.15835690e-03, -1.36491872e-04,  5.71122509e-04, ...,\n",
       "           1.39308174e-03, -1.41138653e-03, -2.54820241e-03]],\n",
       "\n",
       "        [[ 1.12682418e-03, -6.13103679e-04,  5.47639385e-04, ...,\n",
       "           1.42039906e-03, -1.79591333e-03, -3.06186220e-03],\n",
       "         [ 1.76175497e-03,  8.09328048e-05, -1.48063758e-04, ...,\n",
       "           1.15562591e-03, -2.23107799e-03, -2.44198274e-03],\n",
       "         [ 1.29537913e-03, -4.61533200e-05, -6.16680074e-04, ...,\n",
       "           1.44015905e-03, -1.72355212e-03, -2.92627001e-03],\n",
       "         [ 1.76175497e-03,  8.09328048e-05, -1.48063758e-04, ...,\n",
       "           1.15562591e-03, -2.23107799e-03, -2.44198274e-03],\n",
       "         [ 1.12682418e-03, -6.13103679e-04,  5.47639385e-04, ...,\n",
       "           1.42039906e-03, -1.79591333e-03, -3.06186220e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.47755151e-04, -3.05316527e-03, -8.86141323e-04, ...,\n",
       "          -1.12316199e-03, -7.90364807e-04,  7.01802666e-04],\n",
       "         [-5.17723907e-04, -2.84911180e-03, -3.53078009e-04, ...,\n",
       "          -1.06090575e-03, -4.84567834e-04,  3.29901406e-04],\n",
       "         [-5.17723907e-04, -2.84911180e-03, -3.53078009e-04, ...,\n",
       "          -1.06090575e-03, -4.84567834e-04,  3.29901406e-04],\n",
       "         [-5.22303162e-04, -3.91752599e-03,  1.05439930e-03, ...,\n",
       "          -9.88244894e-04, -1.09849987e-03,  1.55568076e-03],\n",
       "         [-5.17723907e-04, -2.84911180e-03, -3.53078009e-04, ...,\n",
       "          -1.06090575e-03, -4.84567834e-04,  3.29901406e-04]],\n",
       "\n",
       "        [[-8.01816117e-04, -2.52537359e-03, -6.34818105e-04, ...,\n",
       "          -1.83387136e-03, -6.62687467e-04,  5.62556612e-04],\n",
       "         [-1.23848766e-03, -2.35013687e-03,  1.59388059e-04, ...,\n",
       "          -1.73468224e-03, -2.49100383e-04,  1.87591824e-04],\n",
       "         [-1.23848766e-03, -2.35013687e-03,  1.59388059e-04, ...,\n",
       "          -1.73468224e-03, -2.49100383e-04,  1.87591824e-04],\n",
       "         [-1.67299993e-03, -3.06077069e-03,  1.94907770e-03, ...,\n",
       "          -1.62353762e-03, -1.11418020e-03, -3.54389631e-05],\n",
       "         [-1.67299993e-03, -3.06077069e-03,  1.94907770e-03, ...,\n",
       "          -1.62353762e-03, -1.11418020e-03, -3.54389631e-05]],\n",
       "\n",
       "        [[-2.24484969e-03, -2.62476318e-03,  2.91254092e-03, ...,\n",
       "          -2.64742016e-03, -2.15959223e-03,  3.78084078e-04],\n",
       "         [-1.32918288e-03, -1.84737053e-03, -1.66267739e-04, ...,\n",
       "          -2.44262326e-03, -4.77234833e-04,  2.61729001e-04],\n",
       "         [-1.39136810e-03, -2.18344503e-03,  6.69874717e-05, ...,\n",
       "          -1.71364297e-03, -1.06152811e-03,  5.57603431e-04],\n",
       "         [-1.88560574e-03, -2.54739029e-03,  1.63253327e-03, ...,\n",
       "          -2.34309724e-03, -1.45212351e-03,  1.36383518e-04],\n",
       "         [-1.65924011e-03, -1.53685082e-03,  2.79585365e-05, ...,\n",
       "          -2.90602073e-03, -1.41994923e-03,  6.43515494e-04]]],\n",
       "\n",
       "\n",
       "       [[[ 8.71532655e-04, -9.56093572e-06,  3.21271713e-04, ...,\n",
       "           7.53673841e-04, -1.01286662e-03, -2.10808986e-03],\n",
       "         [ 8.71532655e-04, -9.56093572e-06,  3.21271713e-04, ...,\n",
       "           7.53673841e-04, -1.01286662e-03, -2.10808986e-03],\n",
       "         [ 8.71532655e-04, -9.56093572e-06,  3.21271713e-04, ...,\n",
       "           7.53673841e-04, -1.01286662e-03, -2.10808986e-03],\n",
       "         [ 8.71532655e-04, -9.56093572e-06,  3.21271713e-04, ...,\n",
       "           7.53673841e-04, -1.01286662e-03, -2.10808986e-03],\n",
       "         [ 8.71532655e-04, -9.56093572e-06,  3.21271713e-04, ...,\n",
       "           7.53673841e-04, -1.01286662e-03, -2.10808986e-03]],\n",
       "\n",
       "        [[ 7.18809431e-04, -3.66295018e-04,  3.53983371e-04, ...,\n",
       "           1.48189056e-03, -1.52261986e-03, -2.78515392e-03],\n",
       "         [ 7.18809431e-04, -3.66295018e-04,  3.53983371e-04, ...,\n",
       "           1.48189056e-03, -1.52261986e-03, -2.78515392e-03],\n",
       "         [ 1.15708774e-03, -1.56528302e-04,  6.51071547e-04, ...,\n",
       "           1.27790007e-03, -1.44710776e-03, -2.44627823e-03],\n",
       "         [ 7.18809431e-04, -3.66295018e-04,  3.53983371e-04, ...,\n",
       "           1.48189056e-03, -1.52261986e-03, -2.78515392e-03],\n",
       "         [ 1.26055966e-03, -2.12704181e-05,  5.01572686e-05, ...,\n",
       "           1.16350362e-03, -1.60322408e-03, -2.75616674e-03]],\n",
       "\n",
       "        [[ 1.29692233e-03, -8.44466267e-05, -4.89225320e-04, ...,\n",
       "           1.23830920e-03, -1.78712490e-03, -2.77266768e-03],\n",
       "         [ 1.62302505e-03, -3.12868331e-04, -2.70023884e-04, ...,\n",
       "           1.18691812e-03, -1.85486209e-03, -2.59823911e-03],\n",
       "         [ 1.12784223e-03, -6.50164788e-04,  6.74065202e-04, ...,\n",
       "           1.21834222e-03, -1.85675034e-03, -2.90778792e-03],\n",
       "         [ 1.12784223e-03, -6.50164788e-04,  6.74065202e-04, ...,\n",
       "           1.21834222e-03, -1.85675034e-03, -2.90778792e-03],\n",
       "         [ 1.62302505e-03, -3.12868331e-04, -2.70023884e-04, ...,\n",
       "           1.18691812e-03, -1.85486209e-03, -2.59823911e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.85511293e-04, -3.03739961e-03, -1.22562866e-04, ...,\n",
       "          -1.76361599e-03, -7.20076147e-04,  5.97620616e-04],\n",
       "         [-7.04790931e-04, -3.43963131e-03,  1.26363733e-03, ...,\n",
       "          -1.53586769e-03, -8.25346331e-04,  3.54444055e-04],\n",
       "         [-4.85511293e-04, -3.03739961e-03, -1.22562866e-04, ...,\n",
       "          -1.76361599e-03, -7.20076147e-04,  5.97620616e-04],\n",
       "         [-9.16571589e-05, -3.44594615e-03,  1.47799670e-03, ...,\n",
       "          -1.80255587e-03, -8.20449204e-04,  3.63314612e-04],\n",
       "         [-7.04790931e-04, -3.43963131e-03,  1.26363733e-03, ...,\n",
       "          -1.53586769e-03, -8.25346331e-04,  3.54444055e-04]],\n",
       "\n",
       "        [[-1.57573319e-03, -3.01169348e-03,  1.93706481e-03, ...,\n",
       "          -2.23979913e-03, -7.36849033e-04,  2.75066268e-04],\n",
       "         [-1.36453996e-03, -3.67521751e-03,  1.95957394e-03, ...,\n",
       "          -2.40100361e-03, -1.24709564e-03,  1.73994666e-03],\n",
       "         [-1.14263291e-03, -3.18679074e-03,  1.13885768e-03, ...,\n",
       "          -2.33652955e-03, -1.14836369e-03,  6.55980082e-04],\n",
       "         [-1.19872228e-03, -2.55896593e-03,  3.85883090e-04, ...,\n",
       "          -2.49818619e-03, -5.15192747e-04,  4.49111656e-04],\n",
       "         [-1.20070379e-03, -4.13075136e-03,  1.88549899e-03, ...,\n",
       "          -2.38526613e-03, -1.49544748e-03,  2.43068673e-03]],\n",
       "\n",
       "        [[-2.03846116e-03, -3.44019104e-03,  2.32418254e-03, ...,\n",
       "          -2.69602938e-03, -1.35307782e-03,  9.53278621e-04],\n",
       "         [-1.78559194e-03, -2.51695001e-03,  1.60432339e-03, ...,\n",
       "          -3.00906575e-03, -1.10344193e-03,  4.36588249e-04],\n",
       "         [-2.53426144e-03, -2.71182787e-03,  2.75253830e-03, ...,\n",
       "          -3.04814242e-03, -1.31925277e-03,  1.34594971e-04],\n",
       "         [-1.55210984e-03, -4.82867798e-03,  2.18542758e-03, ...,\n",
       "          -2.73622037e-03, -2.19869521e-03,  3.30022071e-03],\n",
       "         [-2.11523380e-03, -2.20742309e-03,  1.79924385e-03, ...,\n",
       "          -3.47089488e-03, -2.04438413e-03,  8.17668624e-04]]],\n",
       "\n",
       "\n",
       "       [[[ 8.84442998e-04,  1.40189295e-05,  2.81008513e-04, ...,\n",
       "           7.08413660e-04, -9.61222278e-04, -2.05806340e-03],\n",
       "         [ 8.84442998e-04,  1.40189295e-05,  2.81008513e-04, ...,\n",
       "           7.08413660e-04, -9.61222278e-04, -2.05806340e-03],\n",
       "         [ 8.84442998e-04,  1.40189295e-05,  2.81008513e-04, ...,\n",
       "           7.08413660e-04, -9.61222278e-04, -2.05806340e-03],\n",
       "         [ 8.84442998e-04,  1.40189295e-05,  2.81008513e-04, ...,\n",
       "           7.08413660e-04, -9.61222278e-04, -2.05806340e-03],\n",
       "         [ 8.84442998e-04,  1.40189295e-05,  2.81008513e-04, ...,\n",
       "           7.08413660e-04, -9.61222278e-04, -2.05806340e-03]],\n",
       "\n",
       "        [[ 7.53445784e-04, -3.03847657e-04,  2.54647515e-04, ...,\n",
       "           1.37627474e-03, -1.38795271e-03, -2.66534532e-03],\n",
       "         [ 7.53445784e-04, -3.03847657e-04,  2.54647515e-04, ...,\n",
       "           1.37627474e-03, -1.38795271e-03, -2.66534532e-03],\n",
       "         [ 7.53445784e-04, -3.03847657e-04,  2.54647515e-04, ...,\n",
       "           1.37627474e-03, -1.38795271e-03, -2.66534532e-03],\n",
       "         [ 1.29552477e-03,  4.14299429e-05, -5.03789124e-05, ...,\n",
       "           1.05838722e-03, -1.46875624e-03, -2.63498700e-03],\n",
       "         [ 7.53445784e-04, -3.03847657e-04,  2.54647515e-04, ...,\n",
       "           1.37627474e-03, -1.38795271e-03, -2.66534532e-03]],\n",
       "\n",
       "        [[ 1.35744619e-03,  2.91053439e-05, -6.55019539e-04, ...,\n",
       "           1.07283809e-03, -1.55761372e-03, -2.57559866e-03],\n",
       "         [ 1.82192062e-03,  1.55119691e-04, -1.84227989e-04, ...,\n",
       "           7.86695455e-04, -2.06435518e-03, -2.09016306e-03],\n",
       "         [ 1.18716620e-03, -5.35328989e-04,  5.09324076e-04, ...,\n",
       "           1.05283281e-03, -1.62540120e-03, -2.71215220e-03],\n",
       "         [ 1.18716620e-03, -5.35328989e-04,  5.09324076e-04, ...,\n",
       "           1.05283281e-03, -1.62540120e-03, -2.71215220e-03],\n",
       "         [ 1.82192062e-03,  1.55119691e-04, -1.84227989e-04, ...,\n",
       "           7.86695455e-04, -2.06435518e-03, -2.09016306e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.98569214e-05, -2.46881880e-03,  2.16290937e-03, ...,\n",
       "          -3.70890979e-04, -2.79291975e-03,  4.63976234e-04],\n",
       "         [-5.49581891e-04, -2.58656219e-03,  1.12458295e-03, ...,\n",
       "          -3.73001327e-04, -1.05143990e-03,  1.78640767e-04],\n",
       "         [-5.49581891e-04, -2.58656219e-03,  1.12458295e-03, ...,\n",
       "          -3.73001327e-04, -1.05143990e-03,  1.78640767e-04],\n",
       "         [ 1.98569214e-05, -2.46881880e-03,  2.16290937e-03, ...,\n",
       "          -3.70890979e-04, -2.79291975e-03,  4.63976234e-04],\n",
       "         [-5.49581891e-04, -2.58656219e-03,  1.12458295e-03, ...,\n",
       "          -3.73001327e-04, -1.05143990e-03,  1.78640767e-04]],\n",
       "\n",
       "        [[-7.51148269e-04, -2.60872254e-03,  1.97911565e-03, ...,\n",
       "          -1.19052676e-03, -2.33220309e-03,  6.68427674e-04],\n",
       "         [-6.04428002e-04, -1.91591005e-03,  1.41927833e-03, ...,\n",
       "          -1.26276701e-03, -1.25141954e-03, -1.43245867e-04],\n",
       "         [-6.04428002e-04, -1.91591005e-03,  1.41927833e-03, ...,\n",
       "          -1.26276701e-03, -1.25141954e-03, -1.43245867e-04],\n",
       "         [ 8.08414770e-05, -1.79379515e-03,  2.86736316e-03, ...,\n",
       "          -1.33114681e-03, -3.50838574e-03,  5.19288529e-04],\n",
       "         [-6.04428002e-04, -1.91591005e-03,  1.41927833e-03, ...,\n",
       "          -1.26276701e-03, -1.25141954e-03, -1.43245867e-04]],\n",
       "\n",
       "        [[ 7.90331978e-06, -9.62709601e-04,  3.72669776e-03, ...,\n",
       "          -2.29760446e-03, -3.94007424e-03, -5.51532139e-07],\n",
       "         [-1.15508074e-03, -9.60705394e-04,  1.67788786e-03, ...,\n",
       "          -1.92012405e-03, -1.61824632e-03, -3.01131629e-04],\n",
       "         [-5.14036627e-04, -1.13437825e-03,  1.76828913e-03, ...,\n",
       "          -2.43820995e-03, -1.56119186e-03, -6.36877201e-04],\n",
       "         [-1.92331732e-04, -1.70183904e-03,  3.70207126e-03, ...,\n",
       "          -1.81128236e-03, -3.66109679e-03,  4.66489844e-04],\n",
       "         [-1.29737612e-03, -8.32193589e-04,  1.89961086e-03, ...,\n",
       "          -2.45116092e-03, -2.02310411e-03, -1.35680049e-04]]]],\n",
       "      dtype=float32), ids=array([[[146, 146, 146, 146, 562],\n",
       "        [146, 146, 641, 146, 562],\n",
       "        [641, 328, 808, 328, 641],\n",
       "        [641, 641, 808, 641, 808],\n",
       "        [641, 641, 808, 808, 641],\n",
       "        [808, 808, 808, 641, 641],\n",
       "        [863, 808, 808, 863, 808],\n",
       "        [863, 166, 863, 166, 166],\n",
       "        [680, 166, 166, 166, 166],\n",
       "        [ 16, 166, 166, 680, 680],\n",
       "        [ 16, 859, 423, 859,  16]],\n",
       "\n",
       "       [[146, 146, 146, 146, 562],\n",
       "        [146, 146, 641, 146, 808],\n",
       "        [641, 328, 808, 808, 328],\n",
       "        [641, 808, 808, 641, 808],\n",
       "        [808, 641, 641, 808, 808],\n",
       "        [808, 808, 641, 808, 641],\n",
       "        [808, 863, 808, 808, 808],\n",
       "        [863, 166, 166, 166, 337],\n",
       "        [166, 166, 166, 166, 166],\n",
       "        [166, 859, 859, 680, 227],\n",
       "        [859, 859, 859, 859, 926]],\n",
       "\n",
       "       [[146, 146, 146, 562, 562],\n",
       "        [146, 146, 146, 808, 146],\n",
       "        [641, 328, 808, 808, 328],\n",
       "        [641, 808, 808, 808, 808],\n",
       "        [808, 808, 808, 641, 808],\n",
       "        [935, 808, 905, 808, 808],\n",
       "        [905, 935, 935, 297, 905],\n",
       "        [905, 905, 297, 905, 297],\n",
       "        [522, 209, 209, 522, 209],\n",
       "        [536, 209, 209, 522, 209],\n",
       "        [536, 227, 536, 209,  98]]], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[146, 146, 146, 146, 562],\n",
       "        [146, 146, 641, 146, 562],\n",
       "        [641, 328, 808, 328, 641],\n",
       "        [641, 641, 808, 641, 808],\n",
       "        [641, 641, 808, 808, 641],\n",
       "        [808, 808, 808, 641, 641],\n",
       "        [863, 808, 808, 863, 808],\n",
       "        [863, 166, 863, 166, 166],\n",
       "        [680, 166, 166, 166, 166],\n",
       "        [ 16, 166, 166, 680, 680],\n",
       "        [ 16, 859, 423, 859,  16]],\n",
       "\n",
       "       [[146, 146, 146, 146, 562],\n",
       "        [146, 146, 641, 146, 808],\n",
       "        [641, 328, 808, 808, 328],\n",
       "        [641, 808, 808, 641, 808],\n",
       "        [808, 641, 641, 808, 808],\n",
       "        [808, 808, 641, 808, 641],\n",
       "        [808, 863, 808, 808, 808],\n",
       "        [863, 166, 166, 166, 337],\n",
       "        [166, 166, 166, 166, 166],\n",
       "        [166, 859, 859, 680, 227],\n",
       "        [859, 859, 859, 859, 926]],\n",
       "\n",
       "       [[146, 146, 146, 562, 562],\n",
       "        [146, 146, 146, 808, 146],\n",
       "        [641, 328, 808, 808, 328],\n",
       "        [641, 808, 808, 808, 808],\n",
       "        [808, 808, 808, 641, 808],\n",
       "        [935, 808, 905, 808, 808],\n",
       "        [905, 935, 935, 297, 905],\n",
       "        [905, 905, 297, 905, 297],\n",
       "        [522, 209, 209, 522, 209],\n",
       "        [536, 209, 209, 522, 209],\n",
       "        [536, 227, 536, 209,  98]]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./configs/config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./configs/config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.911220, perp: 998.882, dev_prep: 998.973, (0.582 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 6.030441, perp: 430.372, dev_prep: 349.892, (0.175 sec/step)\n",
      "step 40, loss = 7.394734, perp: 1508.501, dev_prep: 1610.831, (0.193 sec/step)\n",
      "step 60, loss = 5.606287, perp: 268.046, dev_prep: 215.895, (0.135 sec/step)\n",
      "step 80, loss = 5.190398, perp: 174.399, dev_prep: 199.187, (0.131 sec/step)\n",
      "step 100, loss = 5.039282, perp: 151.549, dev_prep: 181.084, (0.154 sec/step)\n",
      "step 120, loss = 5.072839, perp: 154.516, dev_prep: 150.405, (0.167 sec/step)\n",
      "step 140, loss = 4.875294, perp: 131.143, dev_prep: 163.600, (0.134 sec/step)\n",
      "step 160, loss = 4.971985, perp: 147.643, dev_prep: 134.610, (0.151 sec/step)\n",
      "step 180, loss = 4.888813, perp: 131.376, dev_prep: 120.505, (0.140 sec/step)\n",
      "step 200, loss = 4.880471, perp: 129.916, dev_prep: 127.603, (0.122 sec/step)\n",
      "step 220, loss = 4.646946, perp: 102.317, dev_prep: 123.212, (0.140 sec/step)\n",
      "step 240, loss = 4.872310, perp: 130.002, dev_prep: 127.099, (0.132 sec/step)\n",
      "step 260, loss = 4.652365, perp: 102.190, dev_prep: 128.460, (0.116 sec/step)\n",
      "step 280, loss = 4.566260, perp: 94.780, dev_prep: 132.028, (0.122 sec/step)\n",
      "step 300, loss = 4.757174, perp: 114.447, dev_prep: 127.788, (0.153 sec/step)\n",
      "step 320, loss = 4.665227, perp: 104.693, dev_prep: 115.093, (0.142 sec/step)\n",
      "step 340, loss = 4.804927, perp: 121.491, dev_prep: 113.620, (0.115 sec/step)\n",
      "step 360, loss = 4.851366, perp: 125.504, dev_prep: 119.390, (0.123 sec/step)\n",
      "step 380, loss = 4.927932, perp: 136.628, dev_prep: 121.104, (0.130 sec/step)\n",
      "step 400, loss = 4.635414, perp: 99.866, dev_prep: 95.743, (0.106 sec/step)\n",
      "step 420, loss = 4.604191, perp: 98.576, dev_prep: 95.875, (0.124 sec/step)\n",
      "step 440, loss = 4.412010, perp: 83.303, dev_prep: 109.271, (0.101 sec/step)\n",
      "step 460, loss = 4.557990, perp: 93.544, dev_prep: 97.154, (0.187 sec/step)\n",
      "step 480, loss = 4.523663, perp: 92.660, dev_prep: 93.208, (0.117 sec/step)\n",
      "step 500, loss = 4.484748, perp: 85.396, dev_prep: 100.708, (0.152 sec/step)\n",
      "step 520, loss = 4.314662, perp: 74.097, dev_prep: 77.876, (0.101 sec/step)\n",
      "step 540, loss = 4.412901, perp: 80.046, dev_prep: 77.888, (0.137 sec/step)\n",
      "step 560, loss = 4.549502, perp: 92.401, dev_prep: 70.878, (0.085 sec/step)\n",
      "step 580, loss = 4.452599, perp: 83.222, dev_prep: 63.148, (0.132 sec/step)\n",
      "step 600, loss = 4.117278, perp: 60.410, dev_prep: 78.437, (0.134 sec/step)\n",
      "step 620, loss = 4.069068, perp: 57.905, dev_prep: 65.085, (0.125 sec/step)\n",
      "step 640, loss = 4.084554, perp: 58.726, dev_prep: 65.476, (0.127 sec/step)\n",
      "step 660, loss = 4.173588, perp: 64.480, dev_prep: 73.195, (0.144 sec/step)\n",
      "step 680, loss = 4.296648, perp: 73.722, dev_prep: 65.739, (0.138 sec/step)\n",
      "step 700, loss = 4.092255, perp: 56.579, dev_prep: 60.972, (0.117 sec/step)\n",
      "step 720, loss = 4.138461, perp: 61.940, dev_prep: 61.916, (0.155 sec/step)\n",
      "step 740, loss = 4.059071, perp: 56.672, dev_prep: 54.260, (0.160 sec/step)\n",
      "step 760, loss = 4.246988, perp: 68.109, dev_prep: 55.861, (0.171 sec/step)\n",
      "step 780, loss = 4.032641, perp: 55.614, dev_prep: 64.389, (0.123 sec/step)\n",
      "step 800, loss = 3.995976, perp: 54.414, dev_prep: 56.477, (0.129 sec/step)\n",
      "step 820, loss = 4.074611, perp: 57.898, dev_prep: 60.227, (0.119 sec/step)\n",
      "step 840, loss = 4.144204, perp: 62.172, dev_prep: 59.081, (0.137 sec/step)\n",
      "step 860, loss = 3.985271, perp: 52.888, dev_prep: 53.266, (0.130 sec/step)\n",
      "step 880, loss = 4.119878, perp: 60.146, dev_prep: 59.140, (0.154 sec/step)\n",
      "step 900, loss = 3.995399, perp: 53.969, dev_prep: 52.746, (0.110 sec/step)\n",
      "step 920, loss = 4.035539, perp: 54.920, dev_prep: 57.506, (0.130 sec/step)\n",
      "step 940, loss = 4.040113, perp: 55.161, dev_prep: 53.029, (0.159 sec/step)\n",
      "step 960, loss = 3.972593, perp: 52.057, dev_prep: 54.686, (0.194 sec/step)\n",
      "step 980, loss = 4.044478, perp: 56.079, dev_prep: 59.472, (0.137 sec/step)\n",
      "step 1000, loss = 3.816634, perp: 44.968, dev_prep: 62.940, (0.167 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 3.842452, perp: 45.737, dev_prep: 60.159, (0.161 sec/step)\n",
      "step 1040, loss = 4.088658, perp: 55.891, dev_prep: 51.162, (0.135 sec/step)\n",
      "step 1060, loss = 3.920024, perp: 49.180, dev_prep: 54.103, (0.155 sec/step)\n",
      "step 1080, loss = 3.802136, perp: 44.544, dev_prep: 51.199, (0.114 sec/step)\n",
      "step 1100, loss = 3.865982, perp: 46.863, dev_prep: 46.903, (0.144 sec/step)\n",
      "step 1120, loss = 4.134182, perp: 56.789, dev_prep: 55.821, (0.168 sec/step)\n",
      "step 1140, loss = 3.930018, perp: 48.971, dev_prep: 42.318, (0.105 sec/step)\n",
      "step 1160, loss = 4.044625, perp: 56.310, dev_prep: 47.391, (0.126 sec/step)\n",
      "step 1180, loss = 3.867868, perp: 47.439, dev_prep: 50.649, (0.158 sec/step)\n",
      "step 1200, loss = 3.801051, perp: 43.429, dev_prep: 49.913, (0.165 sec/step)\n",
      "step 1220, loss = 3.973501, perp: 52.907, dev_prep: 49.977, (0.176 sec/step)\n",
      "step 1240, loss = 3.979784, perp: 52.062, dev_prep: 50.097, (0.126 sec/step)\n",
      "step 1260, loss = 3.826638, perp: 45.446, dev_prep: 47.611, (0.141 sec/step)\n",
      "step 1280, loss = 3.770887, perp: 41.299, dev_prep: 45.195, (0.170 sec/step)\n",
      "step 1300, loss = 3.847981, perp: 45.323, dev_prep: 52.186, (0.182 sec/step)\n",
      "step 1320, loss = 3.839217, perp: 44.944, dev_prep: 47.408, (0.146 sec/step)\n",
      "step 1340, loss = 3.810798, perp: 41.948, dev_prep: 41.543, (0.157 sec/step)\n",
      "step 1360, loss = 3.840768, perp: 45.487, dev_prep: 39.908, (0.146 sec/step)\n",
      "step 1380, loss = 3.749158, perp: 41.476, dev_prep: 48.429, (0.158 sec/step)\n",
      "step 1400, loss = 3.678595, perp: 38.192, dev_prep: 42.619, (0.137 sec/step)\n",
      "step 1420, loss = 3.827968, perp: 44.640, dev_prep: 50.021, (0.125 sec/step)\n",
      "step 1440, loss = 3.789293, perp: 44.217, dev_prep: 41.210, (0.149 sec/step)\n",
      "step 1460, loss = 3.656650, perp: 38.281, dev_prep: 46.045, (0.192 sec/step)\n",
      "step 1480, loss = 3.801779, perp: 43.731, dev_prep: 43.774, (0.126 sec/step)\n",
      "step 1500, loss = 3.729999, perp: 40.134, dev_prep: 38.049, (0.141 sec/step)\n",
      "step 1520, loss = 3.752679, perp: 41.170, dev_prep: 41.237, (0.109 sec/step)\n",
      "step 1540, loss = 3.759810, perp: 41.336, dev_prep: 39.164, (0.137 sec/step)\n",
      "step 1560, loss = 3.772189, perp: 42.493, dev_prep: 42.237, (0.124 sec/step)\n",
      "step 1580, loss = 3.676614, perp: 37.919, dev_prep: 43.067, (0.148 sec/step)\n",
      "step 1600, loss = 3.627164, perp: 36.961, dev_prep: 38.086, (0.096 sec/step)\n",
      "step 1620, loss = 3.671968, perp: 38.997, dev_prep: 44.172, (0.145 sec/step)\n",
      "step 1640, loss = 3.780114, perp: 43.411, dev_prep: 42.915, (0.108 sec/step)\n",
      "step 1660, loss = 3.643149, perp: 38.928, dev_prep: 43.299, (0.162 sec/step)\n",
      "step 1680, loss = 3.693767, perp: 39.041, dev_prep: 41.412, (0.142 sec/step)\n",
      "step 1700, loss = 3.617345, perp: 36.047, dev_prep: 39.391, (0.130 sec/step)\n",
      "step 1720, loss = 3.753497, perp: 41.064, dev_prep: 43.401, (0.118 sec/step)\n",
      "step 1740, loss = 3.749074, perp: 42.084, dev_prep: 37.597, (0.153 sec/step)\n",
      "step 1760, loss = 3.586621, perp: 35.481, dev_prep: 37.299, (0.177 sec/step)\n",
      "step 1780, loss = 3.701913, perp: 39.659, dev_prep: 35.781, (0.132 sec/step)\n",
      "step 1800, loss = 3.599241, perp: 36.574, dev_prep: 41.277, (0.140 sec/step)\n",
      "step 1820, loss = 3.606934, perp: 36.640, dev_prep: 37.238, (0.093 sec/step)\n",
      "step 1840, loss = 3.603947, perp: 36.567, dev_prep: 38.820, (0.098 sec/step)\n",
      "step 1860, loss = 3.634274, perp: 36.268, dev_prep: 40.286, (0.173 sec/step)\n",
      "step 1880, loss = 3.722225, perp: 40.802, dev_prep: 39.596, (0.125 sec/step)\n",
      "step 1900, loss = 3.622028, perp: 36.320, dev_prep: 40.758, (0.175 sec/step)\n",
      "step 1920, loss = 3.635798, perp: 37.109, dev_prep: 38.260, (0.143 sec/step)\n",
      "step 1940, loss = 3.504660, perp: 32.665, dev_prep: 41.994, (0.149 sec/step)\n",
      "step 1960, loss = 3.752092, perp: 40.223, dev_prep: 38.222, (0.135 sec/step)\n",
      "step 1980, loss = 3.658767, perp: 36.703, dev_prep: 40.181, (0.093 sec/step)\n",
      "step 2000, loss = 3.716513, perp: 39.947, dev_prep: 38.873, (0.181 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.601027, perp: 36.547, dev_prep: 36.695, (0.182 sec/step)\n",
      "step 2040, loss = 3.657911, perp: 38.665, dev_prep: 40.333, (0.142 sec/step)\n",
      "step 2060, loss = 3.628511, perp: 36.497, dev_prep: 36.482, (0.173 sec/step)\n",
      "step 2080, loss = 3.597451, perp: 35.933, dev_prep: 32.334, (0.138 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.509713, perp: 32.638, dev_prep: 35.122, (0.146 sec/step)\n",
      "step 2120, loss = 3.559252, perp: 33.840, dev_prep: 36.901, (0.186 sec/step)\n",
      "step 2140, loss = 3.502967, perp: 32.659, dev_prep: 34.365, (0.157 sec/step)\n",
      "step 2160, loss = 3.560767, perp: 33.557, dev_prep: 34.531, (0.125 sec/step)\n",
      "step 2180, loss = 3.593625, perp: 33.892, dev_prep: 40.000, (0.139 sec/step)\n",
      "step 2200, loss = 3.713312, perp: 39.356, dev_prep: 37.462, (0.123 sec/step)\n",
      "step 2220, loss = 3.479600, perp: 33.232, dev_prep: 37.255, (0.137 sec/step)\n",
      "step 2240, loss = 3.513561, perp: 33.209, dev_prep: 35.411, (0.158 sec/step)\n",
      "step 2260, loss = 3.587578, perp: 34.386, dev_prep: 35.015, (0.143 sec/step)\n",
      "step 2280, loss = 3.585872, perp: 35.314, dev_prep: 34.958, (0.132 sec/step)\n",
      "step 2300, loss = 3.427714, perp: 30.386, dev_prep: 33.344, (0.125 sec/step)\n",
      "step 2320, loss = 3.562445, perp: 33.443, dev_prep: 28.127, (0.146 sec/step)\n",
      "step 2340, loss = 3.503874, perp: 32.530, dev_prep: 35.938, (0.153 sec/step)\n",
      "step 2360, loss = 3.593940, perp: 34.770, dev_prep: 35.866, (0.121 sec/step)\n",
      "step 2380, loss = 3.361075, perp: 27.816, dev_prep: 33.338, (0.136 sec/step)\n",
      "step 2400, loss = 3.579701, perp: 34.821, dev_prep: 36.739, (0.174 sec/step)\n",
      "step 2420, loss = 3.491555, perp: 32.076, dev_prep: 32.028, (0.114 sec/step)\n",
      "step 2440, loss = 3.401790, perp: 29.677, dev_prep: 36.834, (0.158 sec/step)\n",
      "step 2460, loss = 3.440926, perp: 30.341, dev_prep: 32.332, (0.171 sec/step)\n",
      "step 2480, loss = 3.420037, perp: 30.301, dev_prep: 33.692, (0.148 sec/step)\n",
      "step 2500, loss = 3.460007, perp: 31.666, dev_prep: 36.173, (0.095 sec/step)\n",
      "step 2520, loss = 3.468585, perp: 32.495, dev_prep: 32.187, (0.101 sec/step)\n",
      "step 2540, loss = 3.507695, perp: 32.809, dev_prep: 33.686, (0.138 sec/step)\n",
      "step 2560, loss = 3.447618, perp: 30.716, dev_prep: 34.213, (0.116 sec/step)\n",
      "step 2580, loss = 3.567655, perp: 34.488, dev_prep: 32.606, (0.157 sec/step)\n",
      "step 2600, loss = 3.607708, perp: 36.525, dev_prep: 34.763, (0.121 sec/step)\n",
      "step 2620, loss = 3.457536, perp: 31.712, dev_prep: 32.564, (0.127 sec/step)\n",
      "step 2640, loss = 3.420609, perp: 29.443, dev_prep: 35.328, (0.154 sec/step)\n",
      "step 2660, loss = 3.468621, perp: 30.914, dev_prep: 34.668, (0.168 sec/step)\n",
      "step 2680, loss = 3.443510, perp: 30.048, dev_prep: 38.066, (0.135 sec/step)\n",
      "step 2700, loss = 3.445754, perp: 31.355, dev_prep: 32.427, (0.105 sec/step)\n",
      "step 2720, loss = 3.440892, perp: 30.114, dev_prep: 31.369, (0.129 sec/step)\n",
      "step 2740, loss = 3.438639, perp: 29.958, dev_prep: 29.107, (0.131 sec/step)\n",
      "step 2760, loss = 3.395409, perp: 28.684, dev_prep: 33.164, (0.142 sec/step)\n",
      "step 2780, loss = 3.609630, perp: 35.659, dev_prep: 31.593, (0.157 sec/step)\n",
      "step 2800, loss = 3.455699, perp: 31.779, dev_prep: 31.984, (0.165 sec/step)\n",
      "step 2820, loss = 3.333538, perp: 27.031, dev_prep: 33.431, (0.144 sec/step)\n",
      "step 2840, loss = 3.394110, perp: 29.222, dev_prep: 36.014, (0.113 sec/step)\n",
      "step 2860, loss = 3.543931, perp: 32.968, dev_prep: 31.201, (0.175 sec/step)\n",
      "step 2880, loss = 3.392949, perp: 29.527, dev_prep: 30.916, (0.136 sec/step)\n",
      "step 2900, loss = 3.350240, perp: 28.104, dev_prep: 38.643, (0.125 sec/step)\n",
      "step 2920, loss = 3.443047, perp: 29.887, dev_prep: 28.092, (0.121 sec/step)\n",
      "step 2940, loss = 3.366487, perp: 28.579, dev_prep: 32.540, (0.135 sec/step)\n",
      "step 2960, loss = 3.421077, perp: 30.090, dev_prep: 30.026, (0.159 sec/step)\n",
      "step 2980, loss = 3.488489, perp: 32.077, dev_prep: 30.950, (0.145 sec/step)\n",
      "step 3000, loss = 3.421457, perp: 30.123, dev_prep: 31.000, (0.159 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.491153, perp: 31.412, dev_prep: 30.627, (0.165 sec/step)\n",
      "step 3040, loss = 3.401584, perp: 29.244, dev_prep: 28.450, (0.121 sec/step)\n",
      "step 3060, loss = 3.449645, perp: 31.494, dev_prep: 30.189, (0.155 sec/step)\n",
      "step 3080, loss = 3.370560, perp: 28.323, dev_prep: 29.713, (0.160 sec/step)\n",
      "step 3100, loss = 3.546984, perp: 33.499, dev_prep: 30.130, (0.147 sec/step)\n",
      "step 3120, loss = 3.365923, perp: 27.591, dev_prep: 29.448, (0.143 sec/step)\n",
      "step 3140, loss = 3.396578, perp: 30.744, dev_prep: 31.536, (0.125 sec/step)\n",
      "step 3160, loss = 3.458570, perp: 30.041, dev_prep: 27.047, (0.114 sec/step)\n",
      "step 3180, loss = 3.334549, perp: 27.391, dev_prep: 25.486, (0.135 sec/step)\n",
      "step 3200, loss = 3.333467, perp: 27.356, dev_prep: 26.006, (0.125 sec/step)\n",
      "step 3220, loss = 3.330394, perp: 27.419, dev_prep: 28.878, (0.164 sec/step)\n",
      "step 3240, loss = 3.365018, perp: 28.309, dev_prep: 26.336, (0.111 sec/step)\n",
      "step 3260, loss = 3.460198, perp: 30.366, dev_prep: 29.361, (0.165 sec/step)\n",
      "step 3280, loss = 3.121951, perp: 22.117, dev_prep: 29.056, (0.181 sec/step)\n",
      "step 3300, loss = 3.242089, perp: 24.598, dev_prep: 28.099, (0.168 sec/step)\n",
      "step 3320, loss = 3.344989, perp: 28.286, dev_prep: 29.440, (0.128 sec/step)\n",
      "step 3340, loss = 3.234429, perp: 24.971, dev_prep: 25.427, (0.173 sec/step)\n",
      "step 3360, loss = 3.172345, perp: 23.834, dev_prep: 25.118, (0.134 sec/step)\n",
      "step 3380, loss = 3.288700, perp: 25.957, dev_prep: 26.661, (0.166 sec/step)\n",
      "step 3400, loss = 3.302313, perp: 26.451, dev_prep: 28.359, (0.129 sec/step)\n",
      "step 3420, loss = 3.381888, perp: 28.703, dev_prep: 26.570, (0.152 sec/step)\n",
      "step 3440, loss = 3.290856, perp: 25.478, dev_prep: 27.681, (0.139 sec/step)\n",
      "step 3460, loss = 3.218758, perp: 24.487, dev_prep: 27.221, (0.162 sec/step)\n",
      "step 3480, loss = 3.322795, perp: 27.576, dev_prep: 27.988, (0.181 sec/step)\n",
      "step 3500, loss = 3.415760, perp: 28.747, dev_prep: 25.744, (0.155 sec/step)\n",
      "step 3520, loss = 3.286962, perp: 26.113, dev_prep: 27.661, (0.169 sec/step)\n",
      "step 3540, loss = 3.222535, perp: 24.395, dev_prep: 26.659, (0.127 sec/step)\n",
      "step 3560, loss = 3.197993, perp: 23.502, dev_prep: 26.454, (0.159 sec/step)\n",
      "step 3580, loss = 3.248055, perp: 25.034, dev_prep: 26.013, (0.136 sec/step)\n",
      "step 3600, loss = 3.238491, perp: 24.583, dev_prep: 25.952, (0.162 sec/step)\n",
      "step 3620, loss = 3.166440, perp: 23.990, dev_prep: 24.772, (0.105 sec/step)\n",
      "step 3640, loss = 3.240632, perp: 25.151, dev_prep: 26.358, (0.185 sec/step)\n",
      "step 3660, loss = 3.194824, perp: 23.402, dev_prep: 27.674, (0.152 sec/step)\n",
      "step 3680, loss = 3.233420, perp: 24.330, dev_prep: 26.200, (0.151 sec/step)\n",
      "step 3700, loss = 3.212771, perp: 24.489, dev_prep: 20.398, (0.139 sec/step)\n",
      "step 3720, loss = 3.188141, perp: 23.359, dev_prep: 25.022, (0.164 sec/step)\n",
      "step 3740, loss = 3.133546, perp: 22.577, dev_prep: 22.410, (0.143 sec/step)\n",
      "step 3760, loss = 3.147911, perp: 22.619, dev_prep: 22.800, (0.148 sec/step)\n",
      "step 3780, loss = 3.109178, perp: 21.812, dev_prep: 21.656, (0.107 sec/step)\n",
      "step 3800, loss = 3.184999, perp: 23.661, dev_prep: 22.301, (0.122 sec/step)\n",
      "step 3820, loss = 3.095931, perp: 21.295, dev_prep: 24.522, (0.147 sec/step)\n",
      "step 3840, loss = 3.124583, perp: 22.589, dev_prep: 21.678, (0.151 sec/step)\n",
      "step 3860, loss = 3.120327, perp: 22.185, dev_prep: 22.573, (0.111 sec/step)\n",
      "step 3880, loss = 3.120947, perp: 21.711, dev_prep: 23.789, (0.122 sec/step)\n",
      "step 3900, loss = 3.083766, perp: 21.131, dev_prep: 24.183, (0.102 sec/step)\n",
      "step 3920, loss = 3.144707, perp: 22.735, dev_prep: 23.331, (0.146 sec/step)\n",
      "step 3940, loss = 3.097451, perp: 21.602, dev_prep: 22.432, (0.120 sec/step)\n",
      "step 3960, loss = 3.085033, perp: 21.516, dev_prep: 22.192, (0.124 sec/step)\n",
      "step 3980, loss = 3.072997, perp: 20.281, dev_prep: 22.365, (0.118 sec/step)\n",
      "step 4000, loss = 3.077504, perp: 21.312, dev_prep: 23.642, (0.119 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.144287, perp: 22.643, dev_prep: 24.547, (0.173 sec/step)\n",
      "step 4040, loss = 3.057714, perp: 20.802, dev_prep: 21.014, (0.167 sec/step)\n",
      "step 4060, loss = 3.095847, perp: 21.682, dev_prep: 22.126, (0.136 sec/step)\n",
      "step 4080, loss = 3.082297, perp: 21.033, dev_prep: 22.272, (0.122 sec/step)\n",
      "step 4100, loss = 3.005364, perp: 19.714, dev_prep: 20.737, (0.149 sec/step)\n",
      "step 4120, loss = 3.041221, perp: 20.721, dev_prep: 21.140, (0.132 sec/step)\n",
      "step 4140, loss = 2.967401, perp: 19.057, dev_prep: 20.895, (0.168 sec/step)\n",
      "step 4160, loss = 3.010600, perp: 19.587, dev_prep: 19.522, (0.093 sec/step)\n",
      "step 4180, loss = 2.926546, perp: 18.851, dev_prep: 22.433, (0.125 sec/step)\n",
      "step 4200, loss = 3.038941, perp: 19.899, dev_prep: 18.825, (0.153 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 2.918536, perp: 18.088, dev_prep: 19.843, (0.125 sec/step)\n",
      "step 4240, loss = 3.106674, perp: 21.494, dev_prep: 18.886, (0.130 sec/step)\n",
      "step 4260, loss = 2.884887, perp: 17.316, dev_prep: 19.856, (0.135 sec/step)\n",
      "step 4280, loss = 2.864872, perp: 16.762, dev_prep: 21.145, (0.139 sec/step)\n",
      "step 4300, loss = 3.010873, perp: 19.882, dev_prep: 21.637, (0.203 sec/step)\n",
      "step 4320, loss = 2.875472, perp: 16.908, dev_prep: 21.388, (0.135 sec/step)\n",
      "step 4340, loss = 2.937235, perp: 19.953, dev_prep: 20.320, (0.145 sec/step)\n",
      "step 4360, loss = 3.007597, perp: 19.385, dev_prep: 21.531, (0.160 sec/step)\n",
      "step 4380, loss = 2.931894, perp: 18.292, dev_prep: 21.035, (0.105 sec/step)\n",
      "step 4400, loss = 2.894508, perp: 17.905, dev_prep: 20.857, (0.127 sec/step)\n",
      "step 4420, loss = 3.024348, perp: 19.628, dev_prep: 19.738, (0.121 sec/step)\n",
      "step 4440, loss = 2.846865, perp: 17.204, dev_prep: 20.042, (0.193 sec/step)\n",
      "step 4460, loss = 2.905608, perp: 18.429, dev_prep: 21.652, (0.125 sec/step)\n",
      "step 4480, loss = 2.840092, perp: 17.048, dev_prep: 20.106, (0.118 sec/step)\n",
      "step 4500, loss = 3.051987, perp: 20.482, dev_prep: 19.379, (0.187 sec/step)\n",
      "step 4520, loss = 2.946143, perp: 18.490, dev_prep: 18.736, (0.117 sec/step)\n",
      "step 4540, loss = 2.816885, perp: 16.441, dev_prep: 19.094, (0.133 sec/step)\n",
      "step 4560, loss = 2.929021, perp: 18.158, dev_prep: 17.759, (0.148 sec/step)\n",
      "step 4580, loss = 2.925700, perp: 18.348, dev_prep: 17.201, (0.134 sec/step)\n",
      "step 4600, loss = 2.813250, perp: 16.324, dev_prep: 18.572, (0.142 sec/step)\n",
      "step 4620, loss = 2.948190, perp: 18.506, dev_prep: 18.487, (0.160 sec/step)\n",
      "step 4640, loss = 2.913791, perp: 18.105, dev_prep: 17.601, (0.174 sec/step)\n",
      "step 4660, loss = 2.866808, perp: 17.126, dev_prep: 18.423, (0.131 sec/step)\n",
      "step 4680, loss = 2.953012, perp: 17.986, dev_prep: 17.712, (0.107 sec/step)\n",
      "step 4700, loss = 2.858654, perp: 17.126, dev_prep: 17.059, (0.109 sec/step)\n",
      "step 4720, loss = 2.740766, perp: 15.016, dev_prep: 18.190, (0.146 sec/step)\n",
      "step 4740, loss = 2.775627, perp: 15.563, dev_prep: 17.720, (0.133 sec/step)\n",
      "step 4760, loss = 2.761577, perp: 15.714, dev_prep: 15.936, (0.148 sec/step)\n",
      "step 4780, loss = 2.866814, perp: 17.097, dev_prep: 18.450, (0.149 sec/step)\n",
      "step 4800, loss = 2.822928, perp: 16.356, dev_prep: 18.226, (0.111 sec/step)\n",
      "step 4820, loss = 2.720056, perp: 14.676, dev_prep: 17.396, (0.119 sec/step)\n",
      "step 4840, loss = 2.838248, perp: 15.943, dev_prep: 17.070, (0.148 sec/step)\n",
      "step 4860, loss = 2.771226, perp: 15.319, dev_prep: 15.998, (0.095 sec/step)\n",
      "step 4880, loss = 2.883729, perp: 17.633, dev_prep: 15.580, (0.120 sec/step)\n",
      "step 4900, loss = 2.766542, perp: 15.637, dev_prep: 14.478, (0.149 sec/step)\n",
      "step 4920, loss = 2.776917, perp: 15.746, dev_prep: 16.961, (0.118 sec/step)\n",
      "step 4940, loss = 2.741110, perp: 15.315, dev_prep: 16.203, (0.111 sec/step)\n",
      "step 4960, loss = 2.669779, perp: 14.064, dev_prep: 15.772, (0.115 sec/step)\n",
      "step 4980, loss = 2.824912, perp: 16.494, dev_prep: 15.975, (0.145 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Total loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"Perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.839121280890563"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
