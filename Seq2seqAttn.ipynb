{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        Returns:\n",
    "            h_0: [batch_size, num_units]\n",
    "            context_0: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "        context_0 = self._compute_context(h_0)\n",
    "        h_0 = context_0 * 0\n",
    "\n",
    "        if self._dec_init_states is None:\n",
    "            batch_size = tf.shape(self._memory)[0]\n",
    "            cell_states = self._cell.zero_state(batch_size, self._dtype)\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "\n",
    "        attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, dtype=tf.float32, forget_bias=1.0,\n",
    "                  name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            memory = encoder_outputs\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:24: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=True,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099632, 0.00099486, 0.00099539, ..., 0.000997  ,\n",
       "         0.00099881, 0.0009971 ],\n",
       "        [0.00099658, 0.00099565, 0.00099532, ..., 0.00099893,\n",
       "         0.00099831, 0.00099704],\n",
       "        [0.00099686, 0.00099693, 0.00099567, ..., 0.00100039,\n",
       "         0.00099717, 0.00099753],\n",
       "        [0.00099702, 0.00099816, 0.0009962 , ..., 0.00100136,\n",
       "         0.00099585, 0.00099836]],\n",
       "\n",
       "       [[0.00099861, 0.00099466, 0.00099548, ..., 0.00099815,\n",
       "         0.00099914, 0.00099729],\n",
       "        [0.0009991 , 0.00099479, 0.00099597, ..., 0.00100001,\n",
       "         0.00099916, 0.00099735],\n",
       "        [0.00099934, 0.00099502, 0.00099681, ..., 0.00100002,\n",
       "         0.00099789, 0.00099771],\n",
       "        [0.00099907, 0.00099568, 0.00099708, ..., 0.00099934,\n",
       "         0.0009972 , 0.00099732]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:30: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=False, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[-1.12828030e-03, -1.84812606e-03, -2.01220950e-03, ...,\n",
       "           5.82101289e-04,  2.17203837e-04,  7.69248232e-04],\n",
       "         [-1.12828030e-03, -1.84812606e-03, -2.01220950e-03, ...,\n",
       "           5.82101289e-04,  2.17203837e-04,  7.69248232e-04],\n",
       "         [-1.12828030e-03, -1.84812606e-03, -2.01220950e-03, ...,\n",
       "           5.82101289e-04,  2.17203837e-04,  7.69248232e-04],\n",
       "         [-1.12828030e-03, -1.84812606e-03, -2.01220950e-03, ...,\n",
       "           5.82101289e-04,  2.17203837e-04,  7.69248232e-04],\n",
       "         [-1.12828030e-03, -1.84812606e-03, -2.01220950e-03, ...,\n",
       "           5.82101289e-04,  2.17203837e-04,  7.69248232e-04]],\n",
       "\n",
       "        [[-1.07706338e-03, -2.68910476e-03, -3.22821271e-03, ...,\n",
       "           9.21289087e-04, -5.24016286e-06,  1.15283486e-03],\n",
       "         [-9.97722847e-04, -2.52680527e-03, -2.59632990e-03, ...,\n",
       "           1.12287025e-03,  1.03475315e-04,  1.08213164e-03],\n",
       "         [-9.55107447e-04, -2.51402543e-03, -2.74363626e-03, ...,\n",
       "           6.53337338e-04, -2.71077908e-04,  1.82753545e-03],\n",
       "         [-9.55107447e-04, -2.51402543e-03, -2.74363626e-03, ...,\n",
       "           6.53337338e-04, -2.71077908e-04,  1.82753545e-03],\n",
       "         [-7.12829235e-04, -2.70824041e-03, -2.41351360e-03, ...,\n",
       "           9.83329257e-04,  6.82058744e-04,  1.22408418e-03]],\n",
       "\n",
       "        [[-2.37144166e-04, -2.43717851e-03, -2.80788029e-03, ...,\n",
       "           4.10863082e-04, -1.00281904e-03,  2.89167138e-03],\n",
       "         [-6.39212318e-04, -2.82516144e-03, -3.50500410e-03, ...,\n",
       "           7.94456806e-04, -6.17518439e-04,  1.89312850e-03],\n",
       "         [-5.60163113e-04, -3.17718578e-03, -3.55340820e-03, ...,\n",
       "           6.19157800e-04, -5.86158421e-06,  1.08276145e-03],\n",
       "         [-2.37144166e-04, -2.43717851e-03, -2.80788029e-03, ...,\n",
       "           4.10863082e-04, -1.00281904e-03,  2.89167138e-03],\n",
       "         [-6.39212318e-04, -2.82516144e-03, -3.50500410e-03, ...,\n",
       "           7.94456806e-04, -6.17518439e-04,  1.89312850e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 6.68528862e-03,  3.72024428e-04, -1.45143626e-04, ...,\n",
       "           5.37521264e-04, -1.56965409e-03,  1.93133287e-03],\n",
       "         [ 6.68528862e-03,  3.72024428e-04, -1.45143626e-04, ...,\n",
       "           5.37521264e-04, -1.56965409e-03,  1.93133287e-03],\n",
       "         [ 5.29022329e-03,  1.14365714e-03, -5.09782229e-04, ...,\n",
       "          -8.30781646e-05, -2.68871849e-03,  3.42748826e-03],\n",
       "         [ 6.68528862e-03,  3.72024428e-04, -1.45143626e-04, ...,\n",
       "           5.37521264e-04, -1.56965409e-03,  1.93133287e-03],\n",
       "         [ 6.68528862e-03,  3.72024428e-04, -1.45143626e-04, ...,\n",
       "           5.37521264e-04, -1.56965409e-03,  1.93133287e-03]],\n",
       "\n",
       "        [[ 7.41544273e-03,  1.01509737e-03,  1.87183337e-04, ...,\n",
       "           5.46580472e-04, -2.19196314e-03,  1.84327667e-03],\n",
       "         [ 7.28032552e-03,  9.11629410e-04,  6.69942936e-04, ...,\n",
       "           3.44910775e-04, -1.17256539e-03,  8.90092459e-04],\n",
       "         [ 7.68156070e-03,  5.19652036e-04,  2.98666739e-04, ...,\n",
       "           8.10478232e-04, -1.27043598e-03,  1.45367160e-03],\n",
       "         [ 7.41544273e-03,  1.01509737e-03,  1.87183337e-04, ...,\n",
       "           5.46580472e-04, -2.19196314e-03,  1.84327667e-03],\n",
       "         [ 7.28032552e-03,  9.11629410e-04,  6.69942936e-04, ...,\n",
       "           3.44910775e-04, -1.17256539e-03,  8.90092459e-04]],\n",
       "\n",
       "        [[ 6.90877810e-03,  1.15332380e-03,  1.60842505e-03, ...,\n",
       "           6.65656175e-04, -1.42103736e-03,  6.44210551e-04],\n",
       "         [ 7.08239246e-03,  1.40896975e-03,  1.56686478e-03, ...,\n",
       "          -1.34501839e-04, -5.66272822e-04, -3.10898409e-04],\n",
       "         [ 8.75491835e-03,  8.10935046e-04,  1.25851319e-03, ...,\n",
       "           8.39082524e-04, -9.98796429e-04, -1.82163974e-04],\n",
       "         [ 7.61769246e-03,  1.35104614e-03,  1.53171318e-03, ...,\n",
       "           4.76259855e-04, -1.29982713e-03,  6.14531105e-04],\n",
       "         [ 6.87759230e-03,  1.47342100e-03,  6.51646696e-04, ...,\n",
       "           5.77724655e-04, -1.31360581e-03,  1.98451336e-04]]],\n",
       "\n",
       "\n",
       "       [[[-1.12061086e-03, -1.85355847e-03, -2.04595295e-03, ...,\n",
       "           5.97869395e-04,  2.28544217e-04,  7.59450078e-04],\n",
       "         [-1.12061086e-03, -1.85355847e-03, -2.04595295e-03, ...,\n",
       "           5.97869395e-04,  2.28544217e-04,  7.59450078e-04],\n",
       "         [-1.12061086e-03, -1.85355847e-03, -2.04595295e-03, ...,\n",
       "           5.97869395e-04,  2.28544217e-04,  7.59450078e-04],\n",
       "         [-1.12061086e-03, -1.85355847e-03, -2.04595295e-03, ...,\n",
       "           5.97869395e-04,  2.28544217e-04,  7.59450078e-04],\n",
       "         [-1.12061086e-03, -1.85355847e-03, -2.04595295e-03, ...,\n",
       "           5.97869395e-04,  2.28544217e-04,  7.59450078e-04]],\n",
       "\n",
       "        [[-1.07164460e-03, -2.69792671e-03, -3.31810652e-03, ...,\n",
       "           9.57946468e-04,  2.54658953e-05,  1.12802372e-03],\n",
       "         [-9.90476459e-04, -2.53590033e-03, -2.68547470e-03, ...,\n",
       "           1.16030069e-03,  1.33999361e-04,  1.05566252e-03],\n",
       "         [-8.67647817e-04, -2.87502888e-03, -2.87837628e-03, ...,\n",
       "           5.13328065e-04,  3.64767940e-04,  9.90819419e-04],\n",
       "         [-9.48598201e-04, -2.52386043e-03, -2.83342623e-03, ...,\n",
       "           6.89989072e-04, -2.40759560e-04,  1.80193700e-03],\n",
       "         [-9.48598201e-04, -2.52386043e-03, -2.83342623e-03, ...,\n",
       "           6.89989072e-04, -2.40759560e-04,  1.80193700e-03]],\n",
       "\n",
       "        [[-2.41547386e-04, -2.44952412e-03, -2.96434877e-03, ...,\n",
       "           4.70164348e-04, -9.50918416e-04,  2.85247457e-03],\n",
       "         [-2.83739973e-06, -2.63967155e-03, -2.63657584e-03, ...,\n",
       "           7.91908009e-04, -6.79661753e-07,  2.24762037e-03],\n",
       "         [-6.45412714e-04, -2.83637666e-03, -3.66192916e-03, ...,\n",
       "           8.53579608e-04, -5.64304937e-04,  1.85476290e-03],\n",
       "         [-2.41547386e-04, -2.44952412e-03, -2.96434877e-03, ...,\n",
       "           4.70164348e-04, -9.50918416e-04,  2.85247457e-03],\n",
       "         [-2.83739973e-06, -2.63967155e-03, -2.63657584e-03, ...,\n",
       "           7.91908009e-04, -6.79661753e-07,  2.24762037e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 6.51215995e-03,  3.71277565e-04, -6.80036203e-04, ...,\n",
       "           7.20924698e-04, -1.43441767e-03,  1.90171634e-03],\n",
       "         [ 6.51215995e-03,  3.71277565e-04, -6.80036203e-04, ...,\n",
       "           7.20924698e-04, -1.43441767e-03,  1.90171634e-03],\n",
       "         [ 6.51215995e-03,  3.71277565e-04, -6.80036203e-04, ...,\n",
       "           7.20924698e-04, -1.43441767e-03,  1.90171634e-03],\n",
       "         [ 6.51215995e-03,  3.71277565e-04, -6.80036203e-04, ...,\n",
       "           7.20924698e-04, -1.43441767e-03,  1.90171634e-03],\n",
       "         [ 6.51215995e-03,  3.71277565e-04, -6.80036203e-04, ...,\n",
       "           7.20924698e-04, -1.43441767e-03,  1.90171634e-03]],\n",
       "\n",
       "        [[ 7.21966475e-03,  1.01412239e-03, -3.93672613e-04, ...,\n",
       "           7.52345135e-04, -2.05803150e-03,  1.82127196e-03],\n",
       "         [ 7.08110770e-03,  9.11735056e-04,  9.20870079e-05, ...,\n",
       "           5.50680561e-04, -1.03775924e-03,  8.71370896e-04],\n",
       "         [ 7.21966475e-03,  1.01412239e-03, -3.93672613e-04, ...,\n",
       "           7.52345135e-04, -2.05803150e-03,  1.82127196e-03],\n",
       "         [ 7.61837559e-03,  8.52908648e-04,  6.09142444e-05, ...,\n",
       "           1.15683628e-03, -1.76448398e-03,  1.79698016e-03],\n",
       "         [ 7.08110770e-03,  9.11735056e-04,  9.20870079e-05, ...,\n",
       "           5.50680561e-04, -1.03775924e-03,  8.71370896e-04]],\n",
       "\n",
       "        [[ 6.68831263e-03,  1.15390541e-03,  9.88083426e-04, ...,\n",
       "           8.89801770e-04, -1.28984475e-03,  6.36724872e-04],\n",
       "         [ 6.86133746e-03,  1.40993355e-03,  9.49127483e-04, ...,\n",
       "           9.24679916e-05, -4.36347793e-04, -3.16868449e-04],\n",
       "         [ 6.65893778e-03,  1.47519377e-03,  3.10775940e-05, ...,\n",
       "           8.00148817e-04, -1.18212181e-03,  1.91136467e-04],\n",
       "         [ 7.39858951e-03,  1.35066011e-03,  9.13170690e-04, ...,\n",
       "           7.00147240e-04, -1.16938795e-03,  6.06949616e-04],\n",
       "         [ 6.99977763e-03,  1.51900621e-03,  4.69529186e-04, ...,\n",
       "           3.00957356e-04, -1.46477623e-03,  6.36660494e-04]]],\n",
       "\n",
       "\n",
       "       [[[-1.11213746e-03, -1.84421311e-03, -2.00418197e-03, ...,\n",
       "           5.46846655e-04,  2.10029510e-04,  7.83232856e-04],\n",
       "         [-1.11213746e-03, -1.84421311e-03, -2.00418197e-03, ...,\n",
       "           5.46846655e-04,  2.10029510e-04,  7.83232856e-04],\n",
       "         [-1.11213746e-03, -1.84421311e-03, -2.00418197e-03, ...,\n",
       "           5.46846655e-04,  2.10029510e-04,  7.83232856e-04],\n",
       "         [-1.11213746e-03, -1.84421311e-03, -2.00418197e-03, ...,\n",
       "           5.46846655e-04,  2.10029510e-04,  7.83232856e-04],\n",
       "         [-1.11213746e-03, -1.84421311e-03, -2.00418197e-03, ...,\n",
       "           5.46846655e-04,  2.10029510e-04,  7.83232856e-04]],\n",
       "\n",
       "        [[-9.75965231e-04, -2.51464942e-03, -2.58297892e-03, ...,\n",
       "           1.03285850e-03,  1.02692291e-04,  1.11908477e-03],\n",
       "         [-8.52106547e-04, -2.85357120e-03, -2.77533289e-03, ...,\n",
       "           3.86220810e-04,  3.34118406e-04,  1.05449138e-03],\n",
       "         [-1.05600152e-03, -2.67577334e-03, -3.21414997e-03, ...,\n",
       "           8.31028330e-04, -4.82612086e-06,  1.19066611e-03],\n",
       "         [-6.90900255e-04, -2.69548432e-03, -2.40008812e-03, ...,\n",
       "           8.93413380e-04,  6.83594262e-04,  1.26140774e-03],\n",
       "         [-6.90900255e-04, -2.69548432e-03, -2.40008812e-03, ...,\n",
       "           8.93413380e-04,  6.83594262e-04,  1.26140774e-03]],\n",
       "\n",
       "        [[ 1.31137094e-05, -2.60541402e-03, -2.46664160e-03, ...,\n",
       "           5.79543295e-04, -3.33410899e-05,  2.35980749e-03],\n",
       "         [-2.25880431e-04, -2.41600978e-03, -2.79344455e-03, ...,\n",
       "           2.56720261e-04, -9.84521816e-04,  2.96528032e-03],\n",
       "         [ 1.31137094e-05, -2.60541402e-03, -2.46664160e-03, ...,\n",
       "           5.79543295e-04, -3.33410899e-05,  2.35980749e-03],\n",
       "         [-6.30552066e-04, -2.80114217e-03, -3.49131599e-03, ...,\n",
       "           6.39797654e-04, -5.97229693e-04,  1.96624291e-03],\n",
       "         [ 1.31137094e-05, -2.60541402e-03, -2.46664160e-03, ...,\n",
       "           5.79543295e-04, -3.33410899e-05,  2.35980749e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.92954326e-04, -2.46762624e-03, -2.92681344e-03, ...,\n",
       "          -3.51478113e-03, -2.67168321e-03,  1.99171552e-03],\n",
       "         [ 2.92954326e-04, -2.46762624e-03, -2.92681344e-03, ...,\n",
       "          -3.51478113e-03, -2.67168321e-03,  1.99171552e-03],\n",
       "         [ 1.53060956e-03, -2.21304828e-03, -2.42149481e-03, ...,\n",
       "          -2.97243940e-03, -2.50711036e-03,  1.87666551e-03],\n",
       "         [ 1.53060956e-03, -2.21304828e-03, -2.42149481e-03, ...,\n",
       "          -2.97243940e-03, -2.50711036e-03,  1.87666551e-03],\n",
       "         [ 2.13487446e-03, -2.18256563e-03, -2.12739361e-03, ...,\n",
       "          -2.65180971e-03, -2.28679599e-03,  1.66267529e-03]],\n",
       "\n",
       "        [[-1.67891500e-04, -2.59420834e-03, -3.35906772e-03, ...,\n",
       "          -4.26234445e-03, -3.42920423e-03,  1.92740583e-03],\n",
       "         [-1.67891500e-04, -2.59420834e-03, -3.35906772e-03, ...,\n",
       "          -4.26234445e-03, -3.42920423e-03,  1.92740583e-03],\n",
       "         [ 1.06880092e-03, -2.34209676e-03, -2.85402988e-03, ...,\n",
       "          -3.71772610e-03, -3.26218735e-03,  1.81009853e-03],\n",
       "         [ 1.06880092e-03, -2.34209676e-03, -2.85402988e-03, ...,\n",
       "          -3.71772610e-03, -3.26218735e-03,  1.81009853e-03],\n",
       "         [-1.67891500e-04, -2.59420834e-03, -3.35906772e-03, ...,\n",
       "          -4.26234445e-03, -3.42920423e-03,  1.92740583e-03]],\n",
       "\n",
       "        [[-6.18480030e-04, -2.71916343e-03, -3.81542020e-03, ...,\n",
       "          -4.96907206e-03, -4.17183014e-03,  1.93047395e-03],\n",
       "         [ 6.17217913e-04, -2.46900832e-03, -3.31084663e-03, ...,\n",
       "          -4.42255847e-03, -4.00273921e-03,  1.81125512e-03],\n",
       "         [ 1.21726270e-03, -2.44083069e-03, -3.01678525e-03, ...,\n",
       "          -4.10052342e-03, -3.78006836e-03,  1.59726315e-03],\n",
       "         [ 2.45957728e-03, -2.18409020e-03, -2.51166895e-03, ...,\n",
       "          -3.56182642e-03, -3.60947847e-03,  1.47832418e-03],\n",
       "         [ 1.42540620e-03, -2.51757074e-03, -2.85776379e-03, ...,\n",
       "          -3.90488235e-03, -3.59163713e-03,  1.39753858e-03]]]],\n",
       "      dtype=float32), ids=array([[[901, 712, 901, 712, 712],\n",
       "        [313, 415, 901, 901, 901],\n",
       "        [901, 313, 313, 901, 313],\n",
       "        [901, 901, 901, 901, 901],\n",
       "        [441, 901, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [441, 441, 441, 441,  69],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [641, 641, 441, 641, 641],\n",
       "        [641, 354, 641, 641, 354],\n",
       "        [354, 354, 354, 641, 641]],\n",
       "\n",
       "       [[901, 901, 712, 712, 712],\n",
       "        [313, 415, 901, 901, 901],\n",
       "        [901, 313, 313, 901, 313],\n",
       "        [901, 901, 901, 901, 901],\n",
       "        [441, 901, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [441, 441, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [641, 641, 641, 641, 641],\n",
       "        [641, 354, 641, 865, 354],\n",
       "        [354, 354, 641, 354, 641]],\n",
       "\n",
       "       [[901, 901, 712, 712, 712],\n",
       "        [415, 313, 901, 901, 901],\n",
       "        [313, 901, 313, 400, 313],\n",
       "        [901, 223, 415, 313, 313],\n",
       "        [223, 441, 223, 441, 441],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 441, 223, 441,  49]]], dtype=int32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[901, 712, 901, 712, 712],\n",
       "        [313, 415, 901, 901, 901],\n",
       "        [901, 313, 313, 901, 313],\n",
       "        [901, 901, 901, 901, 901],\n",
       "        [441, 901, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [441, 441, 441, 441,  69],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [641, 641, 441, 641, 641],\n",
       "        [641, 354, 641, 641, 354],\n",
       "        [354, 354, 354, 641, 641]],\n",
       "\n",
       "       [[901, 901, 712, 712, 712],\n",
       "        [313, 415, 901, 901, 901],\n",
       "        [901, 313, 313, 901, 313],\n",
       "        [901, 901, 901, 901, 901],\n",
       "        [441, 901, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [441, 441, 441,  69, 441],\n",
       "        [441, 441, 441, 441, 441],\n",
       "        [641, 641, 641, 641, 641],\n",
       "        [641, 354, 641, 865, 354],\n",
       "        [354, 354, 641, 354, 641]],\n",
       "\n",
       "       [[901, 901, 712, 712, 712],\n",
       "        [415, 313, 901, 901, 901],\n",
       "        [313, 901, 313, 400, 313],\n",
       "        [901, 223, 415, 313, 313],\n",
       "        [223, 441, 223, 441, 441],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 223, 441, 441, 223],\n",
       "        [223, 441, 223, 441,  49]]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": None,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.909426, perp: 997.326, dev_prep: 996.763, (0.603 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 5.753050, perp: 320.257, dev_prep: 355.461, (0.151 sec/step)\n",
      "step 40, loss = 5.855294, perp: 348.591, dev_prep: 332.928, (0.145 sec/step)\n",
      "step 60, loss = 5.275953, perp: 192.666, dev_prep: 206.653, (0.118 sec/step)\n",
      "step 80, loss = 5.220847, perp: 182.615, dev_prep: 184.107, (0.183 sec/step)\n",
      "step 100, loss = 5.019914, perp: 150.883, dev_prep: 136.240, (0.178 sec/step)\n",
      "step 120, loss = 4.966419, perp: 140.727, dev_prep: 150.528, (0.147 sec/step)\n",
      "step 140, loss = 4.952325, perp: 141.625, dev_prep: 157.964, (0.130 sec/step)\n",
      "step 160, loss = 4.931888, perp: 141.344, dev_prep: 145.375, (0.149 sec/step)\n",
      "step 180, loss = 4.631906, perp: 101.073, dev_prep: 118.769, (0.148 sec/step)\n",
      "step 200, loss = 4.812815, perp: 120.843, dev_prep: 108.684, (0.155 sec/step)\n",
      "step 220, loss = 4.671301, perp: 104.828, dev_prep: 132.946, (0.162 sec/step)\n",
      "step 240, loss = 4.723140, perp: 111.478, dev_prep: 118.329, (0.126 sec/step)\n",
      "step 260, loss = 4.939680, perp: 137.594, dev_prep: 121.193, (0.141 sec/step)\n",
      "step 280, loss = 4.640147, perp: 102.447, dev_prep: 99.615, (0.125 sec/step)\n",
      "step 300, loss = 4.845401, perp: 127.520, dev_prep: 109.676, (0.130 sec/step)\n",
      "step 320, loss = 4.847654, perp: 126.167, dev_prep: 128.688, (0.173 sec/step)\n",
      "step 340, loss = 4.786313, perp: 118.927, dev_prep: 121.752, (0.134 sec/step)\n",
      "step 360, loss = 4.703166, perp: 106.294, dev_prep: 135.919, (0.122 sec/step)\n",
      "step 380, loss = 4.824870, perp: 122.643, dev_prep: 101.769, (0.178 sec/step)\n",
      "step 400, loss = 4.803016, perp: 122.105, dev_prep: 126.734, (0.097 sec/step)\n",
      "step 420, loss = 4.871624, perp: 128.959, dev_prep: 107.856, (0.146 sec/step)\n",
      "step 440, loss = 4.645258, perp: 103.995, dev_prep: 103.770, (0.137 sec/step)\n",
      "step 460, loss = 4.628644, perp: 101.735, dev_prep: 109.865, (0.133 sec/step)\n",
      "step 480, loss = 4.602932, perp: 101.463, dev_prep: 88.439, (0.117 sec/step)\n",
      "step 500, loss = 4.482130, perp: 85.599, dev_prep: 107.054, (0.143 sec/step)\n",
      "step 520, loss = 4.686809, perp: 107.264, dev_prep: 91.266, (0.141 sec/step)\n",
      "step 540, loss = 4.428390, perp: 82.159, dev_prep: 84.458, (0.149 sec/step)\n",
      "step 560, loss = 4.336597, perp: 75.546, dev_prep: 81.448, (0.105 sec/step)\n",
      "step 580, loss = 4.341864, perp: 74.168, dev_prep: 81.425, (0.128 sec/step)\n",
      "step 600, loss = 4.286708, perp: 72.349, dev_prep: 77.434, (0.149 sec/step)\n",
      "step 620, loss = 4.327684, perp: 74.772, dev_prep: 67.610, (0.140 sec/step)\n",
      "step 640, loss = 4.156582, perp: 62.695, dev_prep: 68.017, (0.137 sec/step)\n",
      "step 660, loss = 4.120227, perp: 59.622, dev_prep: 72.296, (0.076 sec/step)\n",
      "step 680, loss = 4.184794, perp: 64.672, dev_prep: 68.902, (0.190 sec/step)\n",
      "step 700, loss = 4.160711, perp: 63.289, dev_prep: 63.444, (0.147 sec/step)\n",
      "step 720, loss = 4.121975, perp: 60.623, dev_prep: 66.827, (0.116 sec/step)\n",
      "step 740, loss = 4.114523, perp: 59.836, dev_prep: 60.276, (0.107 sec/step)\n",
      "step 760, loss = 4.079319, perp: 58.133, dev_prep: 58.138, (0.126 sec/step)\n",
      "step 780, loss = 3.992199, perp: 52.887, dev_prep: 65.243, (0.147 sec/step)\n",
      "step 800, loss = 3.951119, perp: 52.637, dev_prep: 67.662, (0.164 sec/step)\n",
      "step 820, loss = 4.148532, perp: 61.446, dev_prep: 64.631, (0.177 sec/step)\n",
      "step 840, loss = 3.937930, perp: 50.906, dev_prep: 59.525, (0.112 sec/step)\n",
      "step 860, loss = 4.136413, perp: 61.584, dev_prep: 53.083, (0.116 sec/step)\n",
      "step 880, loss = 4.007586, perp: 54.179, dev_prep: 57.645, (0.196 sec/step)\n",
      "step 900, loss = 4.038232, perp: 55.988, dev_prep: 57.846, (0.120 sec/step)\n",
      "step 920, loss = 4.067756, perp: 57.549, dev_prep: 51.744, (0.139 sec/step)\n",
      "step 940, loss = 4.024235, perp: 54.351, dev_prep: 49.621, (0.162 sec/step)\n",
      "step 960, loss = 4.032843, perp: 55.201, dev_prep: 56.716, (0.132 sec/step)\n",
      "step 980, loss = 3.864497, perp: 46.463, dev_prep: 54.353, (0.142 sec/step)\n",
      "step 1000, loss = 3.881130, perp: 48.100, dev_prep: 58.157, (0.096 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 3.902480, perp: 48.162, dev_prep: 58.038, (0.114 sec/step)\n",
      "step 1040, loss = 4.024542, perp: 54.248, dev_prep: 46.084, (0.121 sec/step)\n",
      "step 1060, loss = 3.834566, perp: 45.216, dev_prep: 53.770, (0.132 sec/step)\n",
      "step 1080, loss = 3.730267, perp: 41.497, dev_prep: 45.517, (0.153 sec/step)\n",
      "step 1100, loss = 3.979567, perp: 51.737, dev_prep: 57.507, (0.140 sec/step)\n",
      "step 1120, loss = 3.831410, perp: 45.663, dev_prep: 56.868, (0.110 sec/step)\n",
      "step 1140, loss = 3.883676, perp: 46.144, dev_prep: 48.056, (0.147 sec/step)\n",
      "step 1160, loss = 3.870739, perp: 46.825, dev_prep: 53.281, (0.109 sec/step)\n",
      "step 1180, loss = 4.018996, perp: 55.145, dev_prep: 51.148, (0.132 sec/step)\n",
      "step 1200, loss = 3.797776, perp: 43.916, dev_prep: 52.834, (0.105 sec/step)\n",
      "step 1220, loss = 3.761926, perp: 41.977, dev_prep: 44.852, (0.109 sec/step)\n",
      "step 1240, loss = 4.000707, perp: 54.351, dev_prep: 47.245, (0.157 sec/step)\n",
      "step 1260, loss = 3.846256, perp: 45.509, dev_prep: 42.951, (0.110 sec/step)\n",
      "step 1280, loss = 3.937608, perp: 50.182, dev_prep: 47.916, (0.157 sec/step)\n",
      "step 1300, loss = 3.859597, perp: 45.657, dev_prep: 46.926, (0.090 sec/step)\n",
      "step 1320, loss = 3.863493, perp: 46.950, dev_prep: 48.737, (0.116 sec/step)\n",
      "step 1340, loss = 3.860500, perp: 45.584, dev_prep: 45.601, (0.133 sec/step)\n",
      "step 1360, loss = 3.950535, perp: 50.626, dev_prep: 53.063, (0.127 sec/step)\n",
      "step 1380, loss = 3.847177, perp: 45.461, dev_prep: 43.897, (0.106 sec/step)\n",
      "step 1400, loss = 3.743651, perp: 41.419, dev_prep: 42.368, (0.159 sec/step)\n",
      "step 1420, loss = 3.843149, perp: 46.419, dev_prep: 43.833, (0.131 sec/step)\n",
      "step 1440, loss = 3.831937, perp: 46.053, dev_prep: 46.232, (0.164 sec/step)\n",
      "step 1460, loss = 3.845887, perp: 46.360, dev_prep: 47.021, (0.123 sec/step)\n",
      "step 1480, loss = 3.687898, perp: 38.868, dev_prep: 46.414, (0.113 sec/step)\n",
      "step 1500, loss = 3.684486, perp: 38.886, dev_prep: 44.616, (0.191 sec/step)\n",
      "step 1520, loss = 3.642003, perp: 37.469, dev_prep: 40.079, (0.171 sec/step)\n",
      "step 1540, loss = 3.602317, perp: 35.953, dev_prep: 41.441, (0.178 sec/step)\n",
      "step 1560, loss = 3.731456, perp: 40.842, dev_prep: 40.982, (0.133 sec/step)\n",
      "step 1580, loss = 3.687263, perp: 39.110, dev_prep: 43.734, (0.186 sec/step)\n",
      "step 1600, loss = 3.726232, perp: 40.998, dev_prep: 43.851, (0.178 sec/step)\n",
      "step 1620, loss = 3.692318, perp: 40.612, dev_prep: 40.887, (0.128 sec/step)\n",
      "step 1640, loss = 3.689309, perp: 39.490, dev_prep: 47.242, (0.146 sec/step)\n",
      "step 1660, loss = 3.553452, perp: 33.805, dev_prep: 36.574, (0.162 sec/step)\n",
      "step 1680, loss = 3.637197, perp: 36.211, dev_prep: 38.793, (0.148 sec/step)\n",
      "step 1700, loss = 3.662314, perp: 37.214, dev_prep: 42.844, (0.116 sec/step)\n",
      "step 1720, loss = 3.794395, perp: 44.617, dev_prep: 35.877, (0.125 sec/step)\n",
      "step 1740, loss = 3.675517, perp: 38.219, dev_prep: 37.967, (0.134 sec/step)\n",
      "step 1760, loss = 3.619807, perp: 36.357, dev_prep: 37.386, (0.109 sec/step)\n",
      "step 1780, loss = 3.653878, perp: 38.193, dev_prep: 35.557, (0.152 sec/step)\n",
      "step 1800, loss = 3.564341, perp: 34.659, dev_prep: 34.269, (0.161 sec/step)\n",
      "step 1820, loss = 3.714775, perp: 39.880, dev_prep: 37.989, (0.098 sec/step)\n",
      "step 1840, loss = 3.641529, perp: 37.398, dev_prep: 37.988, (0.120 sec/step)\n",
      "step 1860, loss = 3.620745, perp: 35.901, dev_prep: 31.043, (0.142 sec/step)\n",
      "step 1880, loss = 3.516648, perp: 33.172, dev_prep: 35.519, (0.150 sec/step)\n",
      "step 1900, loss = 3.676541, perp: 38.513, dev_prep: 43.436, (0.168 sec/step)\n",
      "step 1920, loss = 3.682406, perp: 38.748, dev_prep: 41.402, (0.135 sec/step)\n",
      "step 1940, loss = 3.723388, perp: 39.997, dev_prep: 36.958, (0.095 sec/step)\n",
      "step 1960, loss = 3.585901, perp: 35.220, dev_prep: 38.882, (0.144 sec/step)\n",
      "step 1980, loss = 3.689306, perp: 38.564, dev_prep: 33.308, (0.153 sec/step)\n",
      "step 2000, loss = 3.609046, perp: 35.547, dev_prep: 40.603, (0.137 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.644181, perp: 37.776, dev_prep: 36.146, (0.134 sec/step)\n",
      "step 2040, loss = 3.699219, perp: 39.586, dev_prep: 36.118, (0.173 sec/step)\n",
      "step 2060, loss = 3.625399, perp: 37.201, dev_prep: 36.493, (0.120 sec/step)\n",
      "step 2080, loss = 3.595250, perp: 35.654, dev_prep: 35.378, (0.194 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.560927, perp: 35.083, dev_prep: 32.547, (0.113 sec/step)\n",
      "step 2120, loss = 3.540424, perp: 34.554, dev_prep: 40.462, (0.111 sec/step)\n",
      "step 2140, loss = 3.551397, perp: 35.238, dev_prep: 37.045, (0.139 sec/step)\n",
      "step 2160, loss = 3.536463, perp: 33.434, dev_prep: 35.786, (0.135 sec/step)\n",
      "step 2180, loss = 3.572628, perp: 35.105, dev_prep: 34.553, (0.149 sec/step)\n",
      "step 2200, loss = 3.467165, perp: 31.506, dev_prep: 36.089, (0.143 sec/step)\n",
      "step 2220, loss = 3.497056, perp: 32.358, dev_prep: 32.563, (0.143 sec/step)\n",
      "step 2240, loss = 3.644671, perp: 38.081, dev_prep: 36.827, (0.137 sec/step)\n",
      "step 2260, loss = 3.538977, perp: 33.806, dev_prep: 37.509, (0.138 sec/step)\n",
      "step 2280, loss = 3.626806, perp: 37.428, dev_prep: 36.332, (0.130 sec/step)\n",
      "step 2300, loss = 3.487251, perp: 31.681, dev_prep: 36.952, (0.137 sec/step)\n",
      "step 2320, loss = 3.545013, perp: 34.165, dev_prep: 34.724, (0.178 sec/step)\n",
      "step 2340, loss = 3.524214, perp: 33.762, dev_prep: 33.336, (0.128 sec/step)\n",
      "step 2360, loss = 3.446104, perp: 30.484, dev_prep: 35.607, (0.151 sec/step)\n",
      "step 2380, loss = 3.578658, perp: 34.931, dev_prep: 30.158, (0.147 sec/step)\n",
      "step 2400, loss = 3.635007, perp: 38.446, dev_prep: 35.910, (0.150 sec/step)\n",
      "step 2420, loss = 3.537422, perp: 33.829, dev_prep: 39.731, (0.169 sec/step)\n",
      "step 2440, loss = 3.547457, perp: 34.075, dev_prep: 32.260, (0.118 sec/step)\n",
      "step 2460, loss = 3.544734, perp: 33.574, dev_prep: 36.043, (0.107 sec/step)\n",
      "step 2480, loss = 3.466769, perp: 30.749, dev_prep: 37.940, (0.112 sec/step)\n",
      "step 2500, loss = 3.513419, perp: 32.786, dev_prep: 33.165, (0.160 sec/step)\n",
      "step 2520, loss = 3.367616, perp: 29.306, dev_prep: 34.283, (0.135 sec/step)\n",
      "step 2540, loss = 3.420568, perp: 29.697, dev_prep: 33.276, (0.135 sec/step)\n",
      "step 2560, loss = 3.538491, perp: 34.270, dev_prep: 33.016, (0.094 sec/step)\n",
      "step 2580, loss = 3.501210, perp: 32.197, dev_prep: 33.446, (0.143 sec/step)\n",
      "step 2600, loss = 3.436909, perp: 30.381, dev_prep: 31.840, (0.122 sec/step)\n",
      "step 2620, loss = 3.581180, perp: 35.245, dev_prep: 35.780, (0.139 sec/step)\n",
      "step 2640, loss = 3.406354, perp: 29.199, dev_prep: 35.260, (0.094 sec/step)\n",
      "step 2660, loss = 3.569022, perp: 33.656, dev_prep: 37.088, (0.115 sec/step)\n",
      "step 2680, loss = 3.515270, perp: 32.796, dev_prep: 34.995, (0.143 sec/step)\n",
      "step 2700, loss = 3.491034, perp: 32.440, dev_prep: 30.687, (0.141 sec/step)\n",
      "step 2720, loss = 3.586046, perp: 35.445, dev_prep: 33.540, (0.154 sec/step)\n",
      "step 2740, loss = 3.395212, perp: 29.139, dev_prep: 32.151, (0.117 sec/step)\n",
      "step 2760, loss = 3.433927, perp: 30.412, dev_prep: 30.940, (0.124 sec/step)\n",
      "step 2780, loss = 3.442310, perp: 30.743, dev_prep: 33.738, (0.157 sec/step)\n",
      "step 2800, loss = 3.463047, perp: 31.464, dev_prep: 31.722, (0.125 sec/step)\n",
      "step 2820, loss = 3.234995, perp: 24.670, dev_prep: 33.517, (0.143 sec/step)\n",
      "step 2840, loss = 3.435337, perp: 30.773, dev_prep: 34.321, (0.173 sec/step)\n",
      "step 2860, loss = 3.320182, perp: 27.421, dev_prep: 32.178, (0.111 sec/step)\n",
      "step 2880, loss = 3.376785, perp: 28.607, dev_prep: 31.289, (0.170 sec/step)\n",
      "step 2900, loss = 3.385420, perp: 28.812, dev_prep: 32.387, (0.128 sec/step)\n",
      "step 2920, loss = 3.447394, perp: 31.092, dev_prep: 32.776, (0.134 sec/step)\n",
      "step 2940, loss = 3.339881, perp: 27.934, dev_prep: 30.986, (0.133 sec/step)\n",
      "step 2960, loss = 3.446386, perp: 30.825, dev_prep: 28.657, (0.141 sec/step)\n",
      "step 2980, loss = 3.349971, perp: 27.707, dev_prep: 31.220, (0.182 sec/step)\n",
      "step 3000, loss = 3.384902, perp: 29.019, dev_prep: 27.766, (0.128 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.281622, perp: 26.253, dev_prep: 32.906, (0.112 sec/step)\n",
      "step 3040, loss = 3.421821, perp: 29.266, dev_prep: 28.216, (0.085 sec/step)\n",
      "step 3060, loss = 3.467243, perp: 30.988, dev_prep: 31.793, (0.119 sec/step)\n",
      "step 3080, loss = 3.373752, perp: 28.336, dev_prep: 32.733, (0.121 sec/step)\n",
      "step 3100, loss = 3.420220, perp: 30.076, dev_prep: 32.506, (0.160 sec/step)\n",
      "step 3120, loss = 3.311676, perp: 27.197, dev_prep: 29.597, (0.155 sec/step)\n",
      "step 3140, loss = 3.357386, perp: 28.577, dev_prep: 30.583, (0.137 sec/step)\n",
      "step 3160, loss = 3.425999, perp: 29.984, dev_prep: 27.950, (0.169 sec/step)\n",
      "step 3180, loss = 3.238028, perp: 24.998, dev_prep: 28.665, (0.112 sec/step)\n",
      "step 3200, loss = 3.263928, perp: 26.407, dev_prep: 30.527, (0.142 sec/step)\n",
      "step 3220, loss = 3.392135, perp: 29.004, dev_prep: 30.218, (0.159 sec/step)\n",
      "step 3240, loss = 3.397066, perp: 28.825, dev_prep: 30.864, (0.131 sec/step)\n",
      "step 3260, loss = 3.349307, perp: 27.890, dev_prep: 30.997, (0.134 sec/step)\n",
      "step 3280, loss = 3.391838, perp: 28.973, dev_prep: 28.480, (0.132 sec/step)\n",
      "step 3300, loss = 3.357278, perp: 28.080, dev_prep: 27.891, (0.125 sec/step)\n",
      "step 3320, loss = 3.258745, perp: 25.799, dev_prep: 27.212, (0.129 sec/step)\n",
      "step 3340, loss = 3.429620, perp: 30.465, dev_prep: 30.091, (0.128 sec/step)\n",
      "step 3360, loss = 3.202553, perp: 24.079, dev_prep: 29.923, (0.137 sec/step)\n",
      "step 3380, loss = 3.251376, perp: 25.290, dev_prep: 31.755, (0.121 sec/step)\n",
      "step 3400, loss = 3.307130, perp: 26.593, dev_prep: 26.694, (0.147 sec/step)\n",
      "step 3420, loss = 3.415601, perp: 29.733, dev_prep: 30.048, (0.186 sec/step)\n",
      "step 3440, loss = 3.372044, perp: 28.168, dev_prep: 27.653, (0.135 sec/step)\n",
      "step 3460, loss = 3.397132, perp: 28.839, dev_prep: 28.477, (0.148 sec/step)\n",
      "step 3480, loss = 3.265783, perp: 25.635, dev_prep: 29.969, (0.132 sec/step)\n",
      "step 3500, loss = 3.202666, perp: 24.067, dev_prep: 28.169, (0.117 sec/step)\n",
      "step 3520, loss = 3.282950, perp: 26.222, dev_prep: 28.504, (0.179 sec/step)\n",
      "step 3540, loss = 3.295735, perp: 26.149, dev_prep: 28.206, (0.098 sec/step)\n",
      "step 3560, loss = 3.175330, perp: 23.136, dev_prep: 26.734, (0.077 sec/step)\n",
      "step 3580, loss = 3.212544, perp: 23.959, dev_prep: 23.878, (0.124 sec/step)\n",
      "step 3600, loss = 3.283276, perp: 26.391, dev_prep: 25.557, (0.177 sec/step)\n",
      "step 3620, loss = 3.170876, perp: 22.965, dev_prep: 26.778, (0.167 sec/step)\n",
      "step 3640, loss = 3.108291, perp: 21.825, dev_prep: 27.461, (0.198 sec/step)\n",
      "step 3660, loss = 3.246228, perp: 25.139, dev_prep: 27.237, (0.155 sec/step)\n",
      "step 3680, loss = 3.248877, perp: 25.269, dev_prep: 23.811, (0.169 sec/step)\n",
      "step 3700, loss = 3.188229, perp: 24.044, dev_prep: 27.033, (0.139 sec/step)\n",
      "step 3720, loss = 3.225154, perp: 24.103, dev_prep: 26.072, (0.173 sec/step)\n",
      "step 3740, loss = 3.183800, perp: 23.807, dev_prep: 28.032, (0.138 sec/step)\n",
      "step 3760, loss = 3.243543, perp: 25.101, dev_prep: 27.467, (0.150 sec/step)\n",
      "step 3780, loss = 3.183531, perp: 23.975, dev_prep: 24.829, (0.210 sec/step)\n",
      "step 3800, loss = 3.276516, perp: 25.845, dev_prep: 27.315, (0.142 sec/step)\n",
      "step 3820, loss = 3.096229, perp: 21.726, dev_prep: 26.470, (0.164 sec/step)\n",
      "step 3840, loss = 3.138936, perp: 22.449, dev_prep: 25.021, (0.123 sec/step)\n",
      "step 3860, loss = 3.132425, perp: 22.237, dev_prep: 24.402, (0.178 sec/step)\n",
      "step 3880, loss = 3.189611, perp: 24.294, dev_prep: 26.280, (0.143 sec/step)\n",
      "step 3900, loss = 3.185847, perp: 23.682, dev_prep: 24.860, (0.125 sec/step)\n",
      "step 3920, loss = 3.164871, perp: 23.050, dev_prep: 24.133, (0.117 sec/step)\n",
      "step 3940, loss = 3.094874, perp: 21.426, dev_prep: 26.945, (0.163 sec/step)\n",
      "step 3960, loss = 3.072635, perp: 21.144, dev_prep: 24.078, (0.161 sec/step)\n",
      "step 3980, loss = 3.247568, perp: 24.410, dev_prep: 22.394, (0.182 sec/step)\n",
      "step 4000, loss = 3.181244, perp: 23.559, dev_prep: 23.050, (0.144 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.195049, perp: 23.662, dev_prep: 23.714, (0.122 sec/step)\n",
      "step 4040, loss = 3.211412, perp: 24.220, dev_prep: 26.895, (0.165 sec/step)\n",
      "step 4060, loss = 3.152691, perp: 22.671, dev_prep: 27.427, (0.117 sec/step)\n",
      "step 4080, loss = 2.993968, perp: 19.955, dev_prep: 23.729, (0.126 sec/step)\n",
      "step 4100, loss = 3.056188, perp: 20.705, dev_prep: 23.272, (0.146 sec/step)\n",
      "step 4120, loss = 3.123742, perp: 21.509, dev_prep: 22.820, (0.147 sec/step)\n",
      "step 4140, loss = 3.164472, perp: 22.754, dev_prep: 24.223, (0.146 sec/step)\n",
      "step 4160, loss = 2.947395, perp: 18.574, dev_prep: 21.877, (0.139 sec/step)\n",
      "step 4180, loss = 3.036728, perp: 20.336, dev_prep: 21.864, (0.135 sec/step)\n",
      "step 4200, loss = 3.073425, perp: 21.295, dev_prep: 22.186, (0.107 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.098603, perp: 21.197, dev_prep: 23.602, (0.094 sec/step)\n",
      "step 4240, loss = 3.087997, perp: 21.314, dev_prep: 22.029, (0.144 sec/step)\n",
      "step 4260, loss = 3.008933, perp: 19.913, dev_prep: 22.868, (0.151 sec/step)\n",
      "step 4280, loss = 3.076299, perp: 21.633, dev_prep: 22.166, (0.145 sec/step)\n",
      "step 4300, loss = 3.020086, perp: 19.347, dev_prep: 22.199, (0.127 sec/step)\n",
      "step 4320, loss = 3.065240, perp: 20.903, dev_prep: 19.417, (0.120 sec/step)\n",
      "step 4340, loss = 3.018229, perp: 20.091, dev_prep: 21.481, (0.130 sec/step)\n",
      "step 4360, loss = 3.133813, perp: 22.554, dev_prep: 22.537, (0.152 sec/step)\n",
      "step 4380, loss = 2.880358, perp: 17.590, dev_prep: 20.793, (0.119 sec/step)\n",
      "step 4400, loss = 2.961519, perp: 19.301, dev_prep: 20.974, (0.098 sec/step)\n",
      "step 4420, loss = 2.997795, perp: 19.278, dev_prep: 21.258, (0.175 sec/step)\n",
      "step 4440, loss = 3.068987, perp: 20.993, dev_prep: 20.851, (0.114 sec/step)\n",
      "step 4460, loss = 2.906922, perp: 18.084, dev_prep: 20.472, (0.143 sec/step)\n",
      "step 4480, loss = 2.963557, perp: 18.938, dev_prep: 22.878, (0.152 sec/step)\n",
      "step 4500, loss = 2.948756, perp: 18.776, dev_prep: 20.178, (0.183 sec/step)\n",
      "step 4520, loss = 2.922657, perp: 17.890, dev_prep: 20.978, (0.155 sec/step)\n",
      "step 4540, loss = 2.954915, perp: 18.712, dev_prep: 20.116, (0.133 sec/step)\n",
      "step 4560, loss = 2.951620, perp: 18.726, dev_prep: 20.645, (0.110 sec/step)\n",
      "step 4580, loss = 2.962838, perp: 19.282, dev_prep: 20.282, (0.179 sec/step)\n",
      "step 4600, loss = 2.943194, perp: 18.360, dev_prep: 19.174, (0.174 sec/step)\n",
      "step 4620, loss = 2.910322, perp: 17.894, dev_prep: 20.131, (0.126 sec/step)\n",
      "step 4640, loss = 2.910759, perp: 17.827, dev_prep: 19.899, (0.097 sec/step)\n",
      "step 4660, loss = 2.832406, perp: 16.770, dev_prep: 19.926, (0.162 sec/step)\n",
      "step 4680, loss = 2.948587, perp: 18.373, dev_prep: 20.882, (0.121 sec/step)\n",
      "step 4700, loss = 2.864319, perp: 17.037, dev_prep: 19.831, (0.156 sec/step)\n",
      "step 4720, loss = 2.816446, perp: 15.926, dev_prep: 17.472, (0.102 sec/step)\n",
      "step 4740, loss = 3.045295, perp: 20.752, dev_prep: 20.062, (0.117 sec/step)\n",
      "step 4760, loss = 2.868809, perp: 16.873, dev_prep: 19.379, (0.133 sec/step)\n",
      "step 4780, loss = 2.859322, perp: 16.968, dev_prep: 20.310, (0.117 sec/step)\n",
      "step 4800, loss = 2.879297, perp: 17.549, dev_prep: 18.740, (0.181 sec/step)\n",
      "step 4820, loss = 2.846734, perp: 16.869, dev_prep: 18.483, (0.180 sec/step)\n",
      "step 4840, loss = 2.879930, perp: 17.375, dev_prep: 18.273, (0.158 sec/step)\n",
      "step 4860, loss = 2.788567, perp: 15.914, dev_prep: 16.741, (0.136 sec/step)\n",
      "step 4880, loss = 2.760800, perp: 15.408, dev_prep: 19.159, (0.170 sec/step)\n",
      "step 4900, loss = 2.824353, perp: 16.263, dev_prep: 17.622, (0.130 sec/step)\n",
      "step 4920, loss = 2.828189, perp: 16.650, dev_prep: 19.303, (0.170 sec/step)\n",
      "step 4940, loss = 2.937124, perp: 18.358, dev_prep: 18.088, (0.124 sec/step)\n",
      "step 4960, loss = 2.809934, perp: 16.333, dev_prep: 19.248, (0.156 sec/step)\n",
      "step 4980, loss = 2.913144, perp: 17.765, dev_prep: 19.245, (0.106 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEWCAYAAABG030jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNXdx/HPL3vYwhZ2MCCLrCJGFAQRQUDwsdbqU1FbbG1p3aptn7a41KV1Qe1iba3Vql3c675gFRVQQQET2QVkl00Ie1gCWc7zx9yETBZmQjKZO5Pv+/WaV2buvXPnd+L45ebcc8815xwiIhI7EqJdgIiI1IyCW0Qkxii4RURijIJbRCTGKLhFRGKMgltEJMYouKVBMLMsM3NmlhTtWkRqS8EtvmZml5lZjpntN7OtZvZfMxvmrbvDzAq9daWPPdGuWSTSFNziW2b2M+BB4B6gLdAF+CvwjXKbveCca1Lu0TwKpYrUKwW3+JKZZQC/Aa51zr3inDvgnCt0zr3pnPtFHey/g5m9YWa7zGy1mf2w3LrB3lH+PjPbZmZ/8JanmdnTZrbTzPaY2Wdm1ra2tYjUlPr7xK+GAGnAqxHa//PAUqADcBLwnpmtcc7NAP4E/Mk595SZNQH6ee+ZBGQAnYHDwEDgUITqE6mWjrjFr1oBO5xzRSG2+1/v6Lf0MTPUjs2sM3Am8CvnXIFzbiHwOPBdb5NCoLuZtXbO7XfOzS23vBXQ3TlX7JzLdc7tO67WidSCglv8aifQOoxRIP9xzjUv9xgZxr47ALucc/nllm0AOnrPrwJ6Aiu87pDzveVPAe8Cz5vZFjO738ySw2+SSN1QcItffUqgO+LCCOx7C9DSzJqWW9YF2AzgnFvlnJsItAHuA14ys8ZeH/udzrk+wFDgfI4epYvUGwW3+JJzbi9wG/CwmV1oZo3MLNnMzjOz+2u5743AJ8C93gnHAQSOsp8GMLMrzCzTOVcClA4vLDGzkWbW38wSgX0Euk5KalOLyPFQcItvOed+D/wMuBXIAzYC1wGvldvs2xXGce83szZh7H4ikEXg6PtV4Hbn3PveunHAMjPbT+BE5aXOuUNAO+AlAqG9HPiQQPeJSL0y3UhBRCS26IhbRCTGKLhFRGKMgltEJMYouEVEYkxELnlv3bq1y8rKisSuRUTiUm5u7g7nXGY424YV3Gb2U+AHgAOWAN9zzhVUt31WVhY5OTnh7FpERAAz2xDutiG7SsysI/ATINs51w9IBC49/vJERKQ2wu3jTgLSvXkjGhG4aEFERKIgZHA75zYDvwO+ArYCe51z0ytuZ2aTvTmMc/Ly8uq+UhERAcLrKmlB4I4jXQnMqtbYzK6ouJ1z7jHnXLZzLjszM6z+dREROQ7hdJWMBtY55/Kcc4XAKwRmRhMRkSgIJ7i/As7wZmczYBSBCXZERCQKwunjnkdgRrTPCQwFTAAei3BdIiJSjbDGcTvnbgduj3AtZT5bv4tmacn0atc09MYiIg2ML28WfMnfPgVg/dQJUa5ERMR/NFeJiEiMUXCLiMQYBbeISIxRcIuIxBgFt4hIjFFwi4jEGAW3iEiMUXCLiMQYBbeISIxRcIuIxBgFt4hIjFFwi4jEGAW3iEiMUXCLiMQYBbeISIxRcIuIxBgFt4hIjFFwi4jEGAW3iEiMUXCLiMQYBbeISIxRcIuIxBgFt4hIjFFwi4jEGAW3iEiMUXCLiMQYBbeISIxRcIuIxBgFt4hIjAkZ3GbWy8wWlnvsM7Mb66M4ERGpLCnUBs65lcBAADNLBDYDr0a4LhERqUZNu0pGAWuccxsiUYyIiIRW0+C+FHiuqhVmNtnMcswsJy8vr/aViYhIlcIObjNLAS4AXqxqvXPuMedctnMuOzMzs67qExGRCkL2cZdzHvC5c25bJAopKCzmyTnr6N8xIxK7FxGJGzUJ7olU001SF1ISE3ji43Wc1VNH6yIixxJWV4mZNQbOBV6JWCEJxildWrBsy95IfYSISFwIK7idcwecc62ccxFN1U4t0tm6pyCSHyEiEvN8deVkRnoy+YeLol2GiIiv+Sq4G6UkRrsEERHf81dwp9bkXKmISMPkr+BO1hG3iEgovgru5CRflSMi4ku+SsqkBIt2CSIivuer4E4wBbeISCi+Cu5EHXGLiITkq+BWV4mISGi+Cu4EBbeISEi+Cu5E9XGLiITkr+DWEbeISEgKbhGRGOOz4I52BSIi/uerqExM8FU5IiK+5Kuk1MlJEZHQfBXcOuAWEQnNV1GZpOQWEQnJV0mpk5MiIqH5Kio1yZSISGi+Cm51lYiIhOarpFRui4iE5quo1JWTIiKhKbhFRGKMv4JbJydFRELyV3DriFtEJCQFt4hIjFFwi4jEGF8Fty7AEREJLazgNrPmZvaSma0ws+VmNiQSxehmwSIioSWFud2fgHeccxebWQrQKBLFqKtERCS0kMFtZhnAWcCVAM65I8CRSBRj6ioREQkpnK6SrkAe8A8zW2Bmj5tZ44obmdlkM8sxs5y8vLw6L1RERALCCe4kYBDwiHPuFOAAMKXiRs65x5xz2c657MzMzDouU0RESoUT3JuATc65ed7rlwgEuYiIREHI4HbOfQ1sNLNe3qJRwBcRrUpERKoV7qiS64FnvBEla4HvRa4kERE5lrCC2zm3EMiOcC0iIhIGX105KSIioSm4RURijIJbRCTG+Dq4nXPRLkFExHd8Hdxf7yuIdgkiIr7j6+D+9WtLo12CiIjv+Dq4C4vVVSIiUpGvg7u4RMEtIlKR74K7a+ujEw/OXr0jipWIiPiT74L7tWvOjHYJIiK+5rvgzmiUHO0SRER8zXfBLSIix+b74J6jfm4RkSC+DO71UyeUPb/88XnH2FJEpOHxZXCLiEj1FNwiIjFGwS0iEmMU3CIiMSYmgnvl1/nRLkFExDdiIrjHPvhRtEsQEfGNmAhuERE5KmaCu7C4JNoliIj4gm+D+5undAx6fdMrS6JUiYiIv/g2uP/47YFBr1/K3aSjbhERfBzcVfnzjNXRLkFEJOpiKrhfXbCJNxZtiXYZIiJRFVPBvXHXIX7y3AJ2HTgS7VJERKLG18H9/OQzqlw+6Lfv8e9P19drLSIifuHr4D6jW6tq1932+jL+MmNVPVYjIuIPvg7uUH43/ctolyAiUu+SwtnIzNYD+UAxUOScy45kUSIiUr2wgtsz0jmn+4iJiERZTHeVAGzafbDadW8v2cqz874ia8o08vIP12NVIiKRE25wO2C6meWa2eSqNjCzyWaWY2Y5eXl5dVdhCMPum1ntumue+ZybXw1cKp+7YXd9lSQiElHhBvcw59wg4DzgWjM7q+IGzrnHnHPZzrnszMzMOi0ylI++DPxDccXj83jvi231+tkiIvUtrOB2zm32fm4HXgUGR7Ko8tbcMz7kNt99cj6zV+1g9uod/PDfOVz297kcLioO2kYX7YhIvAgZ3GbW2Myalj4HxgBLI11YqQQLb7srnphX9vyTNTsZeu+MoPWlXSY563fxcu6mOqtPRKS+hTOqpC3wqpmVbv+sc+6diFZVjve5NbaziiPsl3I38X8vLgJgwoD2pCUn1qo2EZFoCHnE7Zxb65w72Xv0dc7dXR+FlXfnBX3rZD+loQ2weNPeOtmniEh9i4nhgJOGZtX5Pj9Yvo3V2/MpLnF1vm8RkUiKieCOhEc/WsvoP3zENc/ksmXPoWiXIyIStgYb3KXeXbaNoVNnhN5QRMQnGnxwi4jEmpgJ7pPaNY3o/g8XFTNr5faIfoaISF2ImeB++eqhzL95VMT2f/XTn3PlPz5j3tqdEfsMEZG6EDPB3Tg1iTbN0iK2/xkrAkfbL3y2MWKfISJSF2ImuEu1aJRc7bp+HZvVev+vLNhc632IiERSzAV36ZWUCQYntGoEQOeW6Sy+YwxvXT+c68/pXuvPuPSxT2u9DxGRSIm54L55fG+SEoyVd53Hm9cPY8KA9rx+7TCapQWOxK+rg+Ceu3ZXrfchIhIpNbkDji9cfGonLj61EwDJiQk8fNmgoPWpSYlcfnoXcjfsZsXX+dEoUUQkomLuiDscd3+zP/dfPKBW+9ixX3fMERF/isvgBnDlpiCZ/tOzanziMvuu98ueHzpSTGFxSV2VJiJSK/Eb3N7Pk9o1pWfbprx1/XBevnoI2Se0YO5No3jpx0OYM+WcY+4ja8o0Plmzg963vcPlf593zG1FROpLzPVxh8t5h9ypSUf/bTr1hJa8dPVQANplpJVtcyyXeYE9f71OWIqIP8TtEXfv9s3o3zGD248xl/fx3qRBRCSa4vaIOy05kTevHxbtMkRE6lzcHnFHyqEjxWRNmcZTczdEuxQRaaAU3DWwbV8BOw8Ehgk+MnN1lKsRkYZKwe257PQuPDTxlGNu81LuJobdNxM4Omrl9YWb+WT1jghXJyJyVNz2cYfrF2N7MbxHawZ0as7BI0XH3DYv/+hFOVv3FlBc4rjh+YUArJ86IaJ1ioiUavDBfe3Io3ObJCYce5TJPz9ZH/T6tteXRqIkEZFjUldJOalJidw6oXfY23+86mgXyXXPfh6JkkREKlFwV/CD4d146qrBYW371a6DZc/fWrw1UiWJiARRcFdheI/MaJcgIlItBXc12mfU/DZpj8xaw2MfreHA4SJe+OyrsC6pFxGpKYtEuGRnZ7ucnJw63299OlJUQu6G3Uz8+9zj3sf3z+zKhad0YECn5nVYmYjEIzPLdc5lh7OtjrirkZKUwJATW7F+6gTu/ma/49rHk3PWccFf5tT4yPvgkaKQQxNFpOFScIehX4eMWr3/Fy8tZvaqHce8OUNhceAI3zlHn9vepf8d02v1mSISv8Iex21miUAOsNk5d37kSvKfkzvXrqvjpdxNvJS7CYAHvz2QNxZt4c8TT6FxahIzVmxjbd4BHv94HV/vK+Ceb/YHoLhE/eMiUrWaXIBzA7AcqNmtZOJM55bpbNx16Ljff+MLgSst+97+Lm9dP4zv/zP4XMDCjburfN/iTXvYvu8wo/u0DVq+afdBdu4/Uut/XEQkdoTVVWJmnYAJwOORLce/Xr56KL+/5GQeuvTY85nUxPl/nl1pWfkbHM9etYM1efsBuOAvc/jBv3P4dM1OAEpKHPsKChl230y+8fCcOqtJRPwv3D7uB4FfAg32xounntCCb53aiYGdmzP5rG4ADOnWihNaNarTz1m8aW/Z8yuemMeo33/IvLU7y5bd8cYyAKa+s4IB5frBx/7xozqtQ0T8K2RXiZmdD2x3zuWa2dnH2G4yMBmgS5cudVag35gZN4/vTdfWjRnTpy2tmqTyxqIt/OS5BRH7zG8/dnRI4spt+Yx78KOgI/PS5QcOF/GrlxdzxwV9OVxUQoeMNN3lRyQOhRzHbWb3At8BioA0An3crzjnrqjuPfEwjrsmnHN0ventaJfB+P7teHvJ12WvfzXuJK4++8QoViQi4arTcdzOuZucc52cc1nApcCMY4V2Q2RmrJ86gRW/Hcdj3zk1anWUD22AT9ZonnCReKRx3HUoLTmRMX3bRbuMMmvzDnDD8wuCAnzbvgI27DxQ7Xte+XwTWVOmsfdgYX2UKCLHoUbB7Zyb1dDGcNdGq8YpAEwc3JmhJ7aq98/fvOcQry/cwmV/n8fmPYcoLnGcfs8HjHhgFtvzC6p8zxOz1wHBMx+KiL80+BspRFLur89l/+EimqQmUVRcQvdb/hu1Ws6cOiPo9bce+YTpN44gPSWRbfsKeHruBs7ulcmyLfsA0DlNEf/SJFMRsGpbPkmJCXRt3Tho+SOz1nDfOyuiVFXVmqUl0aF5eqVRKqNOasNvLuxHx+bpZcucc1z77OdcPaI7/TuFngZg5srtdG6RTvc2Teu8bpF4U5OTkwruKNiy5xApSQm0apzCizmb+OP7X7J1b9VdF36w7t7x/PzFRRwuKmGad8OIt64fRo+2TVibd4A5q3fwg+Hd+Ous1Yzr245umU34Yss+xj/0MQDzbxlFm6Y1nyZXpCFRcMeYDTsPMOKBWdEuo1oXndKRVxZsrrT84lM7lc3BcnrXlsxbt4umaUksuWMs/12ylaufCdzO7SejevCzc3vWa80isUbTusaYVk1SAbhyaBZndq//k5ihVBXaQFloA8xbtwuA/IIiNuw8UBbaAA99sIrpy76u9H4ROT464vaJ3QeO0Cw9mSNFJdz0ymJuntCbNk3T2LLnEE/OXsfj3miPWNA0LYn8gsrzia+7dzy5G3YzZ/VOPlqVR+6G3VxyaiceuORkAN5espX0lETumbacET0z6dWuKZdkdy57f17+YZISjBbeaJ2KctbvIv9wESN7tQHgnN/Nok2zVJ6fPCQCrRSpWzU54taoEp8oDaP0lEQeLDeRVYfm6VwzsjuPz17Hj87qxp6DhbyQszHs/aYlJ1BQWL9TzFQV2gB3vvkF//xkfdCyF3M3ccUZJ3By5+ZcU+4ofdX2wORanVs2Yu+hQnbsP8wtry4FYP3UCZX2/eGXeUx6cn7Q+rU7DrB2x9Ex6845XszZxP+c3IH0lMTjb6BIlCm4Y0DLxil8dstoWjZOITHBGNErk+ysFgy++4Og7QZ3bcnVZ5/IWT0yOfHmwCX4Pds2DZq4KpoqhnapY81ueOljlW8dlzVlGhAc4KWhDVBUXEJSYuVewI9W7eCXLy/m49U7+OHwrmSkJ1NY7GjbLJWmacnhNkMk6hTcMSKzaWrZ8/H92wOB4Hrog1X8/aO1XHZ6F64f1YMmqYH/pK9fe2bcT/c6/k8f88XWfSy6fUzQ8u63/DdoKOb2/IKybieANxdt4c1FW8rW9++YQXpKIhcP6sRNry7h1xN607FFIzLSkxnctWX9NEakBtTHHacKi0u48YWF3DCqB2PifMrXq88+kUdmrTnmNm9cdyYX/KXm/5Atu3MsJc7piFwiTsMBJcjYP37Eym35TL2oP+f0bsOSTXu56l/671MTb/9kOH06BN/8afaqHTw3/yv+ctkpmj5Xak3BLUGKiksodo7UpMAJuZISx6MfreWS7E6kJyeSX1DElr2HuOivn4TcV98Ozcoui29o0pIT+PPEQQzolMG9by/ntYWB7pa194xn1pfby25D98mUc+hQ7opTkXAouOW4lZ74q07OraPJvuv9eqomNtw6oTd3TVte9vqy07swtm87RvTM5M1FW2ifkUZ2Vui+8i17DtFeN79osHQBjhy31649k7su7FflujlTzqF1k9Qq11UnPTmRDhnxfbl7+dAGeHbeV0x6cj6ff7Wb659bwMV/+5QLH57DiAdmBm3nnOP301eyNm8/n3+1m6FTZ/BiziZEQtGoEgkysHNzBnZuzq2vBcZM/2RUD7bsOcT4/u2CJpwC+OnonlwwsAO/n76St7w5TEqlJSfwwMUn8z8ndwDg8sfnMmf1TipKSUrgSFF83sq0fNfTwo17APjXJ+txznHHm1+UrXt1wWb6tA/0n8/6cju/fHkxAHd/sx9pSYl869ROYX9mQWExH36Zxz/mrOPPEwcFjUaS+KGuEqnSoo17OHC4iKHdW1dad/6fP2bp5n1B46jveXs5LRqlkJxo3DVtORMHd+Hei/qXrZ+2eCvXPnv0ApvfXtiPvPzD/OzcniG7Zxq6JXeMoVFKErkbdnP107m8fcNwMpukcstrS+nauhGTzzp6e7ryv8vRvdvw+KTTolGyHAddOSm1dnLn5tWu+8+PhrDvUPDVkTeP7w3A3oOFTP9iG9eODL7X5fj+7fjH907jx0/lcriohO+ccUKl/TZNS+KzW0az88AR1mzfT3JiAgM6ZdD39nePqw0/HN6VRRv3Mn/9ruN6v1/0v2M6Vw7NKruA6fR7gi+8Kg3u2auCb1W3afchNuw8wAmtjo5pd86Rt/+wZmuMcTrilnq1Nm8/C77aE/Tn/xn3fMDX+wqqvJQdQp8wrbR9q0bM+sVIAPYVFDLgjunHX3AMaNU4hZ0HjlS7ftmdY9m5/whnPTCTsX3b8u6ybQD8Ymwvrh3Zvb7KlBB0clJ8q1tmk0p9th/8fAQLbzu32vd8d0jlo/PSI/ozurXkh8O7BoV+aWgDJCWEHqFRse8+1hwrtAF+/fpSZqwIhHVpaAM88O5KBv32PbbtK2BfQSEby92urrC4hMNFxSzbspd3NbOj76irRKKuceqxv4a/+UY/tu0r4N1l2/jTpQMZ27cdG3Ye5OGZa7jrwv50b9MECFzmX3EkXaOUo/tu3SSFv383m7cWby27t2Zp4Dvn6HrT22XbnnNSG2as2F4XzYu6Vz7fTJeWjapct+vAkaCul9Lfx+g/fMiGnUeD/PLTu9CycQo/H9MrssVKWBTcEhPG9WvHu8u2MaBTc9KSE+nVrmmlrpXq+uXX3jMeM8rGR5/SpQXn9WsXNJ9J+bHTS+4Yw/Kt+cxYsZ2Jgzvz3PzwZ2P0qwffXxXWdmP++CEv/nhoUGgDPDPvKwB+MKwbW/Yeonf7ZlW9HYCDR4pIMCMtuW5mYNy46yDb8ws49QTNG1NKfdwSMw4XFZdd/RkJ5WcddM7x+sItjO3bjt63vQPAw5cNYuHG3aQnJ9IkLYl73l7BoC7NuXZk9wYzhcAJrRqxYefBY97VKGvKNNKTE1n+23FByzfuOkje/sMM6tKiRp9Z1WyQ8Uh93BKXIhnaAHde0Jf7vzUACByBX3hKR9JTEhnWvTU92jRhwoD23DKhDz8r110wqEsLRvVuS0pSw/hfqfRI/KEPAkfwY/74YZUnjw8VFldaNvz+mWVj299avIVfvLgogpXGN3WViHgmDc2qcvnTPzi90rLSP1RLe1jeuO5Mxj34cdn6iYO7sHnPIbJaNeLfn26o61J9oXxg7z1YiMNR/g/4vPzDNElNIj0lkfJ/2TvnuO7ZBQBldz+SmlFwixyHCwZ24InZ67js9MCIl9JJpVKSEvj5uT35/rCuJHs3c3h67gZKKvRIfjLlHC7526ds3nOIX4ztRY82TZj8VG7QNu2apXFun7Y8Ndf/wX/ybyoPuTzt7qrntHl94ZYql4eydPNe+nXMOK73xpuG8fedSB1rn5HO/FtGl53gTPFCenBWS3404sSy0AZYffd41t4znt98oy9PX3U6824eRYfm6QzvEbgqtWXjFMb0bRe0///N7sTcm0fx2wv7saJCXzHA/RcPOGZ9b/9keK3aF0mbvRtalFq+dR9ZU6axcddB9hw8wrZ9BWXr3vvi6PDF8/88u95q9DsdcYvUgbTkRN68bhhdMxtXWpfgjSX/7pCsoOUZjQI3Z0ivMPrikcsHcZ53l6PSfVdUUvEQHujXsRm79h/hnov606dDMwZ2bl42R4qffLH16LTAlz72KXPXBq5sfW7+Vzw+ex1HikoY3bst4/q14/+q6Ac/XFRMcYkLGurZ0GhUiUiUFBQW88y8r7hyaBaJCcbLuZsY0CmDHm2bVto2a8o0BnZuzos/HoJzUFziuOXVJYzq3ZbcDbsZ3LUlI0/KDDqB+88564Ims7rm7BP5a4g7BcWSeBtlovm4ReLMwSNFJCcmBHXBhOKcY8/BQoqdo2laEqlJiXy18yBnPTCTTi3SuWpYV+4sF+yxZv3UCby1eAsDOjanZZMUbnhuAY9ccWqlET5r8/bTtXVjzIzvPjmfkztl+PJCIk0yJRJnjqdbwMxo0TglaFmrJoHXPxzejUlDsxjXrx13vLGM4hLH+8sDV4q2aJTM7oOFAGSf0IKcDbtrWX1kVDUMcfxDH/P+z0aUvf5k9Q4ue3wePxrRjUc/XAvAR1/m8fMxvXDOMWPFds7u1YbEMKZG8JOQ3wYzSwM+AlK97V9yzt0e6cJEpO41Tk0K6mJon5HOo98JHOR9uS2ff3+6nvP6tefyx+fxjytPIzurBRMems3uA0fIP1xUzV79Y/X2/azN2885v/+w7GIhoCy0y5v+xTZ+5I3kKb3oak3eAZqlB/46yUj37w2iQ3aVWOBa4MbOuf1mlgzMBm5wzs2t7j3qKhGJbbsPHAk6Wv/RUzlBE1Tde1F/bnplSTRKqxPrp05g2H0z2LQ7MMLl9WvPZMnmvWU3EGmWlsTiO8bWa0112lXiAsm+33uZ7D3qvmNcRHyjYhfLvRcNoH/HDfxu+pd0admIS0/rzPkD2pOYYHy2fjeTnpxftu1J7Zqy4uv8+i65Rm58fkFZaAN84+E5Qev3FRz966KgsJglm/dyWhj3Da0vYZ3pMLNEM1sIbAfec87Nq2KbyWaWY2Y5eXl5dV2niERRy8YpXHdOD9bcM55Z/3c2ZkbTtGQapSQxomdmpe0///W5ZVPvThpyAq9cM7S+Sz6m18K8CGjmiu0MnTqDS/72KRt2HgACc66UH2b53hfbyJoyjVcXbKpymGYkhBXczrli59xAoBMw2Mwq3U3WOfeYcy7bOZedmVn5P6SIxL7EBCsbl17eGd2OHo1efGonWjZO4dqR3fnWoE7cMLong7q04I3rzqzPUmttw84DfO+fn7HLm++8dJ6V4ffP5MKH57Bx10Gcczw3PzBz4k9fWMT9766sl9pqPBzQzG4DDjrnflfdNurjFmlYDhwu4pl5G0hJTGDS0KygaXIrKh0Nsvru87j1taU8/1lg2twfjziRv314dJz5iJ6ZfPjl0b/erxrWlf/kbCS/wN8nSY93fHmdzg5oZplm1tx7ng6cC6w4rspEJC41Tk1i8lkncuWZXY8Z2uUlJSYw9VsDaOrdSOOXY4+OrV551zge/c6pZa/X3TueX5/fhyX1fMLQr8IZHNoe+JeZJRII+v84596KbFkiEq+emJQdNC59/i2jKXGOhAQjMcEoLnEYRmpyAk3TksgvKAr6x2BY99bMXh24MfJFgzryyueb670N0RbOqJLFwCn1UIuINACjercNep2ecvQy/XN7t+WdZV9T2o3+wc9HsH3f4aDt//G903hi9jquHJpFWnIit07ogxG4MfSIB2ZFuHp/0CXvIuIbh4uK2bH/SK1u4Pzg+1+Gfau2SKiPPm5d8i4ivpGalFir0Aa4cXRPzh/Qnrz8I0z8e+A6waeuGkxhcQnnnNSWU3/7Hju9kSIA//zeacxduyvoxKjfKbhFJO50b9OU7m3g+nO6U1ziGN7j6BDlYT1cPgzQAAAGtUlEQVRa8/rCLfzr+4MpKXGc3asNZ/dqw6/G9WLUHz5kbd6BSvsb3bst7y/fVml5tCi4RSRuVTUL4P0XD+DG0T3LboJRysw4pXOLoOBedNsY/jprNdeM7M7Jd1a+y0+0KLhFpEFJTUqsFNqlbju/D0UlJWRntWR8v3ZkNErmpvG9AfjPj4bwv49+GrR909SkqEy+peAWEfFkNErmT5dWPYhucNeW/GrcSYzp25b2GWkYVjYi5vWFm+nVrimZTVLrpU4Ft4hImK4++8Qql39jYMd6rUM3CxYRiTEKbhGRGKPgFhGJMQpuEZEYo+AWEYkxCm4RkRij4BYRiTEKbhGRGBORaV3NLA/YcJxvbw3sqMNyYoHaHP8aWntBba6pE5xzYd2wNyLBXRtmlhPunLTxQm2Ofw2tvaA2R5K6SkREYoyCW0QkxvgxuB+LdgFRoDbHv4bWXlCbI8Z3fdwiInJsfjziFhGRY1Bwi4jEGN8Et5mNM7OVZrbazKZEu57aMLMnzWy7mS0tt6ylmb1nZqu8ny285WZmD3ntXmxmg8q9Z5K3/SozmxSNtoTLzDqb2Uwz+8LMlpnZDd7yuG23maWZ2XwzW+S1+U5veVczm+e17QUzS/GWp3qvV3vrs8rt6yZv+UozGxudFoXHzBLNbIGZveW9juv2ApjZejNbYmYLzSzHWxa977ZzLuoPIBFYA3QDUoBFQJ9o11WL9pwFDAKWllt2PzDFez4FuM97Ph74L2DAGcA8b3lLYK33s4X3vEW023aMNrcHBnnPmwJfAn3iud1e7U2858nAPK8t/wEu9Zb/Dbjae34N8Dfv+aXAC97zPt53PhXo6v2/kBjt9h2j3T8DngXe8l7HdXu9mtcDrSssi9p3O+q/EK9BQ4B3y72+Cbgp2nXVsk1ZFYJ7JdDee94eWOk9fxSYWHE7YCLwaLnlQdv5/QG8DpzbUNoNNAI+B04ncOVckre87LsNvAsM8Z4nedtZxe97+e389gA6AR8A5wBvefXHbXvL1VhVcEftu+2XrpKOwMZyrzd5y+JJW+fcVu/510Bb73l1bY/Z34n3J/EpBI5A47rdXrfBQmA78B6Bo8c9zrnSW3+Xr7+sbd76vUArYqvNDwK/BEq8162I7/aWcsB0M8s1s8nesqh9t3Wz4Chwzjkzi8txmGbWBHgZuNE5t8/MytbFY7udc8XAQDNrDrwKnBTlkiLGzM4Htjvncs3s7GjXU8+GOec2m1kb4D0zW1F+ZX1/t/1yxL0Z6FzudSdvWTzZZmbtAbyf273l1bU95n4nZpZMILSfcc694i2O+3YDOOf2ADMJdBU0N7PSg6Ly9Ze1zVufAewkdtp8JnCBma0HnifQXfIn4re9ZZxzm72f2wn8Az2YKH63/RLcnwE9vLPTKQROZLwR5Zrq2htA6VnkSQT6gEuXf9c7E30GsNf78+tdYIyZtfDOVo/xlvmSBQ6tnwCWO+f+UG5V3LbbzDK9I23MLJ1An/5yAgF+sbdZxTaX/i4uBma4QGfnG8Cl3iiMrkAPYH79tCJ8zrmbnHOdnHNZBP4fneGcu5w4bW8pM2tsZk1LnxP4Ti4lmt/taHf6l+uoH09gJMIa4JZo11PLtjwHbAUKCfRjXUWgb+8DYBXwPtDS29aAh712LwGyy+3n+8Bq7/G9aLcrRJuHEegHXAws9B7j47ndwABggdfmpcBt3vJuBIJoNfAikOotT/Ner/bWdyu3r1u838VK4Lxoty2Mtp/N0VElcd1er32LvMey0nyK5ndbl7yLiMQYv3SViIhImBTcIiIxRsEtIhJjFNwiIjFGwS0iEmMU3BK3zOxGM2sU7TpE6pqGA0rc8q7wy3bO7Yh2LSJ1SUfcEhe8q9umeXNjLzWz24EOwEwzm+ltM8bMPjWzz83sRW9eldK5lu/35lueb2bdo9kWkVAU3BIvxgFbnHMnO+f6EZjFbgsw0jk30sxaA7cCo51zg4AcAvNKl9rrnOsP/MV7r4hvKbglXiwBzjWz+8xsuHNub4X1ZxCYwH+ONw3rJOCEcuufK/dzSMSrFakFTesqccE596V3i6jxwF1m9kGFTQx4zzk3sbpdVPNcxHd0xC1xwcw6AAedc08DDxC4dVw+gduoAcwFziztv/b6xHuW28W3y/38tH6qFjk+OuKWeNEfeMDMSgjMyng1gS6Pd8xsi9fPfSXwnJmleu+5lcCMlAAtzGwxcJjALaZEfEvDAaXB07BBiTXqKhERiTE64hYRiTE64hYRiTEKbhGRGKPgFhGJMQpuEZEYo+AWEYkx/w/oH2AMCvqJCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"CE loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.savefig(loss_fig)\n",
    "plt.show()\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.savefig(perp_fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.597469740297253"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
