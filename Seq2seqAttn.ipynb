{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        # transpose to [batch_size, num_units, max_time]\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        \"\"\"\n",
    "        if self._dec_init_states is None:\n",
    "            attn_state_0 = None\n",
    "\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "            h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "            context_0 = self._compute_context(h_0)\n",
    "            h_0 = context_0 * 0\n",
    "            attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECMWrapper(RNNCell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, time_major=False, dtype=tf.float32,\n",
    "                  forget_bias=1.0, name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        mode: train or infer\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            # need batch-major\n",
    "            if time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=False,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099527, 0.00099685, 0.00099804, ..., 0.00099652,\n",
       "         0.00099729, 0.0009987 ],\n",
       "        [0.00099426, 0.00099705, 0.00099913, ..., 0.00099621,\n",
       "         0.000998  , 0.00099956],\n",
       "        [0.00099372, 0.00099742, 0.0010001 , ..., 0.00099612,\n",
       "         0.00099871, 0.00099994],\n",
       "        [0.00099348, 0.00099778, 0.00100084, ..., 0.00099622,\n",
       "         0.00099925, 0.00100009]],\n",
       "\n",
       "       [[0.00099527, 0.00099685, 0.00099804, ..., 0.00099652,\n",
       "         0.00099729, 0.0009987 ],\n",
       "        [0.00099425, 0.00099697, 0.0009991 , ..., 0.00099622,\n",
       "         0.00099802, 0.00099953],\n",
       "        [0.00099447, 0.00099776, 0.00099964, ..., 0.00099613,\n",
       "         0.0009985 , 0.00099983],\n",
       "        [0.00099425, 0.00099838, 0.00099942, ..., 0.00099603,\n",
       "         0.00099805, 0.00099985]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=True, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[ 1.93638448e-03, -5.23794675e-04,  1.43888255e-03, ...,\n",
       "          -6.44994527e-03,  3.96791787e-04, -1.38647342e-03],\n",
       "         [ 1.93638448e-03, -5.23794675e-04,  1.43888255e-03, ...,\n",
       "          -6.44994527e-03,  3.96791787e-04, -1.38647342e-03],\n",
       "         [ 1.93638448e-03, -5.23794675e-04,  1.43888255e-03, ...,\n",
       "          -6.44994527e-03,  3.96791787e-04, -1.38647342e-03],\n",
       "         [ 1.93638448e-03, -5.23794675e-04,  1.43888255e-03, ...,\n",
       "          -6.44994527e-03,  3.96791787e-04, -1.38647342e-03],\n",
       "         [ 1.93638448e-03, -5.23794675e-04,  1.43888255e-03, ...,\n",
       "          -6.44994527e-03,  3.96791787e-04, -1.38647342e-03]],\n",
       "\n",
       "        [[ 5.87788643e-04, -6.11365598e-04,  9.57452459e-04, ...,\n",
       "          -5.31771965e-03,  1.28298602e-03, -5.45570743e-04],\n",
       "         [-1.40168064e-04, -5.27717406e-04,  6.65158615e-04, ...,\n",
       "          -5.97953144e-03,  1.29185198e-03, -1.05726928e-03],\n",
       "         [ 5.87788643e-04, -6.11365598e-04,  9.57452459e-04, ...,\n",
       "          -5.31771965e-03,  1.28298602e-03, -5.45570743e-04],\n",
       "         [ 7.29985884e-04, -2.68261821e-04,  1.77189684e-03, ...,\n",
       "          -6.25581015e-03,  1.14935916e-03, -8.58279294e-04],\n",
       "         [ 3.23404995e-04, -2.26336822e-04,  1.88008556e-03, ...,\n",
       "          -5.44158928e-03,  1.31273072e-03, -8.32406397e-04]],\n",
       "\n",
       "        [[-1.74730527e-03, -9.47531313e-04,  2.08150363e-04, ...,\n",
       "          -4.08617919e-03,  1.95362885e-03, -3.09285475e-04],\n",
       "         [-1.01089478e-03, -1.02493411e-03,  5.07696648e-04, ...,\n",
       "          -3.42241861e-03,  1.93755468e-03,  1.98507623e-04],\n",
       "         [-1.74730527e-03, -9.47531313e-04,  2.08150363e-04, ...,\n",
       "          -4.08617919e-03,  1.95362885e-03, -3.09285475e-04],\n",
       "         [-2.83522042e-03, -6.99642871e-04, -2.47190008e-04, ...,\n",
       "          -5.15939854e-03,  2.00028834e-03, -9.55402502e-04],\n",
       "         [-1.01089478e-03, -1.02493411e-03,  5.07696648e-04, ...,\n",
       "          -3.42241861e-03,  1.93755468e-03,  1.98507623e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-8.41975491e-03,  3.50603741e-03, -8.32772581e-04, ...,\n",
       "           1.38758018e-03,  4.89140721e-03,  1.70798204e-03],\n",
       "         [-8.41975491e-03,  3.50603741e-03, -8.32772581e-04, ...,\n",
       "           1.38758018e-03,  4.89140721e-03,  1.70798204e-03],\n",
       "         [-8.41975491e-03,  3.50603741e-03, -8.32772581e-04, ...,\n",
       "           1.38758018e-03,  4.89140721e-03,  1.70798204e-03],\n",
       "         [-8.41975491e-03,  3.50603741e-03, -8.32772581e-04, ...,\n",
       "           1.38758018e-03,  4.89140721e-03,  1.70798204e-03],\n",
       "         [-8.41975491e-03,  3.50603741e-03, -8.32772581e-04, ...,\n",
       "           1.38758018e-03,  4.89140721e-03,  1.70798204e-03]],\n",
       "\n",
       "        [[-8.22045747e-03,  3.79317068e-03, -9.95856361e-04, ...,\n",
       "           2.18817196e-03,  5.14850346e-03,  2.21759500e-03],\n",
       "         [-8.28591362e-03,  3.20874876e-03, -1.37880770e-03, ...,\n",
       "           2.97128689e-03,  4.99090180e-03,  2.36310437e-03],\n",
       "         [-8.22045747e-03,  3.79317068e-03, -9.95856361e-04, ...,\n",
       "           2.18817196e-03,  5.14850346e-03,  2.21759500e-03],\n",
       "         [-8.28591362e-03,  3.20874876e-03, -1.37880770e-03, ...,\n",
       "           2.97128689e-03,  4.99090180e-03,  2.36310437e-03],\n",
       "         [-8.22045747e-03,  3.79317068e-03, -9.95856361e-04, ...,\n",
       "           2.18817196e-03,  5.14850346e-03,  2.21759500e-03]],\n",
       "\n",
       "        [[-7.90332444e-03,  3.96424579e-03, -1.21237675e-03, ...,\n",
       "           2.89386907e-03,  5.34489471e-03,  2.68962327e-03],\n",
       "         [-7.96785578e-03,  3.37702548e-03, -1.59638713e-03, ...,\n",
       "           3.67469667e-03,  5.18787559e-03,  2.83438293e-03],\n",
       "         [-7.39953062e-03,  3.65195237e-03, -9.48761706e-04, ...,\n",
       "           2.99325888e-03,  5.10388380e-03,  3.16484203e-03],\n",
       "         [-8.15996714e-03,  3.60719790e-03, -1.44093449e-03, ...,\n",
       "           2.76733609e-03,  5.46679925e-03,  2.10814970e-03],\n",
       "         [-7.85718299e-03,  3.42677510e-03, -7.30134139e-04, ...,\n",
       "           2.76295142e-03,  4.67751641e-03,  2.29640724e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 3.25441768e-04, -3.33177252e-03,  4.99912770e-04, ...,\n",
       "          -1.99744874e-03,  1.56987016e-03, -9.44266678e-04],\n",
       "         [ 3.25441768e-04, -3.33177252e-03,  4.99912770e-04, ...,\n",
       "          -1.99744874e-03,  1.56987016e-03, -9.44266678e-04],\n",
       "         [ 3.25441768e-04, -3.33177252e-03,  4.99912770e-04, ...,\n",
       "          -1.99744874e-03,  1.56987016e-03, -9.44266678e-04],\n",
       "         [ 3.25441768e-04, -3.33177252e-03,  4.99912770e-04, ...,\n",
       "          -1.99744874e-03,  1.56987016e-03, -9.44266678e-04],\n",
       "         [ 3.25441768e-04, -3.33177252e-03,  4.99912770e-04, ...,\n",
       "          -1.99744874e-03,  1.56987016e-03, -9.44266678e-04]],\n",
       "\n",
       "        [[-8.97133548e-04, -3.50088789e-03,  9.01687774e-04, ...,\n",
       "          -2.68696202e-03,  2.75441981e-03, -2.96047307e-04],\n",
       "         [-1.45873812e-03, -3.56720528e-03,  1.00533408e-03, ...,\n",
       "          -1.89506437e-03,  1.67783094e-03, -2.99240899e-04],\n",
       "         [-1.80710177e-03, -3.75373848e-03, -1.13716116e-04, ...,\n",
       "          -2.88773584e-03,  2.11405428e-03, -4.91171202e-04],\n",
       "         [-5.75134181e-04, -3.34664294e-03,  1.14982156e-03, ...,\n",
       "          -2.50336877e-03,  1.78289204e-03, -7.12194538e-04],\n",
       "         [-8.97133548e-04, -3.50088789e-03,  9.01687774e-04, ...,\n",
       "          -2.68696202e-03,  2.75441981e-03, -2.96047307e-04]],\n",
       "\n",
       "        [[-3.18742427e-03, -3.35928006e-03,  1.81313674e-03, ...,\n",
       "          -6.97765034e-04,  1.32949091e-03, -7.89610203e-06],\n",
       "         [-3.18742427e-03, -3.35928006e-03,  1.81313674e-03, ...,\n",
       "          -6.97765034e-04,  1.32949091e-03, -7.89610203e-06],\n",
       "         [-3.18742427e-03, -3.35928006e-03,  1.81313674e-03, ...,\n",
       "          -6.97765034e-04,  1.32949091e-03, -7.89610203e-06],\n",
       "         [-2.28773407e-03, -3.28646787e-03,  1.64561113e-03, ...,\n",
       "          -2.06792168e-03,  2.80235196e-03,  5.51987032e-05],\n",
       "         [-3.18742427e-03, -3.35928006e-03,  1.81313674e-03, ...,\n",
       "          -6.97765034e-04,  1.32949091e-03, -7.89610203e-06]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-7.00253900e-03,  2.91452568e-04,  5.38770994e-03, ...,\n",
       "           9.76532605e-03, -9.61068261e-04, -5.27518103e-04],\n",
       "         [-6.81794621e-03,  2.47981807e-04,  5.84882079e-03, ...,\n",
       "           9.84647032e-03, -8.29989673e-04, -7.89255835e-04],\n",
       "         [-7.18823075e-03, -8.18184053e-04,  4.82847402e-03, ...,\n",
       "           9.35268216e-03,  4.32878558e-04, -9.46319895e-04],\n",
       "         [-6.87284395e-03, -3.25464644e-05,  7.04950560e-03, ...,\n",
       "           9.97381378e-03, -5.34559134e-04, -1.34080346e-03],\n",
       "         [-6.88160583e-03, -2.55764404e-04,  6.10991335e-03, ...,\n",
       "           9.72959772e-03, -1.79615570e-04, -1.17811922e-03]],\n",
       "\n",
       "        [[-6.50115032e-03, -8.53478501e-04,  4.71528387e-03, ...,\n",
       "           9.86672752e-03,  1.50251936e-03, -1.40356319e-03],\n",
       "         [-5.88608719e-03,  1.09155825e-03,  6.50608540e-03, ...,\n",
       "           1.06945150e-02, -7.22279714e-04, -1.14967243e-03],\n",
       "         [-5.77124814e-03,  1.21272635e-03,  5.32627292e-03, ...,\n",
       "           1.05558978e-02, -8.64339876e-04, -1.60669209e-03],\n",
       "         [-6.50115032e-03, -8.53478501e-04,  4.71528387e-03, ...,\n",
       "           9.86672752e-03,  1.50251936e-03, -1.40356319e-03],\n",
       "         [-5.54948440e-03,  1.29886670e-03,  5.39049692e-03, ...,\n",
       "           1.05713680e-02, -7.98051711e-04, -1.73157558e-03]],\n",
       "\n",
       "        [[-5.77377202e-03, -1.18638901e-03,  4.63563576e-03, ...,\n",
       "           1.00479396e-02,  2.62038619e-03, -1.74641295e-03],\n",
       "         [-4.16910276e-03,  2.05499958e-03,  4.82113706e-03, ...,\n",
       "           1.10658258e-02, -9.22985957e-04, -2.98259314e-03],\n",
       "         [-3.63990664e-03,  1.96285732e-03,  6.41363347e-03, ...,\n",
       "           1.12474840e-02, -3.96468415e-04, -3.95872304e-03],\n",
       "         [-4.25728457e-03,  1.97839434e-03,  6.51901960e-03, ...,\n",
       "           1.11965863e-02, -6.10203948e-04, -2.25628656e-03],\n",
       "         [-3.99553217e-03,  2.19762512e-03,  4.69011255e-03, ...,\n",
       "           1.10279601e-02, -8.80107400e-04, -3.03207524e-03]]],\n",
       "\n",
       "\n",
       "       [[[-1.26798684e-03, -2.63020396e-03,  3.43415863e-03, ...,\n",
       "          -8.78956635e-04,  2.13699904e-03,  9.15111683e-04],\n",
       "         [-1.26798684e-03, -2.63020396e-03,  3.43415863e-03, ...,\n",
       "          -8.78956635e-04,  2.13699904e-03,  9.15111683e-04],\n",
       "         [-1.26798684e-03, -2.63020396e-03,  3.43415863e-03, ...,\n",
       "          -8.78956635e-04,  2.13699904e-03,  9.15111683e-04],\n",
       "         [-1.26798684e-03, -2.63020396e-03,  3.43415863e-03, ...,\n",
       "          -8.78956635e-04,  2.13699904e-03,  9.15111683e-04],\n",
       "         [-1.26798684e-03, -2.63020396e-03,  3.43415863e-03, ...,\n",
       "          -8.78956635e-04,  2.13699904e-03,  9.15111683e-04]],\n",
       "\n",
       "        [[-2.54218397e-03, -9.71326139e-04,  4.00885241e-03, ...,\n",
       "          -2.40464136e-03,  2.60597491e-03,  1.19149545e-03],\n",
       "         [-2.54218397e-03, -9.71326139e-04,  4.00885241e-03, ...,\n",
       "          -2.40464136e-03,  2.60597491e-03,  1.19149545e-03],\n",
       "         [-2.54218397e-03, -9.71326139e-04,  4.00885241e-03, ...,\n",
       "          -2.40464136e-03,  2.60597491e-03,  1.19149545e-03],\n",
       "         [-2.43230490e-03, -1.17037550e-03,  3.70297814e-03, ...,\n",
       "          -2.41245423e-03,  1.83185539e-03,  9.21150320e-04],\n",
       "         [-2.51102494e-03, -8.45937466e-04,  3.79896723e-03, ...,\n",
       "          -1.55165722e-03,  2.87604192e-03,  1.63536542e-03]],\n",
       "\n",
       "        [[-3.44768562e-03,  4.37784765e-05,  4.92167752e-03, ...,\n",
       "          -3.61706433e-03,  2.70682853e-03,  1.29880744e-03],\n",
       "         [-3.63491056e-03,  3.47415917e-04,  4.55312058e-03, ...,\n",
       "          -3.43159656e-03,  2.44253618e-03,  1.14772178e-03],\n",
       "         [-3.31079122e-03, -1.65023448e-04,  4.36979858e-03, ...,\n",
       "          -3.49940057e-03,  1.39241526e-03,  9.96943214e-04],\n",
       "         [-3.17508285e-03,  4.01522324e-04,  4.27894713e-03, ...,\n",
       "          -2.75990670e-03,  3.41683999e-03,  1.07944640e-03],\n",
       "         [-3.16876872e-03,  1.67810096e-04,  4.26893681e-03, ...,\n",
       "          -2.79415771e-03,  1.95909850e-03,  1.24492776e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.39920658e-03,  9.51419643e-04,  9.71058756e-03, ...,\n",
       "          -4.06501861e-03,  4.23065946e-03, -5.37102344e-04],\n",
       "         [-1.18634617e-03,  1.25669315e-03,  9.89979412e-03, ...,\n",
       "          -3.88895883e-03,  4.37367149e-03, -1.84446923e-03],\n",
       "         [-9.66031686e-04,  1.49601081e-03,  9.65359434e-03, ...,\n",
       "          -3.83265479e-03,  4.29959968e-03, -1.79018045e-03],\n",
       "         [-1.18634617e-03,  1.25669315e-03,  9.89979412e-03, ...,\n",
       "          -3.88895883e-03,  4.37367149e-03, -1.84446923e-03],\n",
       "         [-1.75890478e-03,  8.98699393e-04,  9.28778946e-03, ...,\n",
       "          -3.51628847e-03,  4.22970951e-03, -1.74834114e-03]],\n",
       "\n",
       "        [[-8.88834242e-04,  7.83797470e-04,  9.95373540e-03, ...,\n",
       "          -3.66622536e-03,  4.31617443e-03,  2.89926946e-04],\n",
       "         [-6.21137791e-04,  9.07973328e-04,  1.02389203e-02, ...,\n",
       "          -3.43051739e-03,  4.45276219e-03, -1.19507872e-03],\n",
       "         [-8.88834242e-04,  7.83797470e-04,  9.95373540e-03, ...,\n",
       "          -3.66622536e-03,  4.31617443e-03,  2.89926946e-04],\n",
       "         [-8.88834242e-04,  7.83797470e-04,  9.95373540e-03, ...,\n",
       "          -3.66622536e-03,  4.31617443e-03,  2.89926946e-04],\n",
       "         [-8.88834242e-04,  7.83797470e-04,  9.95373540e-03, ...,\n",
       "          -3.66622536e-03,  4.31617443e-03,  2.89926946e-04]],\n",
       "\n",
       "        [[-4.42548131e-04,  8.12726095e-04,  1.00032045e-02, ...,\n",
       "          -3.30487825e-03,  4.36068792e-03,  1.23015745e-03],\n",
       "         [-1.44672260e-04,  7.10904656e-04,  1.03663662e-02, ...,\n",
       "          -3.06806527e-03,  4.45729773e-03, -2.21709910e-04],\n",
       "         [ 9.67285305e-05,  8.86235270e-04,  1.00524407e-02, ...,\n",
       "          -2.99143558e-03,  4.38688090e-03,  5.21069451e-05],\n",
       "         [-1.69629842e-04,  9.86817293e-04,  1.00836102e-02, ...,\n",
       "          -2.92410329e-03,  4.23158985e-03, -5.19274035e-04],\n",
       "         [-8.29334429e-04,  7.51971384e-05,  9.52042919e-03, ...,\n",
       "          -2.83341017e-03,  4.16827993e-03,  5.82450739e-05]]]],\n",
       "      dtype=float32), ids=array([[[ 484,   62,  484,   62,   62],\n",
       "        [  62,   62,   62,  484,  822],\n",
       "        [ 822,  665,  822,  822,  665],\n",
       "        [ 665,  822,  822,  665,  665],\n",
       "        [ 665,  863,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  863],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  199,  665,  199,  665],\n",
       "        [ 199,  199,  315,  315,  665]],\n",
       "\n",
       "       [[ 243,  754,  754,  754,  386],\n",
       "        [ 243,  243,  243,  529,  243],\n",
       "        [ 243,  243,  243,  243,  243],\n",
       "        [ 243,  243,  243,  243,  243],\n",
       "        [ 243,  243,  243,  243,  921],\n",
       "        [ 243,  243,  921,  548,  243],\n",
       "        [ 921,  548,  548,  658,  548],\n",
       "        [ 658,  548,  548,  548,  921],\n",
       "        [ 658,  862,  862,  548,  862],\n",
       "        [ 658,  862,  862,  658,  862],\n",
       "        [ 658,  862,  862, 1000,  862]],\n",
       "\n",
       "       [[ 416,  416,  630,  530,  824],\n",
       "        [ 416,  416,  416,  416,  416],\n",
       "        [ 416,  776,  413,  416,  416],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 213,  776,  505,  776,  776],\n",
       "        [ 213,  213,  213,  213,  213],\n",
       "        [ 213,  213,  317,  213,  213],\n",
       "        [ 213,  213,  213,  213,  213],\n",
       "        [ 213,  213,  701,  317,    2]]], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 484,   62,  484,   62,   62],\n",
       "        [  62,   62,   62,  484,  822],\n",
       "        [ 822,  665,  822,  822,  665],\n",
       "        [ 665,  822,  822,  665,  665],\n",
       "        [ 665,  863,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  863],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  665,  665,  665,  665],\n",
       "        [ 665,  199,  665,  199,  665],\n",
       "        [ 199,  199,  315,  315,  665]],\n",
       "\n",
       "       [[ 243,  754,  754,  754,  386],\n",
       "        [ 243,  243,  243,  529,  243],\n",
       "        [ 243,  243,  243,  243,  243],\n",
       "        [ 243,  243,  243,  243,  243],\n",
       "        [ 243,  243,  243,  243,  921],\n",
       "        [ 243,  243,  921,  548,  243],\n",
       "        [ 921,  548,  548,  658,  548],\n",
       "        [ 658,  548,  548,  548,  921],\n",
       "        [ 658,  862,  862,  548,  862],\n",
       "        [ 658,  862,  862,  658,  862],\n",
       "        [ 658,  862,  862, 1000,  862]],\n",
       "\n",
       "       [[ 416,  416,  630,  530,  824],\n",
       "        [ 416,  416,  416,  416,  416],\n",
       "        [ 416,  776,  413,  416,  416],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 776,  776,  776,  776,  776],\n",
       "        [ 213,  776,  505,  776,  776],\n",
       "        [ 213,  213,  213,  213,  213],\n",
       "        [ 213,  213,  317,  213,  213],\n",
       "        [ 213,  213,  213,  213,  213],\n",
       "        [ 213,  213,  701,  317,    2]]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": ECMWrapper,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaronlai/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.911455, perp: 998.888, dev_prep: 998.888, (0.394 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 5.842235, perp: 360.421, dev_prep: 343.873, (0.073 sec/step)\n",
      "step 40, loss = 5.459324, perp: 236.472, dev_prep: 296.502, (0.072 sec/step)\n",
      "step 60, loss = 5.585471, perp: 262.496, dev_prep: 237.419, (0.077 sec/step)\n",
      "step 80, loss = 5.329376, perp: 204.037, dev_prep: 200.045, (0.073 sec/step)\n",
      "step 100, loss = 4.992484, perp: 145.444, dev_prep: 171.572, (0.073 sec/step)\n",
      "step 120, loss = 5.043649, perp: 157.299, dev_prep: 153.990, (0.073 sec/step)\n",
      "step 140, loss = 4.923616, perp: 134.702, dev_prep: 142.021, (0.073 sec/step)\n",
      "step 160, loss = 4.880643, perp: 128.951, dev_prep: 135.001, (0.071 sec/step)\n",
      "step 180, loss = 4.889031, perp: 130.653, dev_prep: 129.235, (0.073 sec/step)\n",
      "step 200, loss = 4.912619, perp: 135.597, dev_prep: 124.722, (0.072 sec/step)\n",
      "step 220, loss = 4.621561, perp: 100.424, dev_prep: 121.560, (0.072 sec/step)\n",
      "step 240, loss = 4.773728, perp: 116.514, dev_prep: 120.704, (0.071 sec/step)\n",
      "step 260, loss = 4.693574, perp: 108.365, dev_prep: 119.511, (0.074 sec/step)\n",
      "step 280, loss = 4.716786, perp: 110.637, dev_prep: 119.072, (0.072 sec/step)\n",
      "step 300, loss = 4.737536, perp: 112.400, dev_prep: 117.955, (0.072 sec/step)\n",
      "step 320, loss = 4.766743, perp: 116.822, dev_prep: 116.890, (0.073 sec/step)\n",
      "step 340, loss = 4.935670, perp: 136.036, dev_prep: 114.410, (0.073 sec/step)\n",
      "step 360, loss = 4.688381, perp: 108.075, dev_prep: 113.590, (0.073 sec/step)\n",
      "step 380, loss = 4.619586, perp: 100.402, dev_prep: 112.260, (0.073 sec/step)\n",
      "step 400, loss = 4.738293, perp: 113.093, dev_prep: 111.158, (0.072 sec/step)\n",
      "step 420, loss = 4.781421, perp: 118.289, dev_prep: 109.395, (0.073 sec/step)\n",
      "step 440, loss = 4.748865, perp: 115.024, dev_prep: 105.633, (0.071 sec/step)\n",
      "step 460, loss = 4.685373, perp: 109.762, dev_prep: 107.645, (0.070 sec/step)\n",
      "step 480, loss = 4.639787, perp: 102.620, dev_prep: 98.447, (0.073 sec/step)\n",
      "step 500, loss = 4.528543, perp: 91.545, dev_prep: 92.647, (0.071 sec/step)\n",
      "step 520, loss = 4.676463, perp: 106.162, dev_prep: 89.540, (0.073 sec/step)\n",
      "step 540, loss = 4.373280, perp: 78.473, dev_prep: 90.177, (0.075 sec/step)\n",
      "step 560, loss = 4.370356, perp: 78.187, dev_prep: 83.754, (0.074 sec/step)\n",
      "step 580, loss = 4.322953, perp: 74.762, dev_prep: 81.241, (0.073 sec/step)\n",
      "step 600, loss = 4.406273, perp: 79.144, dev_prep: 78.188, (0.073 sec/step)\n",
      "step 620, loss = 4.394168, perp: 78.916, dev_prep: 75.633, (0.070 sec/step)\n",
      "step 640, loss = 4.248592, perp: 69.201, dev_prep: 73.167, (0.072 sec/step)\n",
      "step 660, loss = 4.329499, perp: 73.677, dev_prep: 71.223, (0.073 sec/step)\n",
      "step 680, loss = 4.262555, perp: 69.221, dev_prep: 69.796, (0.071 sec/step)\n",
      "step 700, loss = 4.132786, perp: 61.780, dev_prep: 67.526, (0.073 sec/step)\n",
      "step 720, loss = 4.167211, perp: 63.952, dev_prep: 66.667, (0.072 sec/step)\n",
      "step 740, loss = 4.079998, perp: 58.536, dev_prep: 65.372, (0.072 sec/step)\n",
      "step 760, loss = 4.152519, perp: 63.193, dev_prep: 64.487, (0.074 sec/step)\n",
      "step 780, loss = 3.977839, perp: 52.606, dev_prep: 62.167, (0.072 sec/step)\n",
      "step 800, loss = 4.128759, perp: 61.176, dev_prep: 60.733, (0.071 sec/step)\n",
      "step 820, loss = 3.958396, perp: 51.656, dev_prep: 59.822, (0.070 sec/step)\n",
      "step 840, loss = 4.128567, perp: 60.996, dev_prep: 59.398, (0.074 sec/step)\n",
      "step 860, loss = 4.069943, perp: 57.470, dev_prep: 57.795, (0.072 sec/step)\n",
      "step 880, loss = 4.150831, perp: 62.435, dev_prep: 57.507, (0.072 sec/step)\n",
      "step 900, loss = 4.058382, perp: 56.904, dev_prep: 57.718, (0.072 sec/step)\n",
      "step 920, loss = 4.085787, perp: 58.657, dev_prep: 55.765, (0.071 sec/step)\n",
      "step 940, loss = 3.871888, perp: 47.594, dev_prep: 55.401, (0.075 sec/step)\n",
      "step 960, loss = 4.075734, perp: 57.533, dev_prep: 54.429, (0.074 sec/step)\n",
      "step 980, loss = 3.926368, perp: 49.464, dev_prep: 53.500, (0.074 sec/step)\n",
      "step 1000, loss = 4.140486, perp: 61.139, dev_prep: 53.616, (0.073 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 3.964198, perp: 51.550, dev_prep: 54.344, (0.073 sec/step)\n",
      "step 1040, loss = 3.950702, perp: 50.551, dev_prep: 53.148, (0.076 sec/step)\n",
      "step 1060, loss = 3.904202, perp: 49.089, dev_prep: 52.711, (0.070 sec/step)\n",
      "step 1080, loss = 3.963312, perp: 51.846, dev_prep: 52.917, (0.072 sec/step)\n",
      "step 1100, loss = 3.913779, perp: 50.183, dev_prep: 52.009, (0.073 sec/step)\n",
      "step 1120, loss = 3.868443, perp: 47.826, dev_prep: 51.823, (0.074 sec/step)\n",
      "step 1140, loss = 3.812042, perp: 44.509, dev_prep: 49.931, (0.073 sec/step)\n",
      "step 1160, loss = 3.965295, perp: 54.674, dev_prep: 53.527, (0.070 sec/step)\n",
      "step 1180, loss = 4.091635, perp: 57.668, dev_prep: 50.237, (0.071 sec/step)\n",
      "step 1200, loss = 4.004760, perp: 52.714, dev_prep: 49.684, (0.073 sec/step)\n",
      "step 1220, loss = 3.835959, perp: 45.499, dev_prep: 48.942, (0.072 sec/step)\n",
      "step 1240, loss = 3.896246, perp: 47.907, dev_prep: 48.553, (0.073 sec/step)\n",
      "step 1260, loss = 3.917491, perp: 47.727, dev_prep: 49.465, (0.070 sec/step)\n",
      "step 1280, loss = 4.060893, perp: 56.674, dev_prep: 48.568, (0.074 sec/step)\n",
      "step 1300, loss = 3.925179, perp: 49.561, dev_prep: 48.332, (0.072 sec/step)\n",
      "step 1320, loss = 3.846327, perp: 47.488, dev_prep: 48.556, (0.074 sec/step)\n",
      "step 1340, loss = 3.900655, perp: 47.803, dev_prep: 47.423, (0.073 sec/step)\n",
      "step 1360, loss = 3.919301, perp: 46.958, dev_prep: 47.758, (0.074 sec/step)\n",
      "step 1380, loss = 3.835781, perp: 45.012, dev_prep: 47.434, (0.074 sec/step)\n",
      "step 1400, loss = 3.741343, perp: 41.657, dev_prep: 47.066, (0.077 sec/step)\n",
      "step 1420, loss = 3.771569, perp: 42.954, dev_prep: 46.298, (0.073 sec/step)\n",
      "step 1440, loss = 3.762172, perp: 41.386, dev_prep: 45.185, (0.073 sec/step)\n",
      "step 1460, loss = 3.960374, perp: 50.086, dev_prep: 45.207, (0.072 sec/step)\n",
      "step 1480, loss = 3.768762, perp: 42.124, dev_prep: 44.875, (0.071 sec/step)\n",
      "step 1500, loss = 3.684411, perp: 39.160, dev_prep: 44.653, (0.075 sec/step)\n",
      "step 1520, loss = 3.822687, perp: 44.416, dev_prep: 44.990, (0.072 sec/step)\n",
      "step 1540, loss = 3.861987, perp: 45.836, dev_prep: 44.510, (0.075 sec/step)\n",
      "step 1560, loss = 3.734656, perp: 40.389, dev_prep: 43.849, (0.073 sec/step)\n",
      "step 1580, loss = 3.827946, perp: 44.242, dev_prep: 43.056, (0.071 sec/step)\n",
      "step 1600, loss = 3.697245, perp: 39.620, dev_prep: 43.134, (0.074 sec/step)\n",
      "step 1620, loss = 3.757723, perp: 41.953, dev_prep: 42.751, (0.073 sec/step)\n",
      "step 1640, loss = 3.708242, perp: 40.212, dev_prep: 42.533, (0.074 sec/step)\n",
      "step 1660, loss = 3.638291, perp: 37.264, dev_prep: 42.212, (0.072 sec/step)\n",
      "step 1680, loss = 3.777094, perp: 43.272, dev_prep: 42.393, (0.073 sec/step)\n",
      "step 1700, loss = 3.734870, perp: 41.388, dev_prep: 42.182, (0.072 sec/step)\n",
      "step 1720, loss = 3.735975, perp: 40.636, dev_prep: 41.211, (0.074 sec/step)\n",
      "step 1740, loss = 3.657414, perp: 37.958, dev_prep: 41.570, (0.073 sec/step)\n",
      "step 1760, loss = 3.711644, perp: 41.443, dev_prep: 42.731, (0.074 sec/step)\n",
      "step 1780, loss = 3.682339, perp: 38.908, dev_prep: 41.634, (0.074 sec/step)\n",
      "step 1800, loss = 3.578124, perp: 35.659, dev_prep: 42.174, (0.072 sec/step)\n",
      "step 1820, loss = 3.517819, perp: 33.359, dev_prep: 40.985, (0.071 sec/step)\n",
      "step 1840, loss = 3.620744, perp: 36.831, dev_prep: 41.104, (0.074 sec/step)\n",
      "step 1860, loss = 3.692524, perp: 39.363, dev_prep: 40.253, (0.072 sec/step)\n",
      "step 1880, loss = 3.777196, perp: 42.640, dev_prep: 40.048, (0.072 sec/step)\n",
      "step 1900, loss = 3.772240, perp: 42.800, dev_prep: 40.403, (0.073 sec/step)\n",
      "step 1920, loss = 3.617123, perp: 36.380, dev_prep: 39.970, (0.072 sec/step)\n",
      "step 1940, loss = 3.592089, perp: 35.798, dev_prep: 39.608, (0.070 sec/step)\n",
      "step 1960, loss = 3.625266, perp: 37.001, dev_prep: 39.285, (0.070 sec/step)\n",
      "step 1980, loss = 3.649828, perp: 37.750, dev_prep: 39.097, (0.073 sec/step)\n",
      "step 2000, loss = 3.663561, perp: 37.850, dev_prep: 38.668, (0.071 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.662915, perp: 38.874, dev_prep: 38.965, (0.073 sec/step)\n",
      "step 2040, loss = 3.529201, perp: 33.553, dev_prep: 38.122, (0.073 sec/step)\n",
      "step 2060, loss = 3.624833, perp: 36.687, dev_prep: 37.693, (0.073 sec/step)\n",
      "step 2080, loss = 3.542186, perp: 33.759, dev_prep: 37.237, (0.073 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.625997, perp: 37.470, dev_prep: 38.270, (0.073 sec/step)\n",
      "step 2120, loss = 3.640334, perp: 37.649, dev_prep: 37.933, (0.073 sec/step)\n",
      "step 2140, loss = 3.605115, perp: 35.694, dev_prep: 37.433, (0.077 sec/step)\n",
      "step 2160, loss = 3.586976, perp: 36.043, dev_prep: 38.279, (0.073 sec/step)\n",
      "step 2180, loss = 3.527465, perp: 33.556, dev_prep: 37.591, (0.070 sec/step)\n",
      "step 2200, loss = 3.528904, perp: 34.222, dev_prep: 37.857, (0.073 sec/step)\n",
      "step 2220, loss = 3.584416, perp: 35.393, dev_prep: 36.676, (0.072 sec/step)\n",
      "step 2240, loss = 3.629762, perp: 37.540, dev_prep: 36.998, (0.072 sec/step)\n",
      "step 2260, loss = 3.420391, perp: 30.201, dev_prep: 36.843, (0.073 sec/step)\n",
      "step 2280, loss = 3.565075, perp: 32.953, dev_prep: 37.223, (0.072 sec/step)\n",
      "step 2300, loss = 3.595614, perp: 35.555, dev_prep: 37.220, (0.073 sec/step)\n",
      "step 2320, loss = 3.565652, perp: 34.673, dev_prep: 36.141, (0.074 sec/step)\n",
      "step 2340, loss = 3.560869, perp: 34.819, dev_prep: 36.365, (0.073 sec/step)\n",
      "step 2360, loss = 3.455489, perp: 31.451, dev_prep: 36.531, (0.074 sec/step)\n",
      "step 2380, loss = 3.596269, perp: 36.358, dev_prep: 36.225, (0.075 sec/step)\n",
      "step 2400, loss = 3.479460, perp: 32.470, dev_prep: 36.375, (0.074 sec/step)\n",
      "step 2420, loss = 3.496855, perp: 33.272, dev_prep: 37.134, (0.074 sec/step)\n",
      "step 2440, loss = 3.515128, perp: 33.145, dev_prep: 36.060, (0.073 sec/step)\n",
      "step 2460, loss = 3.662940, perp: 38.115, dev_prep: 35.342, (0.076 sec/step)\n",
      "step 2480, loss = 3.569428, perp: 34.536, dev_prep: 35.189, (0.073 sec/step)\n",
      "step 2500, loss = 3.589312, perp: 35.394, dev_prep: 35.045, (0.074 sec/step)\n",
      "step 2520, loss = 3.590199, perp: 35.608, dev_prep: 35.182, (0.073 sec/step)\n",
      "step 2540, loss = 3.429468, perp: 30.478, dev_prep: 35.444, (0.078 sec/step)\n",
      "step 2560, loss = 3.555190, perp: 34.409, dev_prep: 35.399, (0.079 sec/step)\n",
      "step 2580, loss = 3.472545, perp: 32.629, dev_prep: 38.131, (0.075 sec/step)\n",
      "step 2600, loss = 3.544688, perp: 34.326, dev_prep: 35.695, (0.071 sec/step)\n",
      "step 2620, loss = 3.582973, perp: 34.669, dev_prep: 34.809, (0.072 sec/step)\n",
      "step 2640, loss = 3.577338, perp: 34.608, dev_prep: 34.826, (0.074 sec/step)\n",
      "step 2660, loss = 3.602307, perp: 36.339, dev_prep: 34.352, (0.076 sec/step)\n",
      "step 2680, loss = 3.330839, perp: 27.250, dev_prep: 34.561, (0.075 sec/step)\n",
      "step 2700, loss = 3.480826, perp: 32.568, dev_prep: 35.596, (0.075 sec/step)\n",
      "step 2720, loss = 3.532105, perp: 34.246, dev_prep: 34.813, (0.077 sec/step)\n",
      "step 2740, loss = 3.470552, perp: 31.485, dev_prep: 34.054, (0.075 sec/step)\n",
      "step 2760, loss = 3.540264, perp: 34.369, dev_prep: 34.321, (0.073 sec/step)\n",
      "step 2780, loss = 3.518992, perp: 33.127, dev_prep: 33.581, (0.072 sec/step)\n",
      "step 2800, loss = 3.465584, perp: 31.377, dev_prep: 33.130, (0.072 sec/step)\n",
      "step 2820, loss = 3.702649, perp: 40.196, dev_prep: 34.199, (0.074 sec/step)\n",
      "step 2840, loss = 3.484599, perp: 32.989, dev_prep: 34.166, (0.073 sec/step)\n",
      "step 2860, loss = 3.476726, perp: 31.875, dev_prep: 33.333, (0.071 sec/step)\n",
      "step 2880, loss = 3.545722, perp: 32.553, dev_prep: 33.322, (0.074 sec/step)\n",
      "step 2900, loss = 3.540803, perp: 34.133, dev_prep: 33.409, (0.074 sec/step)\n",
      "step 2920, loss = 3.533776, perp: 32.469, dev_prep: 33.450, (0.076 sec/step)\n",
      "step 2940, loss = 3.433843, perp: 30.187, dev_prep: 33.101, (0.074 sec/step)\n",
      "step 2960, loss = 3.452429, perp: 30.972, dev_prep: 32.740, (0.076 sec/step)\n",
      "step 2980, loss = 3.443994, perp: 31.175, dev_prep: 32.872, (0.074 sec/step)\n",
      "step 3000, loss = 3.474968, perp: 31.908, dev_prep: 32.367, (0.074 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.485976, perp: 31.775, dev_prep: 32.228, (0.073 sec/step)\n",
      "step 3040, loss = 3.493380, perp: 31.948, dev_prep: 32.328, (0.071 sec/step)\n",
      "step 3060, loss = 3.514983, perp: 33.040, dev_prep: 33.086, (0.073 sec/step)\n",
      "step 3080, loss = 3.330142, perp: 27.748, dev_prep: 32.617, (0.072 sec/step)\n",
      "step 3100, loss = 3.415233, perp: 29.853, dev_prep: 32.072, (0.071 sec/step)\n",
      "step 3120, loss = 3.389349, perp: 28.874, dev_prep: 31.634, (0.072 sec/step)\n",
      "step 3140, loss = 3.431773, perp: 30.122, dev_prep: 31.606, (0.083 sec/step)\n",
      "step 3160, loss = 3.434396, perp: 30.608, dev_prep: 31.741, (0.074 sec/step)\n",
      "step 3180, loss = 3.397025, perp: 29.328, dev_prep: 31.625, (0.073 sec/step)\n",
      "step 3200, loss = 3.460139, perp: 31.309, dev_prep: 31.682, (0.073 sec/step)\n",
      "step 3220, loss = 3.424029, perp: 30.595, dev_prep: 32.776, (0.072 sec/step)\n",
      "step 3240, loss = 3.474350, perp: 32.189, dev_prep: 32.089, (0.074 sec/step)\n",
      "step 3260, loss = 3.459084, perp: 30.949, dev_prep: 31.396, (0.073 sec/step)\n",
      "step 3280, loss = 3.445920, perp: 31.012, dev_prep: 31.595, (0.079 sec/step)\n",
      "step 3300, loss = 3.381486, perp: 29.047, dev_prep: 31.392, (0.073 sec/step)\n",
      "step 3320, loss = 3.473021, perp: 31.499, dev_prep: 31.144, (0.074 sec/step)\n",
      "step 3340, loss = 3.405506, perp: 29.245, dev_prep: 31.717, (0.075 sec/step)\n",
      "step 3360, loss = 3.442613, perp: 31.044, dev_prep: 31.595, (0.074 sec/step)\n",
      "step 3380, loss = 3.533200, perp: 32.871, dev_prep: 30.998, (0.074 sec/step)\n",
      "step 3400, loss = 3.380812, perp: 28.550, dev_prep: 31.130, (0.073 sec/step)\n",
      "step 3420, loss = 3.458827, perp: 30.862, dev_prep: 30.616, (0.074 sec/step)\n",
      "step 3440, loss = 3.454988, perp: 30.714, dev_prep: 30.112, (0.072 sec/step)\n",
      "step 3460, loss = 3.402137, perp: 29.969, dev_prep: 31.927, (0.073 sec/step)\n",
      "step 3480, loss = 3.367182, perp: 28.044, dev_prep: 30.606, (0.073 sec/step)\n",
      "step 3500, loss = 3.263171, perp: 25.784, dev_prep: 29.970, (0.074 sec/step)\n",
      "step 3520, loss = 3.388430, perp: 29.123, dev_prep: 29.372, (0.075 sec/step)\n",
      "step 3540, loss = 3.318565, perp: 26.819, dev_prep: 29.370, (0.072 sec/step)\n",
      "step 3560, loss = 3.243372, perp: 25.218, dev_prep: 29.038, (0.076 sec/step)\n",
      "step 3580, loss = 3.383404, perp: 29.477, dev_prep: 29.687, (0.074 sec/step)\n",
      "step 3600, loss = 3.372260, perp: 29.211, dev_prep: 29.640, (0.072 sec/step)\n",
      "step 3620, loss = 3.431802, perp: 30.073, dev_prep: 28.975, (0.073 sec/step)\n",
      "step 3640, loss = 3.336328, perp: 27.898, dev_prep: 29.377, (0.072 sec/step)\n",
      "step 3660, loss = 3.383032, perp: 29.774, dev_prep: 30.519, (0.074 sec/step)\n",
      "step 3680, loss = 3.304842, perp: 26.749, dev_prep: 29.190, (0.076 sec/step)\n",
      "step 3700, loss = 3.293355, perp: 26.240, dev_prep: 28.417, (0.073 sec/step)\n",
      "step 3720, loss = 3.287524, perp: 26.381, dev_prep: 27.988, (0.073 sec/step)\n",
      "step 3740, loss = 3.356428, perp: 28.375, dev_prep: 28.928, (0.073 sec/step)\n",
      "step 3760, loss = 3.357142, perp: 28.418, dev_prep: 28.628, (0.071 sec/step)\n",
      "step 3780, loss = 3.269758, perp: 25.612, dev_prep: 27.718, (0.072 sec/step)\n",
      "step 3800, loss = 3.280282, perp: 26.225, dev_prep: 27.727, (0.075 sec/step)\n",
      "step 3820, loss = 3.324330, perp: 27.133, dev_prep: 27.612, (0.074 sec/step)\n",
      "step 3840, loss = 3.350800, perp: 27.892, dev_prep: 27.988, (0.070 sec/step)\n",
      "step 3860, loss = 3.222234, perp: 25.024, dev_prep: 28.691, (0.075 sec/step)\n",
      "step 3880, loss = 3.187158, perp: 23.764, dev_prep: 27.978, (0.071 sec/step)\n",
      "step 3900, loss = 3.241139, perp: 25.252, dev_prep: 27.538, (0.078 sec/step)\n",
      "step 3920, loss = 3.137321, perp: 22.296, dev_prep: 27.035, (0.070 sec/step)\n",
      "step 3940, loss = 3.250613, perp: 25.246, dev_prep: 27.065, (0.071 sec/step)\n",
      "step 3960, loss = 3.286576, perp: 26.064, dev_prep: 27.432, (0.074 sec/step)\n",
      "step 3980, loss = 3.218589, perp: 24.358, dev_prep: 26.540, (0.074 sec/step)\n",
      "step 4000, loss = 3.237284, perp: 24.871, dev_prep: 26.540, (0.074 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.145424, perp: 22.445, dev_prep: 26.082, (0.074 sec/step)\n",
      "step 4040, loss = 3.269764, perp: 25.534, dev_prep: 26.159, (0.073 sec/step)\n",
      "step 4060, loss = 3.249815, perp: 25.502, dev_prep: 26.711, (0.071 sec/step)\n",
      "step 4080, loss = 3.237388, perp: 24.875, dev_prep: 26.394, (0.074 sec/step)\n",
      "step 4100, loss = 3.243320, perp: 25.008, dev_prep: 26.210, (0.073 sec/step)\n",
      "step 4120, loss = 3.253951, perp: 25.154, dev_prep: 26.213, (0.072 sec/step)\n",
      "step 4140, loss = 3.265676, perp: 25.238, dev_prep: 25.641, (0.073 sec/step)\n",
      "step 4160, loss = 3.197001, perp: 23.842, dev_prep: 25.474, (0.074 sec/step)\n",
      "step 4180, loss = 3.152287, perp: 22.739, dev_prep: 25.300, (0.071 sec/step)\n",
      "step 4200, loss = 3.249874, perp: 25.362, dev_prep: 25.899, (0.083 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.187880, perp: 23.605, dev_prep: 25.005, (0.074 sec/step)\n",
      "step 4240, loss = 3.159776, perp: 22.743, dev_prep: 25.021, (0.073 sec/step)\n",
      "step 4260, loss = 3.300063, perp: 26.384, dev_prep: 24.852, (0.072 sec/step)\n",
      "step 4280, loss = 3.053997, perp: 20.420, dev_prep: 24.728, (0.073 sec/step)\n",
      "step 4300, loss = 3.288200, perp: 25.899, dev_prep: 24.661, (0.074 sec/step)\n",
      "step 4320, loss = 3.147234, perp: 22.945, dev_prep: 24.373, (0.070 sec/step)\n",
      "step 4340, loss = 3.134298, perp: 22.146, dev_prep: 24.194, (0.072 sec/step)\n",
      "step 4360, loss = 3.202343, perp: 24.246, dev_prep: 24.607, (0.072 sec/step)\n",
      "step 4380, loss = 3.193156, perp: 23.863, dev_prep: 24.418, (0.072 sec/step)\n",
      "step 4400, loss = 3.188413, perp: 23.305, dev_prep: 24.107, (0.072 sec/step)\n",
      "step 4420, loss = 3.199206, perp: 23.917, dev_prep: 23.662, (0.073 sec/step)\n",
      "step 4440, loss = 3.058904, perp: 20.915, dev_prep: 23.974, (0.074 sec/step)\n",
      "step 4460, loss = 3.199552, perp: 23.729, dev_prep: 23.601, (0.073 sec/step)\n",
      "step 4480, loss = 3.028325, perp: 20.312, dev_prep: 23.345, (0.080 sec/step)\n",
      "step 4500, loss = 3.067962, perp: 20.889, dev_prep: 23.205, (0.075 sec/step)\n",
      "step 4520, loss = 3.126660, perp: 22.614, dev_prep: 23.698, (0.074 sec/step)\n",
      "step 4540, loss = 3.093460, perp: 21.366, dev_prep: 23.184, (0.078 sec/step)\n",
      "step 4560, loss = 3.104964, perp: 21.207, dev_prep: 23.168, (0.073 sec/step)\n",
      "step 4580, loss = 3.087247, perp: 21.478, dev_prep: 22.935, (0.072 sec/step)\n",
      "step 4600, loss = 3.077281, perp: 21.421, dev_prep: 22.911, (0.072 sec/step)\n",
      "step 4620, loss = 3.127971, perp: 22.524, dev_prep: 22.750, (0.074 sec/step)\n",
      "step 4640, loss = 3.099063, perp: 21.195, dev_prep: 22.102, (0.074 sec/step)\n",
      "step 4660, loss = 3.111204, perp: 21.886, dev_prep: 22.954, (0.071 sec/step)\n",
      "step 4680, loss = 2.984944, perp: 19.403, dev_prep: 22.587, (0.073 sec/step)\n",
      "step 4700, loss = 3.120661, perp: 21.618, dev_prep: 22.326, (0.071 sec/step)\n",
      "step 4720, loss = 3.055135, perp: 19.525, dev_prep: 21.872, (0.073 sec/step)\n",
      "step 4740, loss = 3.102162, perp: 21.632, dev_prep: 21.924, (0.075 sec/step)\n",
      "step 4760, loss = 3.039466, perp: 20.424, dev_prep: 21.455, (0.069 sec/step)\n",
      "step 4780, loss = 2.954131, perp: 18.639, dev_prep: 21.301, (0.070 sec/step)\n",
      "step 4800, loss = 3.012654, perp: 19.844, dev_prep: 21.150, (0.073 sec/step)\n",
      "step 4820, loss = 2.952439, perp: 18.735, dev_prep: 20.941, (0.073 sec/step)\n",
      "step 4840, loss = 3.082402, perp: 21.459, dev_prep: 20.705, (0.072 sec/step)\n",
      "step 4860, loss = 2.935097, perp: 18.661, dev_prep: 20.623, (0.071 sec/step)\n",
      "step 4880, loss = 2.857548, perp: 17.222, dev_prep: 20.359, (0.075 sec/step)\n",
      "step 4900, loss = 2.908347, perp: 17.727, dev_prep: 20.266, (0.072 sec/step)\n",
      "step 4920, loss = 2.937644, perp: 18.699, dev_prep: 20.359, (0.072 sec/step)\n",
      "step 4940, loss = 3.027951, perp: 20.249, dev_prep: 19.922, (0.072 sec/step)\n",
      "step 4960, loss = 2.905514, perp: 18.001, dev_prep: 19.804, (0.072 sec/step)\n",
      "step 4980, loss = 3.025421, perp: 20.028, dev_prep: 20.476, (0.070 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data,\n",
    "                    target_ids: dev_target_data,\n",
    "                    sequence_mask: dev_masks,\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks, dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "# plt.savefig(loss_fig)\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps, perps, label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps, dev_perps, label=\"dev\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(perp_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.19838195578249"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(1000, infer_batch_size)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, dev_masks, dev_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
