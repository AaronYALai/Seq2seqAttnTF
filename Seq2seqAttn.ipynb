{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        # transpose to [batch_size, num_units, max_time]\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        \"\"\"\n",
    "        if self._dec_init_states is None:\n",
    "            attn_state_0 = None\n",
    "\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "            h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "            context_0 = self._compute_context(h_0)\n",
    "            h_0 = context_0 * 0\n",
    "            attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECMWrapper(RNNCell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, time_major=False, dtype=tf.float32,\n",
    "                  forget_bias=1.0, name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        mode: train or infer\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            # need batch-major\n",
    "            if time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=False,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099819, 0.0009963 , 0.00099533, ..., 0.00099829,\n",
       "         0.00099767, 0.00099576],\n",
       "        [0.0009988 , 0.00099675, 0.00099481, ..., 0.00099844,\n",
       "         0.00099726, 0.00099524],\n",
       "        [0.00099903, 0.00099766, 0.00099493, ..., 0.00099795,\n",
       "         0.00099635, 0.00099524],\n",
       "        [0.00099902, 0.00099867, 0.00099535, ..., 0.00099716,\n",
       "         0.00099525, 0.00099555]],\n",
       "\n",
       "       [[0.00099819, 0.0009963 , 0.00099533, ..., 0.00099829,\n",
       "         0.00099767, 0.00099576],\n",
       "        [0.00099882, 0.00099672, 0.00099478, ..., 0.00099838,\n",
       "         0.0009972 , 0.00099519],\n",
       "        [0.00099927, 0.00099734, 0.00099428, ..., 0.00099749,\n",
       "         0.00099685, 0.00099499],\n",
       "        [0.00099901, 0.00099811, 0.00099443, ..., 0.00099665,\n",
       "         0.00099732, 0.00099563]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=True, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[ 5.85043430e-03, -1.58674992e-03, -3.98527179e-03, ...,\n",
       "           1.57204398e-03, -1.32556830e-03,  2.29918910e-03],\n",
       "         [ 5.85043430e-03, -1.58674992e-03, -3.98527179e-03, ...,\n",
       "           1.57204398e-03, -1.32556830e-03,  2.29918910e-03],\n",
       "         [ 5.85043430e-03, -1.58674992e-03, -3.98527179e-03, ...,\n",
       "           1.57204398e-03, -1.32556830e-03,  2.29918910e-03],\n",
       "         [ 5.85043430e-03, -1.58674992e-03, -3.98527179e-03, ...,\n",
       "           1.57204398e-03, -1.32556830e-03,  2.29918910e-03],\n",
       "         [ 5.85043430e-03, -1.58674992e-03, -3.98527179e-03, ...,\n",
       "           1.57204398e-03, -1.32556830e-03,  2.29918910e-03]],\n",
       "\n",
       "        [[ 7.02019734e-03, -2.62030936e-03, -5.65548288e-03, ...,\n",
       "          -9.88857239e-04, -2.26824125e-03,  2.93628266e-03],\n",
       "         [ 5.60325198e-03, -1.56731484e-03, -3.61493323e-03, ...,\n",
       "           5.92504453e-04, -1.31786428e-03,  2.33166036e-03],\n",
       "         [ 6.26903214e-03, -2.41258694e-03, -4.16631345e-03, ...,\n",
       "          -1.55825110e-04, -1.62430236e-03,  2.42398120e-03],\n",
       "         [ 5.60325198e-03, -1.56731484e-03, -3.61493323e-03, ...,\n",
       "           5.92504453e-04, -1.31786428e-03,  2.33166036e-03],\n",
       "         [ 7.02019734e-03, -2.62030936e-03, -5.65548288e-03, ...,\n",
       "          -9.88857239e-04, -2.26824125e-03,  2.93628266e-03]],\n",
       "\n",
       "        [[ 7.32166879e-03, -2.22098595e-03, -6.33333158e-03, ...,\n",
       "          -3.24896164e-03, -2.36073975e-03,  3.64591437e-03],\n",
       "         [ 6.58514537e-03, -2.26691412e-03, -5.66377025e-03, ...,\n",
       "          -2.36750953e-03, -2.41336878e-03,  3.72722698e-03],\n",
       "         [ 4.57722135e-03, -1.00000028e-03, -2.65933108e-03, ...,\n",
       "           9.37261793e-05, -1.09686982e-03,  2.63617584e-03],\n",
       "         [ 7.21416855e-03, -2.26605171e-03, -5.83445886e-03, ...,\n",
       "          -2.58422270e-03, -2.19765143e-03,  3.41513869e-03],\n",
       "         [ 5.29982988e-03, -9.46832472e-04, -3.33852042e-03, ...,\n",
       "          -7.86510063e-04, -1.03899580e-03,  2.55844835e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.12658479e-03,  2.25526560e-03, -3.17902653e-03, ...,\n",
       "          -1.08163180e-02, -1.55861757e-03,  4.66383854e-03],\n",
       "         [ 5.39562851e-03,  1.89995638e-03, -2.79528415e-03, ...,\n",
       "          -1.01014469e-02, -2.03178125e-03,  4.31617117e-03],\n",
       "         [ 5.12658479e-03,  2.25526560e-03, -3.17902653e-03, ...,\n",
       "          -1.08163180e-02, -1.55861757e-03,  4.66383854e-03],\n",
       "         [ 4.66738688e-03,  1.74683728e-03, -3.11300694e-03, ...,\n",
       "          -9.95148905e-03, -9.52513190e-04,  4.55900375e-03],\n",
       "         [ 4.66738688e-03,  1.74683728e-03, -3.11300694e-03, ...,\n",
       "          -9.95148905e-03, -9.52513190e-04,  4.55900375e-03]],\n",
       "\n",
       "        [[ 5.55114681e-03,  2.10136268e-03, -2.45764386e-03, ...,\n",
       "          -1.11270230e-02, -2.25364929e-03,  3.94070102e-03],\n",
       "         [ 5.83207421e-03,  1.70691579e-03, -1.80422887e-03, ...,\n",
       "          -9.99074243e-03, -2.89216987e-03,  3.37696681e-03],\n",
       "         [ 5.55114681e-03,  2.10136268e-03, -2.45764386e-03, ...,\n",
       "          -1.11270230e-02, -2.25364929e-03,  3.94070102e-03],\n",
       "         [ 5.83207421e-03,  1.70691579e-03, -1.80422887e-03, ...,\n",
       "          -9.99074243e-03, -2.89216987e-03,  3.37696681e-03],\n",
       "         [ 5.37934341e-03,  1.27606979e-03, -2.46724114e-03, ...,\n",
       "          -1.09472908e-02, -1.73222180e-03,  1.58596947e-03]],\n",
       "\n",
       "        [[ 5.41623076e-03,  1.73174986e-03, -1.56699377e-03, ...,\n",
       "          -1.08130202e-02, -2.54661567e-03,  2.24106875e-03],\n",
       "         [ 5.60446596e-03,  1.42983580e-03, -7.45905796e-04, ...,\n",
       "          -9.47208237e-03, -3.16037377e-03,  1.58690917e-03],\n",
       "         [ 6.02658466e-03,  1.82543858e-03, -1.54004479e-03, ...,\n",
       "          -1.08986525e-02, -3.18124588e-03,  2.92940019e-03],\n",
       "         [ 4.56913374e-03,  1.57872669e-03, -1.76003599e-03, ...,\n",
       "          -1.06201973e-02, -1.59309385e-03,  1.18692359e-03],\n",
       "         [ 5.18002035e-03,  1.67235371e-03, -1.73754897e-03, ...,\n",
       "          -1.07033523e-02, -2.22227257e-03,  1.87659729e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 2.30957405e-03, -1.99808553e-03, -3.30133573e-03, ...,\n",
       "           4.01170488e-04,  1.77540001e-04,  2.23437906e-03],\n",
       "         [ 2.30957405e-03, -1.99808553e-03, -3.30133573e-03, ...,\n",
       "           4.01170488e-04,  1.77540001e-04,  2.23437906e-03],\n",
       "         [ 2.30957405e-03, -1.99808553e-03, -3.30133573e-03, ...,\n",
       "           4.01170488e-04,  1.77540001e-04,  2.23437906e-03],\n",
       "         [ 2.30957405e-03, -1.99808553e-03, -3.30133573e-03, ...,\n",
       "           4.01170488e-04,  1.77540001e-04,  2.23437906e-03],\n",
       "         [ 2.30957405e-03, -1.99808553e-03, -3.30133573e-03, ...,\n",
       "           4.01170488e-04,  1.77540001e-04,  2.23437906e-03]],\n",
       "\n",
       "        [[ 2.32835091e-03, -2.43805209e-03, -4.65385104e-03, ...,\n",
       "          -6.82144251e-04,  7.61365925e-04,  2.26529152e-03],\n",
       "         [ 2.67391023e-03, -1.82589120e-03, -4.93044360e-03, ...,\n",
       "          -7.34615489e-04,  1.18024810e-03,  2.44839070e-03],\n",
       "         [ 2.30286736e-03, -2.18063453e-03, -4.77788178e-03, ...,\n",
       "          -3.46341665e-04,  8.88791052e-04,  1.86305610e-03],\n",
       "         [ 2.67391023e-03, -1.82589120e-03, -4.93044360e-03, ...,\n",
       "          -7.34615489e-04,  1.18024810e-03,  2.44839070e-03],\n",
       "         [ 2.83748843e-03, -2.09996593e-03, -4.79449425e-03, ...,\n",
       "          -7.20561249e-04,  7.65048899e-04,  2.21189833e-03]],\n",
       "\n",
       "        [[ 1.92138483e-03, -2.98147136e-03, -5.32391854e-03, ...,\n",
       "          -1.19176693e-03,  1.13681052e-03,  2.31450330e-03],\n",
       "         [ 2.68531125e-03, -1.41100271e-03, -5.90795511e-03, ...,\n",
       "          -1.20268494e-03,  2.21329555e-03,  2.80471472e-03],\n",
       "         [ 2.68531125e-03, -1.41100271e-03, -5.90795511e-03, ...,\n",
       "          -1.20268494e-03,  2.21329555e-03,  2.80471472e-03],\n",
       "         [ 1.92138483e-03, -2.98147136e-03, -5.32391854e-03, ...,\n",
       "          -1.19176693e-03,  1.13681052e-03,  2.31450330e-03],\n",
       "         [ 2.54728948e-03, -2.36241333e-03, -5.33664925e-03, ...,\n",
       "          -1.81070506e-03,  3.99392447e-04,  2.67078681e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 8.06650845e-04,  1.86324655e-03, -5.89459995e-03, ...,\n",
       "          -4.38568229e-03, -2.75783846e-03,  2.27808300e-03],\n",
       "         [ 8.06650845e-04,  1.86324655e-03, -5.89459995e-03, ...,\n",
       "          -4.38568229e-03, -2.75783846e-03,  2.27808300e-03],\n",
       "         [ 8.06650845e-04,  1.86324655e-03, -5.89459995e-03, ...,\n",
       "          -4.38568229e-03, -2.75783846e-03,  2.27808300e-03],\n",
       "         [ 9.00292653e-04,  1.74770225e-03, -5.14660729e-03, ...,\n",
       "          -4.44337819e-03, -1.78609905e-03,  2.84934766e-03],\n",
       "         [ 8.06650845e-04,  1.86324655e-03, -5.89459995e-03, ...,\n",
       "          -4.38568229e-03, -2.75783846e-03,  2.27808300e-03]],\n",
       "\n",
       "        [[ 1.09534152e-03,  2.92081130e-03, -4.99111973e-03, ...,\n",
       "          -4.72332584e-03, -3.02523142e-03,  2.84159393e-03],\n",
       "         [ 8.27419339e-04,  3.51431151e-03, -5.39551862e-03, ...,\n",
       "          -5.18977176e-03, -3.47166182e-03,  2.45758146e-03],\n",
       "         [ 8.27419339e-04,  3.51431151e-03, -5.39551862e-03, ...,\n",
       "          -5.18977176e-03, -3.47166182e-03,  2.45758146e-03],\n",
       "         [ 5.97026199e-04,  2.90716905e-03, -5.13811130e-03, ...,\n",
       "          -4.14425973e-03, -2.64911912e-03,  2.36790394e-03],\n",
       "         [ 9.21697472e-04,  3.39816161e-03, -4.64812201e-03, ...,\n",
       "          -5.24634589e-03, -2.49778898e-03,  3.02321231e-03]],\n",
       "\n",
       "        [[ 1.13080628e-03,  4.38238028e-03, -4.35124617e-03, ...,\n",
       "          -5.45261987e-03, -3.47345881e-03,  3.08104930e-03],\n",
       "         [ 3.37814447e-04,  3.97727499e-03, -4.55838628e-03, ...,\n",
       "          -5.45549067e-03, -3.61230550e-03,  2.78755324e-03],\n",
       "         [ 1.00773689e-03,  3.79877910e-03, -3.43023008e-03, ...,\n",
       "          -5.21345250e-03, -3.10409768e-03,  2.48338748e-03],\n",
       "         [ 1.38819090e-03,  2.96636578e-03, -2.96495156e-03, ...,\n",
       "          -4.37252782e-03, -2.61085527e-03,  2.99334573e-03],\n",
       "         [ 6.31717965e-04,  4.36640810e-03, -4.49887337e-03, ...,\n",
       "          -4.87168599e-03, -3.10106296e-03,  2.60731694e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 5.45952935e-06, -3.03438841e-03,  4.67727240e-03, ...,\n",
       "          -2.43276963e-03, -1.02097890e-03,  2.93410616e-04],\n",
       "         [ 5.45952935e-06, -3.03438841e-03,  4.67727240e-03, ...,\n",
       "          -2.43276963e-03, -1.02097890e-03,  2.93410616e-04],\n",
       "         [ 5.45952935e-06, -3.03438841e-03,  4.67727240e-03, ...,\n",
       "          -2.43276963e-03, -1.02097890e-03,  2.93410616e-04],\n",
       "         [ 5.45952935e-06, -3.03438841e-03,  4.67727240e-03, ...,\n",
       "          -2.43276963e-03, -1.02097890e-03,  2.93410616e-04],\n",
       "         [ 5.45952935e-06, -3.03438841e-03,  4.67727240e-03, ...,\n",
       "          -2.43276963e-03, -1.02097890e-03,  2.93410616e-04]],\n",
       "\n",
       "        [[ 3.24525172e-06, -3.91828176e-03,  2.41168821e-03, ...,\n",
       "          -2.00669933e-03, -5.18395333e-04,  1.92037434e-04],\n",
       "         [-4.69845254e-05, -4.38295957e-03,  2.29356554e-03, ...,\n",
       "          -1.48503669e-03, -2.57428910e-05, -5.45908697e-05],\n",
       "         [ 3.24525172e-06, -3.91828176e-03,  2.41168821e-03, ...,\n",
       "          -2.00669933e-03, -5.18395333e-04,  1.92037434e-04],\n",
       "         [-6.42651576e-05, -4.13234718e-03,  2.83517176e-03, ...,\n",
       "          -1.08719384e-03, -5.34003018e-04, -2.06241384e-04],\n",
       "         [-6.79871300e-05, -4.52698767e-03,  2.40889168e-03, ...,\n",
       "          -1.08175131e-03,  6.80637429e-04,  6.63446379e-04]],\n",
       "\n",
       "        [[ 6.84044673e-04, -4.33543976e-03,  1.78590871e-03, ...,\n",
       "          -7.74103799e-04, -2.18005356e-04,  3.89796682e-04],\n",
       "         [ 6.84044673e-04, -4.33543976e-03,  1.78590871e-03, ...,\n",
       "          -7.74103799e-04, -2.18005356e-04,  3.89796682e-04],\n",
       "         [-6.60262886e-05, -4.47553024e-03,  1.88974896e-03, ...,\n",
       "           2.40730122e-04, -3.31972871e-04, -8.14924599e-04],\n",
       "         [ 6.82083075e-04, -4.85374499e-03,  1.23813283e-03, ...,\n",
       "          -8.72654375e-04,  1.65737071e-03,  1.62522960e-03],\n",
       "         [-6.60262886e-05, -4.47553024e-03,  1.88974896e-03, ...,\n",
       "           2.40730122e-04, -3.31972871e-04, -8.14924599e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.48274070e-03, -5.51067851e-03,  7.78492598e-04, ...,\n",
       "          -1.44314882e-03,  2.39309412e-03,  4.15294524e-03],\n",
       "         [ 5.48274070e-03, -5.51067851e-03,  7.78492598e-04, ...,\n",
       "          -1.44314882e-03,  2.39309412e-03,  4.15294524e-03],\n",
       "         [ 5.62821794e-03, -5.23442030e-03,  3.70219117e-04, ...,\n",
       "          -1.66117318e-03,  1.34800503e-03,  3.51772620e-03],\n",
       "         [ 5.48274070e-03, -5.51067851e-03,  7.78492598e-04, ...,\n",
       "          -1.44314882e-03,  2.39309412e-03,  4.15294524e-03],\n",
       "         [ 5.48274070e-03, -5.51067851e-03,  7.78492598e-04, ...,\n",
       "          -1.44314882e-03,  2.39309412e-03,  4.15294524e-03]],\n",
       "\n",
       "        [[ 5.51712140e-03, -5.76353166e-03,  9.52620176e-04, ...,\n",
       "          -1.02803321e-03,  2.43477849e-03,  3.58137954e-03],\n",
       "         [ 5.51712140e-03, -5.76353166e-03,  9.52620176e-04, ...,\n",
       "          -1.02803321e-03,  2.43477849e-03,  3.58137954e-03],\n",
       "         [ 5.51712140e-03, -5.76353166e-03,  9.52620176e-04, ...,\n",
       "          -1.02803321e-03,  2.43477849e-03,  3.58137954e-03],\n",
       "         [ 5.51712140e-03, -5.76353166e-03,  9.52620176e-04, ...,\n",
       "          -1.02803321e-03,  2.43477849e-03,  3.58137954e-03],\n",
       "         [ 5.51712140e-03, -5.76353166e-03,  9.52620176e-04, ...,\n",
       "          -1.02803321e-03,  2.43477849e-03,  3.58137954e-03]],\n",
       "\n",
       "        [[ 5.57027385e-03, -5.82067203e-03,  1.21785083e-03, ...,\n",
       "          -5.88812749e-04,  2.52749166e-03,  3.05802375e-03],\n",
       "         [ 5.22362255e-03, -5.85155655e-03,  1.01714488e-03, ...,\n",
       "          -9.82539495e-04,  2.77186325e-03,  3.72635759e-03],\n",
       "         [ 5.48915286e-03, -5.92377409e-03,  6.28709327e-04, ...,\n",
       "          -8.17151857e-04,  1.25843636e-03,  2.35034619e-03],\n",
       "         [ 5.76319825e-03, -5.28459763e-03,  1.22131628e-03, ...,\n",
       "          -8.77429498e-04,  2.40373192e-03,  3.58168175e-03],\n",
       "         [ 5.65824006e-03, -5.54142054e-03,  1.49136956e-03, ...,\n",
       "          -8.33683589e-04,  2.88257515e-03,  3.45515832e-03]]]],\n",
       "      dtype=float32), ids=array([[[  0,   0, 986, 986,   0],\n",
       "        [986, 399, 937, 399, 986],\n",
       "        [399, 399, 399, 937, 937],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [814, 814, 399, 399, 921],\n",
       "        [814, 814, 814, 921, 921],\n",
       "        [814, 921, 814, 921, 814],\n",
       "        [921, 921,  44, 814, 921]],\n",
       "\n",
       "       [[661, 490, 525, 525, 525],\n",
       "        [490, 661, 937, 661, 490],\n",
       "        [661, 490, 490, 661, 423],\n",
       "        [423, 490, 817, 812, 817],\n",
       "        [817, 817, 817, 423, 423],\n",
       "        [817, 817, 817, 468, 817],\n",
       "        [817, 817, 817, 468, 817],\n",
       "        [817, 817, 817, 817, 817],\n",
       "        [817, 817, 817, 143, 817],\n",
       "        [214, 143, 143, 484, 415],\n",
       "        [214, 214, 415, 214, 214]],\n",
       "\n",
       "       [[205, 926, 205, 896, 896],\n",
       "        [812, 205, 812, 205, 812],\n",
       "        [812, 812, 651, 651, 651],\n",
       "        [812, 651, 812, 812, 651],\n",
       "        [651, 812, 812, 651, 448],\n",
       "        [448, 651, 448, 448, 448],\n",
       "        [448, 448, 448, 651, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 681, 143, 484, 166]]], dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0, 986, 986,   0],\n",
       "        [986, 399, 937, 399, 986],\n",
       "        [399, 399, 399, 937, 937],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [399, 399, 399, 399, 399],\n",
       "        [814, 814, 399, 399, 921],\n",
       "        [814, 814, 814, 921, 921],\n",
       "        [814, 921, 814, 921, 814],\n",
       "        [921, 921,  44, 814, 921]],\n",
       "\n",
       "       [[661, 490, 525, 525, 525],\n",
       "        [490, 661, 937, 661, 490],\n",
       "        [661, 490, 490, 661, 423],\n",
       "        [423, 490, 817, 812, 817],\n",
       "        [817, 817, 817, 423, 423],\n",
       "        [817, 817, 817, 468, 817],\n",
       "        [817, 817, 817, 468, 817],\n",
       "        [817, 817, 817, 817, 817],\n",
       "        [817, 817, 817, 143, 817],\n",
       "        [214, 143, 143, 484, 415],\n",
       "        [214, 214, 415, 214, 214]],\n",
       "\n",
       "       [[205, 926, 205, 896, 896],\n",
       "        [812, 205, 812, 205, 812],\n",
       "        [812, 812, 651, 651, 651],\n",
       "        [812, 651, 812, 812, 651],\n",
       "        [651, 812, 812, 651, 448],\n",
       "        [448, 651, 448, 448, 448],\n",
       "        [448, 448, 448, 651, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 448, 448, 448, 448],\n",
       "        [448, 681, 143, 484, 166]]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": ECMWrapper,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.909589, perp: 997.298, dev_prep: 997.282, (0.731 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 20, loss = 5.683861, perp: 297.356, dev_prep: 289.451, (0.156 sec/step)\n",
      "step 40, loss = 7.360167, perp: 1452.605, dev_prep: 1419.171, (0.153 sec/step)\n",
      "step 60, loss = 5.336058, perp: 203.951, dev_prep: 207.280, (0.141 sec/step)\n",
      "step 80, loss = 5.275846, perp: 193.998, dev_prep: 179.468, (0.148 sec/step)\n",
      "step 100, loss = 5.074132, perp: 157.708, dev_prep: 159.610, (0.147 sec/step)\n",
      "step 120, loss = 4.978775, perp: 143.698, dev_prep: 140.749, (0.149 sec/step)\n",
      "step 140, loss = 4.925634, perp: 135.697, dev_prep: 148.184, (0.151 sec/step)\n",
      "step 160, loss = 4.984635, perp: 139.898, dev_prep: 117.746, (0.148 sec/step)\n",
      "step 180, loss = 4.859539, perp: 129.531, dev_prep: 150.796, (0.154 sec/step)\n",
      "step 200, loss = 4.811092, perp: 120.424, dev_prep: 128.425, (0.143 sec/step)\n",
      "step 220, loss = 4.788205, perp: 119.303, dev_prep: 121.254, (0.156 sec/step)\n",
      "step 240, loss = 4.931840, perp: 136.126, dev_prep: 131.479, (0.165 sec/step)\n",
      "step 260, loss = 4.854414, perp: 128.730, dev_prep: 126.122, (0.157 sec/step)\n",
      "step 280, loss = 4.725758, perp: 111.579, dev_prep: 111.917, (0.144 sec/step)\n",
      "step 300, loss = 4.646326, perp: 102.568, dev_prep: 117.297, (0.156 sec/step)\n",
      "step 320, loss = 4.697734, perp: 108.571, dev_prep: 126.372, (0.142 sec/step)\n",
      "step 340, loss = 4.728120, perp: 112.303, dev_prep: 131.791, (0.162 sec/step)\n",
      "step 360, loss = 4.922916, perp: 131.837, dev_prep: 138.520, (0.173 sec/step)\n",
      "step 380, loss = 4.643151, perp: 103.669, dev_prep: 118.312, (0.147 sec/step)\n",
      "step 400, loss = 4.730266, perp: 111.054, dev_prep: 120.433, (0.173 sec/step)\n",
      "step 420, loss = 4.747850, perp: 113.400, dev_prep: 115.169, (0.154 sec/step)\n",
      "step 440, loss = 4.797918, perp: 120.710, dev_prep: 126.086, (0.165 sec/step)\n",
      "step 460, loss = 4.597556, perp: 98.327, dev_prep: 111.796, (0.155 sec/step)\n",
      "step 480, loss = 4.794459, perp: 119.362, dev_prep: 96.600, (0.150 sec/step)\n",
      "step 500, loss = 4.606021, perp: 98.138, dev_prep: 105.056, (0.165 sec/step)\n",
      "step 520, loss = 4.728901, perp: 113.149, dev_prep: 120.201, (0.150 sec/step)\n",
      "step 540, loss = 4.612674, perp: 99.353, dev_prep: 110.865, (0.158 sec/step)\n",
      "step 560, loss = 4.540291, perp: 92.214, dev_prep: 74.709, (0.168 sec/step)\n",
      "step 580, loss = 4.475836, perp: 87.645, dev_prep: 116.623, (0.153 sec/step)\n",
      "step 600, loss = 4.387408, perp: 78.546, dev_prep: 72.670, (0.161 sec/step)\n",
      "step 620, loss = 4.547399, perp: 92.067, dev_prep: 86.696, (0.169 sec/step)\n",
      "step 640, loss = 4.183442, perp: 64.980, dev_prep: 78.487, (0.156 sec/step)\n",
      "step 660, loss = 4.261464, perp: 70.244, dev_prep: 74.219, (0.148 sec/step)\n",
      "step 680, loss = 4.290100, perp: 72.064, dev_prep: 82.668, (0.164 sec/step)\n",
      "step 700, loss = 4.234986, perp: 66.606, dev_prep: 72.428, (0.154 sec/step)\n",
      "step 720, loss = 4.161120, perp: 63.129, dev_prep: 80.855, (0.163 sec/step)\n",
      "step 740, loss = 4.269475, perp: 69.968, dev_prep: 73.792, (0.163 sec/step)\n",
      "step 760, loss = 4.032516, perp: 55.831, dev_prep: 73.961, (0.162 sec/step)\n",
      "step 780, loss = 4.241436, perp: 67.336, dev_prep: 71.919, (0.165 sec/step)\n",
      "step 800, loss = 4.086658, perp: 56.865, dev_prep: 69.359, (0.188 sec/step)\n",
      "step 820, loss = 4.112173, perp: 60.100, dev_prep: 62.062, (0.160 sec/step)\n",
      "step 840, loss = 4.280904, perp: 73.009, dev_prep: 58.265, (0.154 sec/step)\n",
      "step 860, loss = 4.040851, perp: 56.757, dev_prep: 69.182, (0.167 sec/step)\n",
      "step 880, loss = 4.132014, perp: 60.968, dev_prep: 59.704, (0.156 sec/step)\n",
      "step 900, loss = 4.066236, perp: 58.298, dev_prep: 61.951, (0.151 sec/step)\n",
      "step 920, loss = 4.025556, perp: 55.161, dev_prep: 63.710, (0.143 sec/step)\n",
      "step 940, loss = 4.039426, perp: 55.875, dev_prep: 64.084, (0.173 sec/step)\n",
      "step 960, loss = 3.831431, perp: 44.976, dev_prep: 51.667, (0.146 sec/step)\n",
      "step 980, loss = 3.940261, perp: 50.373, dev_prep: 56.311, (0.146 sec/step)\n",
      "step 1000, loss = 3.984503, perp: 52.369, dev_prep: 54.877, (0.156 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 1020, loss = 4.084227, perp: 58.225, dev_prep: 58.326, (0.155 sec/step)\n",
      "step 1040, loss = 3.956316, perp: 51.093, dev_prep: 56.912, (0.160 sec/step)\n",
      "step 1060, loss = 4.170290, perp: 65.303, dev_prep: 50.700, (0.166 sec/step)\n",
      "step 1080, loss = 3.997880, perp: 53.394, dev_prep: 47.507, (0.152 sec/step)\n",
      "step 1100, loss = 3.912585, perp: 49.928, dev_prep: 54.324, (0.153 sec/step)\n",
      "step 1120, loss = 3.898821, perp: 48.064, dev_prep: 52.421, (0.143 sec/step)\n",
      "step 1140, loss = 3.816481, perp: 45.363, dev_prep: 50.335, (0.154 sec/step)\n",
      "step 1160, loss = 3.945604, perp: 49.991, dev_prep: 57.397, (0.150 sec/step)\n",
      "step 1180, loss = 3.923126, perp: 49.779, dev_prep: 51.488, (0.158 sec/step)\n",
      "step 1200, loss = 3.916201, perp: 49.590, dev_prep: 56.621, (0.154 sec/step)\n",
      "step 1220, loss = 4.109468, perp: 60.117, dev_prep: 56.150, (0.171 sec/step)\n",
      "step 1240, loss = 3.929928, perp: 50.122, dev_prep: 54.085, (0.158 sec/step)\n",
      "step 1260, loss = 3.875487, perp: 48.748, dev_prep: 49.531, (0.155 sec/step)\n",
      "step 1280, loss = 3.763708, perp: 42.867, dev_prep: 52.442, (0.153 sec/step)\n",
      "step 1300, loss = 3.883933, perp: 47.271, dev_prep: 48.044, (0.147 sec/step)\n",
      "step 1320, loss = 3.952166, perp: 51.092, dev_prep: 48.421, (0.163 sec/step)\n",
      "step 1340, loss = 3.706218, perp: 39.938, dev_prep: 49.752, (0.150 sec/step)\n",
      "step 1360, loss = 3.719821, perp: 40.884, dev_prep: 46.184, (0.162 sec/step)\n",
      "step 1380, loss = 3.879920, perp: 50.231, dev_prep: 47.958, (0.150 sec/step)\n",
      "step 1400, loss = 3.827828, perp: 45.567, dev_prep: 53.980, (0.149 sec/step)\n",
      "step 1420, loss = 3.769976, perp: 42.618, dev_prep: 52.463, (0.150 sec/step)\n",
      "step 1440, loss = 3.766311, perp: 42.763, dev_prep: 50.998, (0.153 sec/step)\n",
      "step 1460, loss = 3.705446, perp: 41.053, dev_prep: 46.553, (0.147 sec/step)\n",
      "step 1480, loss = 3.735718, perp: 41.103, dev_prep: 47.949, (0.152 sec/step)\n",
      "step 1500, loss = 3.767849, perp: 42.531, dev_prep: 43.832, (0.183 sec/step)\n",
      "step 1520, loss = 3.821978, perp: 44.663, dev_prep: 44.769, (0.165 sec/step)\n",
      "step 1540, loss = 3.695162, perp: 39.937, dev_prep: 45.357, (0.155 sec/step)\n",
      "step 1560, loss = 3.889897, perp: 47.603, dev_prep: 41.735, (0.129 sec/step)\n",
      "step 1580, loss = 3.693152, perp: 40.095, dev_prep: 42.011, (0.171 sec/step)\n",
      "step 1600, loss = 3.832736, perp: 45.875, dev_prep: 42.910, (0.149 sec/step)\n",
      "step 1620, loss = 3.700716, perp: 40.251, dev_prep: 48.345, (0.161 sec/step)\n",
      "step 1640, loss = 3.629610, perp: 36.535, dev_prep: 45.652, (0.158 sec/step)\n",
      "step 1660, loss = 3.698859, perp: 39.537, dev_prep: 40.979, (0.151 sec/step)\n",
      "step 1680, loss = 3.791295, perp: 43.374, dev_prep: 49.155, (0.169 sec/step)\n",
      "step 1700, loss = 3.716130, perp: 39.814, dev_prep: 42.261, (0.153 sec/step)\n",
      "step 1720, loss = 3.889906, perp: 47.824, dev_prep: 43.950, (0.144 sec/step)\n",
      "step 1740, loss = 3.687026, perp: 40.097, dev_prep: 43.211, (0.171 sec/step)\n",
      "step 1760, loss = 3.837936, perp: 45.503, dev_prep: 45.699, (0.146 sec/step)\n",
      "step 1780, loss = 3.659184, perp: 38.314, dev_prep: 46.522, (0.151 sec/step)\n",
      "step 1800, loss = 3.703942, perp: 40.003, dev_prep: 39.673, (0.146 sec/step)\n",
      "step 1820, loss = 3.675582, perp: 38.326, dev_prep: 43.165, (0.161 sec/step)\n",
      "step 1840, loss = 3.747516, perp: 41.497, dev_prep: 42.934, (0.170 sec/step)\n",
      "step 1860, loss = 3.615704, perp: 36.082, dev_prep: 43.199, (0.190 sec/step)\n",
      "step 1880, loss = 3.652722, perp: 37.504, dev_prep: 42.011, (0.155 sec/step)\n",
      "step 1900, loss = 3.755010, perp: 41.697, dev_prep: 38.162, (0.160 sec/step)\n",
      "step 1920, loss = 3.588186, perp: 35.803, dev_prep: 42.871, (0.155 sec/step)\n",
      "step 1940, loss = 3.690329, perp: 39.553, dev_prep: 46.832, (0.147 sec/step)\n",
      "step 1960, loss = 3.653886, perp: 36.856, dev_prep: 42.533, (0.150 sec/step)\n",
      "step 1980, loss = 3.602115, perp: 35.930, dev_prep: 38.165, (0.167 sec/step)\n",
      "step 2000, loss = 3.674363, perp: 39.579, dev_prep: 43.287, (0.163 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 2020, loss = 3.618343, perp: 36.758, dev_prep: 45.871, (0.152 sec/step)\n",
      "step 2040, loss = 3.709431, perp: 39.678, dev_prep: 43.254, (0.165 sec/step)\n",
      "step 2060, loss = 3.787422, perp: 43.201, dev_prep: 41.337, (0.154 sec/step)\n",
      "step 2080, loss = 3.675193, perp: 39.491, dev_prep: 41.720, (0.169 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss = 3.645794, perp: 37.003, dev_prep: 41.913, (0.166 sec/step)\n",
      "step 2120, loss = 3.627230, perp: 35.811, dev_prep: 41.091, (0.167 sec/step)\n",
      "step 2140, loss = 3.676892, perp: 39.150, dev_prep: 37.232, (0.172 sec/step)\n",
      "step 2160, loss = 3.682097, perp: 36.862, dev_prep: 40.625, (0.160 sec/step)\n",
      "step 2180, loss = 3.694583, perp: 40.102, dev_prep: 42.662, (0.151 sec/step)\n",
      "step 2200, loss = 3.710526, perp: 38.979, dev_prep: 39.145, (0.162 sec/step)\n",
      "step 2220, loss = 3.585181, perp: 35.400, dev_prep: 39.770, (0.156 sec/step)\n",
      "step 2240, loss = 3.617204, perp: 36.702, dev_prep: 39.796, (0.157 sec/step)\n",
      "step 2260, loss = 3.629633, perp: 35.629, dev_prep: 36.179, (0.171 sec/step)\n",
      "step 2280, loss = 3.750805, perp: 40.675, dev_prep: 40.775, (0.185 sec/step)\n",
      "step 2300, loss = 3.677887, perp: 38.833, dev_prep: 41.403, (0.161 sec/step)\n",
      "step 2320, loss = 3.662566, perp: 39.136, dev_prep: 39.156, (0.181 sec/step)\n",
      "step 2340, loss = 3.676461, perp: 37.971, dev_prep: 35.401, (0.144 sec/step)\n",
      "step 2360, loss = 3.693193, perp: 38.842, dev_prep: 37.487, (0.154 sec/step)\n",
      "step 2380, loss = 3.704155, perp: 41.551, dev_prep: 38.020, (0.147 sec/step)\n",
      "step 2400, loss = 3.574583, perp: 35.256, dev_prep: 38.523, (0.168 sec/step)\n",
      "step 2420, loss = 3.617647, perp: 36.471, dev_prep: 37.701, (0.169 sec/step)\n",
      "step 2440, loss = 3.542198, perp: 33.844, dev_prep: 36.665, (0.160 sec/step)\n",
      "step 2460, loss = 3.604315, perp: 36.181, dev_prep: 35.604, (0.149 sec/step)\n",
      "step 2480, loss = 3.613247, perp: 36.243, dev_prep: 37.487, (0.149 sec/step)\n",
      "step 2500, loss = 3.618970, perp: 36.555, dev_prep: 37.837, (0.163 sec/step)\n",
      "step 2520, loss = 3.507818, perp: 33.526, dev_prep: 37.998, (0.151 sec/step)\n",
      "step 2540, loss = 3.590429, perp: 35.234, dev_prep: 36.455, (0.161 sec/step)\n",
      "step 2560, loss = 3.641006, perp: 36.669, dev_prep: 43.753, (0.169 sec/step)\n",
      "step 2580, loss = 3.586778, perp: 35.131, dev_prep: 40.518, (0.166 sec/step)\n",
      "step 2600, loss = 3.560441, perp: 34.678, dev_prep: 36.919, (0.153 sec/step)\n",
      "step 2620, loss = 3.595716, perp: 35.522, dev_prep: 43.152, (0.177 sec/step)\n",
      "step 2640, loss = 3.570621, perp: 34.976, dev_prep: 35.592, (0.159 sec/step)\n",
      "step 2660, loss = 3.518064, perp: 33.163, dev_prep: 38.587, (0.182 sec/step)\n",
      "step 2680, loss = 3.581789, perp: 35.099, dev_prep: 34.844, (0.153 sec/step)\n",
      "step 2700, loss = 3.512876, perp: 33.398, dev_prep: 42.020, (0.163 sec/step)\n",
      "step 2720, loss = 3.620604, perp: 35.727, dev_prep: 37.749, (0.164 sec/step)\n",
      "step 2740, loss = 3.593318, perp: 35.875, dev_prep: 37.065, (0.165 sec/step)\n",
      "step 2760, loss = 3.423362, perp: 30.261, dev_prep: 37.443, (0.167 sec/step)\n",
      "step 2780, loss = 3.401311, perp: 29.226, dev_prep: 32.972, (0.144 sec/step)\n",
      "step 2800, loss = 3.526025, perp: 33.172, dev_prep: 36.593, (0.151 sec/step)\n",
      "step 2820, loss = 3.546826, perp: 33.995, dev_prep: 41.897, (0.162 sec/step)\n",
      "step 2840, loss = 3.563676, perp: 34.820, dev_prep: 33.584, (0.162 sec/step)\n",
      "step 2860, loss = 3.540091, perp: 33.441, dev_prep: 35.395, (0.142 sec/step)\n",
      "step 2880, loss = 3.604630, perp: 36.972, dev_prep: 33.709, (0.174 sec/step)\n",
      "step 2900, loss = 3.592921, perp: 35.591, dev_prep: 35.245, (0.151 sec/step)\n",
      "step 2920, loss = 3.409573, perp: 29.844, dev_prep: 30.558, (0.160 sec/step)\n",
      "step 2940, loss = 3.489077, perp: 31.877, dev_prep: 33.053, (0.157 sec/step)\n",
      "step 2960, loss = 3.519528, perp: 33.063, dev_prep: 35.196, (0.158 sec/step)\n",
      "step 2980, loss = 3.521572, perp: 33.334, dev_prep: 33.076, (0.159 sec/step)\n",
      "step 3000, loss = 3.441051, perp: 30.680, dev_prep: 36.154, (0.151 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 3020, loss = 3.610379, perp: 36.034, dev_prep: 35.802, (0.161 sec/step)\n",
      "step 3040, loss = 3.497997, perp: 32.133, dev_prep: 34.762, (0.147 sec/step)\n",
      "step 3060, loss = 3.429538, perp: 30.372, dev_prep: 32.614, (0.147 sec/step)\n",
      "step 3080, loss = 3.496083, perp: 31.432, dev_prep: 35.511, (0.165 sec/step)\n",
      "step 3100, loss = 3.370960, perp: 28.126, dev_prep: 36.371, (0.162 sec/step)\n",
      "step 3120, loss = 3.462667, perp: 31.730, dev_prep: 36.707, (0.147 sec/step)\n",
      "step 3140, loss = 3.479908, perp: 31.532, dev_prep: 30.132, (0.203 sec/step)\n",
      "step 3160, loss = 3.421581, perp: 30.419, dev_prep: 33.746, (0.159 sec/step)\n",
      "step 3180, loss = 3.434536, perp: 30.445, dev_prep: 33.065, (0.169 sec/step)\n",
      "step 3200, loss = 3.491027, perp: 32.088, dev_prep: 35.136, (0.159 sec/step)\n",
      "step 3220, loss = 3.525632, perp: 33.647, dev_prep: 33.746, (0.157 sec/step)\n",
      "step 3240, loss = 3.346549, perp: 27.424, dev_prep: 34.691, (0.158 sec/step)\n",
      "step 3260, loss = 3.576293, perp: 34.515, dev_prep: 34.449, (0.151 sec/step)\n",
      "step 3280, loss = 3.330576, perp: 27.328, dev_prep: 32.691, (0.163 sec/step)\n",
      "step 3300, loss = 3.383350, perp: 28.933, dev_prep: 30.862, (0.162 sec/step)\n",
      "step 3320, loss = 3.331790, perp: 27.065, dev_prep: 30.216, (0.154 sec/step)\n",
      "step 3340, loss = 3.339302, perp: 27.681, dev_prep: 27.702, (0.178 sec/step)\n",
      "step 3360, loss = 3.408717, perp: 30.179, dev_prep: 36.700, (0.160 sec/step)\n",
      "step 3380, loss = 3.356187, perp: 28.209, dev_prep: 32.259, (0.176 sec/step)\n",
      "step 3400, loss = 3.507047, perp: 32.625, dev_prep: 31.965, (0.152 sec/step)\n",
      "step 3420, loss = 3.416278, perp: 29.798, dev_prep: 32.662, (0.171 sec/step)\n",
      "step 3440, loss = 3.410115, perp: 30.073, dev_prep: 30.298, (0.176 sec/step)\n",
      "step 3460, loss = 3.437520, perp: 29.975, dev_prep: 31.921, (0.165 sec/step)\n",
      "step 3480, loss = 3.401800, perp: 29.756, dev_prep: 32.047, (0.157 sec/step)\n",
      "step 3500, loss = 3.538356, perp: 33.380, dev_prep: 33.086, (0.158 sec/step)\n",
      "step 3520, loss = 3.473675, perp: 31.607, dev_prep: 35.075, (0.157 sec/step)\n",
      "step 3540, loss = 3.450454, perp: 29.960, dev_prep: 31.709, (0.168 sec/step)\n",
      "step 3560, loss = 3.376187, perp: 29.101, dev_prep: 32.855, (0.162 sec/step)\n",
      "step 3580, loss = 3.302209, perp: 26.369, dev_prep: 31.009, (0.165 sec/step)\n",
      "step 3600, loss = 3.468880, perp: 30.832, dev_prep: 31.832, (0.180 sec/step)\n",
      "step 3620, loss = 3.433607, perp: 30.159, dev_prep: 30.270, (0.168 sec/step)\n",
      "step 3640, loss = 3.283052, perp: 26.612, dev_prep: 34.821, (0.152 sec/step)\n",
      "step 3660, loss = 3.408007, perp: 29.022, dev_prep: 30.543, (0.151 sec/step)\n",
      "step 3680, loss = 3.414726, perp: 29.543, dev_prep: 33.854, (0.155 sec/step)\n",
      "step 3700, loss = 3.336043, perp: 27.489, dev_prep: 29.952, (0.151 sec/step)\n",
      "step 3720, loss = 3.377528, perp: 29.014, dev_prep: 31.990, (0.167 sec/step)\n",
      "step 3740, loss = 3.378230, perp: 28.760, dev_prep: 29.061, (0.153 sec/step)\n",
      "step 3760, loss = 3.321707, perp: 27.382, dev_prep: 31.071, (0.166 sec/step)\n",
      "step 3780, loss = 3.333616, perp: 27.471, dev_prep: 28.806, (0.190 sec/step)\n",
      "step 3800, loss = 3.365734, perp: 28.919, dev_prep: 31.724, (0.164 sec/step)\n",
      "step 3820, loss = 3.309004, perp: 26.902, dev_prep: 29.549, (0.149 sec/step)\n",
      "step 3840, loss = 3.420543, perp: 30.383, dev_prep: 30.729, (0.162 sec/step)\n",
      "step 3860, loss = 3.351201, perp: 27.736, dev_prep: 29.665, (0.148 sec/step)\n",
      "step 3880, loss = 3.352027, perp: 27.507, dev_prep: 30.277, (0.176 sec/step)\n",
      "step 3900, loss = 3.377417, perp: 28.200, dev_prep: 24.290, (0.156 sec/step)\n",
      "step 3920, loss = 3.252604, perp: 25.404, dev_prep: 30.350, (0.166 sec/step)\n",
      "step 3940, loss = 3.306370, perp: 26.757, dev_prep: 31.339, (0.153 sec/step)\n",
      "step 3960, loss = 3.358813, perp: 27.974, dev_prep: 27.858, (0.162 sec/step)\n",
      "step 3980, loss = 3.275136, perp: 24.199, dev_prep: 29.544, (0.195 sec/step)\n",
      "step 4000, loss = 3.304400, perp: 26.311, dev_prep: 28.425, (0.189 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "step 4020, loss = 3.302267, perp: 26.591, dev_prep: 25.871, (0.162 sec/step)\n",
      "step 4040, loss = 3.204991, perp: 23.659, dev_prep: 28.627, (0.151 sec/step)\n",
      "step 4060, loss = 3.346427, perp: 29.226, dev_prep: 31.001, (0.172 sec/step)\n",
      "step 4080, loss = 3.295415, perp: 26.213, dev_prep: 31.858, (0.172 sec/step)\n",
      "step 4100, loss = 3.265158, perp: 25.401, dev_prep: 28.118, (0.173 sec/step)\n",
      "step 4120, loss = 3.254014, perp: 25.277, dev_prep: 29.951, (0.172 sec/step)\n",
      "step 4140, loss = 3.076582, perp: 21.405, dev_prep: 27.683, (0.170 sec/step)\n",
      "step 4160, loss = 3.348269, perp: 28.063, dev_prep: 31.001, (0.189 sec/step)\n",
      "step 4180, loss = 3.240282, perp: 25.758, dev_prep: 23.926, (0.163 sec/step)\n",
      "step 4200, loss = 3.334950, perp: 26.911, dev_prep: 30.784, (0.175 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4220, loss = 3.212474, perp: 24.408, dev_prep: 26.070, (0.152 sec/step)\n",
      "step 4240, loss = 3.114026, perp: 21.249, dev_prep: 25.518, (0.172 sec/step)\n",
      "step 4260, loss = 3.186993, perp: 24.097, dev_prep: 26.070, (0.161 sec/step)\n",
      "step 4280, loss = 3.183968, perp: 23.878, dev_prep: 25.412, (0.157 sec/step)\n",
      "step 4300, loss = 3.230902, perp: 25.224, dev_prep: 23.940, (0.175 sec/step)\n",
      "step 4320, loss = 3.302682, perp: 26.254, dev_prep: 26.831, (0.181 sec/step)\n",
      "step 4340, loss = 3.206807, perp: 23.930, dev_prep: 24.899, (0.142 sec/step)\n",
      "step 4360, loss = 3.185052, perp: 24.017, dev_prep: 26.356, (0.164 sec/step)\n",
      "step 4380, loss = 3.114038, perp: 22.053, dev_prep: 26.161, (0.161 sec/step)\n",
      "step 4400, loss = 3.232150, perp: 24.665, dev_prep: 29.097, (0.180 sec/step)\n",
      "step 4420, loss = 3.245446, perp: 24.835, dev_prep: 25.641, (0.149 sec/step)\n",
      "step 4440, loss = 3.164261, perp: 22.788, dev_prep: 23.935, (0.143 sec/step)\n",
      "step 4460, loss = 3.233606, perp: 24.844, dev_prep: 26.989, (0.153 sec/step)\n",
      "step 4480, loss = 3.089878, perp: 21.655, dev_prep: 27.753, (0.168 sec/step)\n",
      "step 4500, loss = 3.305251, perp: 25.819, dev_prep: 25.907, (0.165 sec/step)\n",
      "step 4520, loss = 3.233981, perp: 24.801, dev_prep: 22.307, (0.162 sec/step)\n",
      "step 4540, loss = 3.116191, perp: 22.044, dev_prep: 23.512, (0.150 sec/step)\n",
      "step 4560, loss = 3.212841, perp: 24.067, dev_prep: 25.876, (0.192 sec/step)\n",
      "step 4580, loss = 3.241647, perp: 25.286, dev_prep: 26.297, (0.151 sec/step)\n",
      "step 4600, loss = 3.104999, perp: 21.611, dev_prep: 23.309, (0.158 sec/step)\n",
      "step 4620, loss = 3.097112, perp: 21.594, dev_prep: 25.544, (0.142 sec/step)\n",
      "step 4640, loss = 3.145716, perp: 22.954, dev_prep: 23.773, (0.167 sec/step)\n",
      "step 4660, loss = 3.150691, perp: 22.816, dev_prep: 23.617, (0.174 sec/step)\n",
      "step 4680, loss = 3.108804, perp: 21.539, dev_prep: 24.389, (0.158 sec/step)\n",
      "step 4700, loss = 3.177398, perp: 23.964, dev_prep: 25.350, (0.160 sec/step)\n",
      "step 4720, loss = 3.219660, perp: 23.969, dev_prep: 24.550, (0.159 sec/step)\n",
      "step 4740, loss = 3.116105, perp: 21.922, dev_prep: 26.727, (0.164 sec/step)\n",
      "step 4760, loss = 3.203120, perp: 23.576, dev_prep: 22.841, (0.171 sec/step)\n",
      "step 4780, loss = 3.066682, perp: 20.640, dev_prep: 23.785, (0.167 sec/step)\n",
      "step 4800, loss = 3.173362, perp: 23.102, dev_prep: 23.912, (0.170 sec/step)\n",
      "step 4820, loss = 3.110090, perp: 21.766, dev_prep: 22.288, (0.166 sec/step)\n",
      "step 4840, loss = 3.117776, perp: 21.461, dev_prep: 24.520, (0.162 sec/step)\n",
      "step 4860, loss = 3.127730, perp: 22.070, dev_prep: 22.592, (0.156 sec/step)\n",
      "step 4880, loss = 3.053483, perp: 20.721, dev_prep: 23.811, (0.191 sec/step)\n",
      "step 4900, loss = 2.945072, perp: 18.886, dev_prep: 23.890, (0.166 sec/step)\n",
      "step 4920, loss = 3.082122, perp: 22.199, dev_prep: 25.766, (0.148 sec/step)\n",
      "step 4940, loss = 2.923201, perp: 18.703, dev_prep: 23.520, (0.162 sec/step)\n",
      "step 4960, loss = 2.964064, perp: 18.429, dev_prep: 20.303, (0.165 sec/step)\n",
      "step 4980, loss = 3.013661, perp: 20.036, dev_prep: 22.687, (0.158 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_inds = np.random.choice(\n",
    "                    len(dev_source_data), batch_size)\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data[dev_inds],\n",
    "                    target_ids: dev_target_data[dev_inds],\n",
    "                    sequence_mask: dev_masks[dev_inds],\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks[dev_inds], dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"CE loss\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.show()\n",
    "plt.savefig(loss_fig)\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps[5:], perps[5:], label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps[5:], dev_perps[5:], label=\"dev\")\n",
    "plt.title(\"perplexity\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(perp_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - embed_shift\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.350233746671854"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(len(dev_source_data), 256)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, m, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
