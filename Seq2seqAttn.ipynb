{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlai/Desktop/ECE276C_RL/env3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Toy Dataset: Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "N = 1000\n",
    "source_data = []\n",
    "target_data = []\n",
    "num_data = 20000\n",
    "\n",
    "for i in range(num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"0\": source_data})\n",
    "tdf = pd.DataFrame(data={\"0\": target_data})\n",
    "\n",
    "if not os.path.exists(\"./example/\"):\n",
    "    os.makedirs(\"./example/\")\n",
    "\n",
    "df.to_csv(\"./example/train_source.txt\", header=None, index=None)\n",
    "tdf.to_csv(\"./example/train_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_source_data = []\n",
    "dev_target_data = []\n",
    "dev_num_data = 1000\n",
    "\n",
    "for i in range(dev_num_data):\n",
    "    length = 15 + np.random.choice(11)\n",
    "    s = np.random.choice(N, length)\n",
    "    t = np.sort(s)\n",
    "\n",
    "    dev_source_data.append(\" \".join(s.astype(str).tolist()))\n",
    "    dev_target_data.append(\" \".join(t.astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.DataFrame(data={\"0\": dev_source_data})\n",
    "tddf = pd.DataFrame(data={\"0\": dev_target_data})\n",
    "\n",
    "ddf.to_csv(\"./example/dev_source.txt\", header=None, index=None)\n",
    "tddf.to_csv(\"./example/dev_target.txt\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(vocab_size, embed_size, dtype=tf.float32,\n",
    "                    initializer=None, initial_values=None,\n",
    "                    name='embeddings'):\n",
    "    \"\"\"\n",
    "    embeddings:\n",
    "        initialize trainable embeddings or load pretrained from files\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if initial_values:\n",
    "            embeddings = tf.Variable(initial_value=initial_values,\n",
    "                                     name=\"embeddings\", dtype=dtype)\n",
    "        else:\n",
    "            if initializer is None:\n",
    "                initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "            embeddings = tf.Variable(\n",
    "                initializer(shape=(vocab_size, embed_size)),\n",
    "                name=\"embeddings\", dtype=dtype)\n",
    "\n",
    "        # id_0 represents SOS token, id_1 represents EOS token\n",
    "        se_embed = tf.get_variable(\"SOS/EOS\", [2, embed_size], dtype)\n",
    "        # id_2 represents constant all zeros\n",
    "        zero_embed = tf.zeros(shape=[1, embed_size])\n",
    "        embeddings = tf.concat([se_embed, zero_embed, embeddings], axis=0)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "\n",
    "def create_cell(num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    Cell: build a recurrent cell\n",
    "        num_units: number of hidden cell units\n",
    "        cell_type: LSTM, GRU, LN_LSTM (layer_normalize)\n",
    "    \"\"\"\n",
    "    if cell_type == \"LSTM\":\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units, forget_bias=forget_bias)\n",
    "\n",
    "    elif cell_type == \"GRU\":\n",
    "        cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "\n",
    "    elif cell_type == \"LN_LSTM\":\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "            num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            layer_norm=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown cell type %s\" % cell_type)\n",
    "\n",
    "    return cell\n",
    "\n",
    "\n",
    "def build_rnn_cell(num_layers, num_units, cell_type, forget_bias=1.0):\n",
    "    \"\"\"\n",
    "    RNN_cell: build a multi-layer rnn cell\n",
    "        num_layers: number of hidden layers\n",
    "    \"\"\"\n",
    "    cell_seq = []\n",
    "    for i in range(num_layers):\n",
    "        cell = create_cell(num_units, cell_type, forget_bias)\n",
    "        cell_seq.append(cell)\n",
    "\n",
    "    if num_layers > 1:\n",
    "        rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cell_seq)\n",
    "    else:\n",
    "        rnn_cell = cell_seq[0]\n",
    "\n",
    "    return rnn_cell\n",
    "\n",
    "\n",
    "def build_encoder(embeddings, source_ids, num_layers, num_units, cell_type,\n",
    "                  forget_bias=1.0, bidir=False, time_major=False,\n",
    "                  dtype=tf.float32, name=\"encoder\"):\n",
    "    \"\"\"\n",
    "    encoder: build rnn encoder for Seq2seq\n",
    "        source_ids: [batch_size, max_time]\n",
    "        bidir: bidirectional or unidirectional\n",
    "\n",
    "    Returns:\n",
    "        encoder_outputs: [batch_size, max_time, num_units]\n",
    "        encoder_states: (StateTuple(shape=(batch_size, num_units)), ...)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        if time_major:\n",
    "            source_ids = tf.transpose(source_ids)\n",
    "\n",
    "        # embedding lookup, embed_inputs: [max_time, batch_size, num_units]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, source_ids)\n",
    "\n",
    "        # bidirectional\n",
    "        if bidir:\n",
    "            encoder_states = []\n",
    "            layer_inputs = embed_inputs\n",
    "\n",
    "            # build rnn layer-by-layer\n",
    "            for i in range(num_layers):\n",
    "                with tf.variable_scope(\"layer_%d\" % (i + 1)):\n",
    "                    fw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "                    bw_cell = build_rnn_cell(\n",
    "                        1, num_units, cell_type, forget_bias)\n",
    "\n",
    "                    dyn_rnn = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        fw_cell, bw_cell, layer_inputs,\n",
    "                        time_major=time_major,\n",
    "                        dtype=dtype,\n",
    "                        swap_memory=True)\n",
    "                    bi_outputs, (state_fw, state_bw) = dyn_rnn\n",
    "\n",
    "                    # handle cell memory state\n",
    "                    if cell_type == \"LSTM\":\n",
    "                        state_c = state_fw.c + state_bw.c\n",
    "                        state_h = state_fw.h + state_bw.h\n",
    "                        encoder_states.append(LSTMStateTuple(state_c, state_h))\n",
    "                    else:\n",
    "                        encoder_states.append(state_fw + state_bw)\n",
    "\n",
    "                    # concat and map as inputs of next layer\n",
    "                    layer_inputs = tf.layers.dense(\n",
    "                        tf.concat(bi_outputs, -1), num_units)\n",
    "\n",
    "            encoder_outputs = layer_inputs\n",
    "            encoder_states = tuple(encoder_states)\n",
    "\n",
    "        # unidirectional\n",
    "        else:\n",
    "            rnn_cell = build_rnn_cell(\n",
    "                num_layers, num_units, cell_type, forget_bias)\n",
    "\n",
    "            encoder_outputs, encoder_states = tf.nn.dynamic_rnn(\n",
    "                rnn_cell, embed_inputs,\n",
    "                time_major=time_major,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "    return encoder_outputs, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = init_embeddings(1000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_ids = tf.placeholder(tf.int32, [None, None])\n",
    "encoder_outputs, encoder_states = build_encoder(embeddings, source_ids, num_layers=2,\n",
    "                                                num_units=256, cell_type=\"LSTM\", bidir=True,\n",
    "                                                name=\"e8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention wrapper\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "AttnState = collections.namedtuple(\n",
    "    \"AttnState\", (\"cell_states\", \"h\", \"context\"))\n",
    "\n",
    "\n",
    "class AttentionWrapper(RNNCell):\n",
    "    \"\"\"\n",
    "    Attention Wrapper: wrap a rnn_cell with attention mechanism (Bahda 2015)\n",
    "        cell: vanilla multi-layer RNNCell\n",
    "        memory: [batch_size, max_time, num_units]\n",
    "    \"\"\"\n",
    "    def __init__(self, cell, memory, dec_init_states, num_hidden,\n",
    "                 num_units, dtype):\n",
    "        self._cell = cell\n",
    "        # transpose to [batch_size, num_units, max_time]\n",
    "        self._memory = memory\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._state_size = AttnState(self._cell.state_size,\n",
    "                                     num_units, memory.shape[-1].value)\n",
    "        self._num_units = num_units\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._num_units\n",
    "\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Generate initial state for attn wrapped rnn cell\n",
    "            dec_init_states: None (no states pass), or encoder final states\n",
    "            num_units: decoder's num of cell units\n",
    "        \"\"\"\n",
    "        if self._dec_init_states is None:\n",
    "            attn_state_0 = None\n",
    "\n",
    "        else:\n",
    "            cell_states = self._dec_init_states\n",
    "            h_0 = tf.zeros([1, self._num_units], self._dtype)\n",
    "            context_0 = self._compute_context(h_0)\n",
    "            h_0 = context_0 * 0\n",
    "            attn_state_0 = AttnState(cell_states, h_0, context_0)\n",
    "\n",
    "        return attn_state_0\n",
    "\n",
    "    def _compute_context(self, query):\n",
    "        \"\"\"\n",
    "        Compute attn scores and weighted sum of memory as the context\n",
    "            query: [batch_size, num_units]\n",
    "        Returns:\n",
    "            context: [batch_size, num_units]\n",
    "        \"\"\"\n",
    "        query = tf.expand_dims(query, -2)\n",
    "        Wq = tf.layers.dense(query, self.num_hidden, use_bias=False)\n",
    "        Wm = tf.layers.dense(self._memory, self.num_hidden, use_bias=False)\n",
    "        e = tf.layers.dense(tf.nn.tanh(Wm + Wq), 1, use_bias=False)\n",
    "        attn_scores = tf.expand_dims(tf.nn.softmax(tf.squeeze(e, axis=-1)), -1)\n",
    "\n",
    "        context = tf.reduce_sum(attn_scores * self._memory, axis=1)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def __call__(self, inputs, attn_states):\n",
    "        \"\"\"\n",
    "            inputs: emebeddings of previous word\n",
    "            states: (cell_states, outputs) at each step\n",
    "        \"\"\"\n",
    "        prev_cell_states, h, context = attn_states\n",
    "\n",
    "        x = tf.concat([inputs, h, context], axis=-1)\n",
    "        new_h, cell_states = self._cell.__call__(x, prev_cell_states)\n",
    "\n",
    "        new_context = self._compute_context(new_h)\n",
    "        new_attn_states = AttnState(cell_states, new_h, new_context)\n",
    "\n",
    "        return (new_h, new_attn_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECMWrapper(RNNCell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decoding\n",
    "\n",
    "class DecoderOutput(collections.namedtuple(\n",
    "                    \"DecoderOutput\", (\"logits\", \"ids\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, vocab_size]\n",
    "        ids: [batch_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class GreedyDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype):\n",
    "        self._embeddings = embeddings\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._dec_init_states\n",
    "\n",
    "        # initial cell inputs: tile [1, embed_dim] => [batch_size, embed_dim]\n",
    "        token = tf.expand_dims(self._start_token, 0)\n",
    "        inputs = tf.tile(token, multiples=[self._batch_size, 1])\n",
    "\n",
    "        # initial ending signals: start with all \"False\"\n",
    "        decode_finished = tf.zeros(shape=[self._batch_size], dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, cell_states, inputs, decode_finished):\n",
    "        # next step of rnn cell and pass the output_layer for logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # get ids of words predicted and their embeddings\n",
    "        new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32)\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        # make a new output for registering into TensorArrays\n",
    "        new_output = DecoderOutput(logits, new_ids)\n",
    "\n",
    "        # check whether the end_token is reached\n",
    "        new_decode_finished = tf.logical_or(decode_finished,\n",
    "                                            tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        return (new_output, new_cell_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Beam Search helpers ###\n",
    "def tile_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: batch-major, [batch_size, ...]\n",
    "    Returns:\n",
    "        tensor: beam_size tiled, [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    tensor = tf.expand_dims(tensor, axis=1)\n",
    "    # set multiples: [1, beam_size, 1, ..., 1]\n",
    "    multiples = [1 for i in range(tensor.shape.ndims)]\n",
    "    multiples[1] = beam_size\n",
    "\n",
    "    return tf.tile(tensor, multiples)\n",
    "\n",
    "\n",
    "def merge_batch_beam(tensor):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    Returns:\n",
    "        tensorL [batch_size * beam_size, ...]\n",
    "    \"\"\"\n",
    "    # tf.shape(t) handles indefinite shape\n",
    "    batch_size = tf.shape(tensor)[0]\n",
    "    # specified shape can be withdrawed right away\n",
    "    beam_size = tensor.shape[1].value\n",
    "\n",
    "    shape = list(tensor.shape)\n",
    "    shape.pop(0)\n",
    "    shape[0] = batch_size * beam_size\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def split_batch_beam(tensor, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size * beam_size, ...]\n",
    "    Returns:\n",
    "        tensor: [batch_size, beam_size, ...]\n",
    "    \"\"\"\n",
    "    shape = list(tensor.shape)\n",
    "    shape[0] = beam_size\n",
    "    shape.insert(0, -1)\n",
    "\n",
    "    return tf.reshape(tensor, shape)\n",
    "\n",
    "\n",
    "def mask_log_probs(log_probs, end_id, decode_finished):\n",
    "    \"\"\"\n",
    "    Set log_probs after end_token to be [-inf, 0, -inf, ...]\n",
    "        log_probs: [batch_size, beam_size, vocab_size]\n",
    "        decode_finished: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    vocab_size = log_probs.shape[-1].value\n",
    "    one_hot = tf.one_hot(end_id, vocab_size, on_value=0.0,\n",
    "                         off_value=log_probs.dtype.min,\n",
    "                         dtype=log_probs.dtype)\n",
    "    I_fin = tf.expand_dims(tf.cast(decode_finished, log_probs.dtype),\n",
    "                           axis=-1)\n",
    "\n",
    "    return (1. - I_fin) * log_probs + I_fin * one_hot\n",
    "\n",
    "\n",
    "def sample_bernoulli(prob, shape):\n",
    "    \"\"\"Samples a boolean tensor with shape = s according to bernouilli\"\"\"\n",
    "    return tf.greater(prob, tf.random_uniform(shape))\n",
    "\n",
    "\n",
    "def add_diversity_penalty(log_probs, div_gamma, div_prob, batch_size,\n",
    "                          beam_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Diversity penalty by Li et al. 2016\n",
    "        div_gamma: (float) diversity parameter\n",
    "        div_prob: adds penalty with div_proba\n",
    "    \"\"\"\n",
    "    if (div_gamma is None) or (div_prob is None):\n",
    "        return log_probs\n",
    "\n",
    "    if (div_gamma == 1) or (div_prob) == 0:\n",
    "        return log_probs\n",
    "\n",
    "    top_probs, top_inds = tf.nn.top_k(log_probs, k=vocab_size, sorted=True)\n",
    "\n",
    "    # inverse permutation to get rank of each entry\n",
    "    top_inds = tf.reshape(top_inds, [-1, vocab_size])\n",
    "    index_rank = tf.map_fn(tf.invert_permutation, top_inds, back_prop=False)\n",
    "    index_rank = tf.reshape(\n",
    "        index_rank, shape=[batch_size, beam_size, vocab_size])\n",
    "\n",
    "    # compute penalty\n",
    "    penalties = tf.log(div_gamma) * tf.cast(index_rank, log_probs.dtype)\n",
    "\n",
    "    # only apply penalty with some probability\n",
    "    apply_penalty = tf.cast(\n",
    "            sample_bernoulli(div_prob, [batch_size, beam_size, vocab_size]),\n",
    "            penalties.dtype)\n",
    "    penalties *= apply_penalty\n",
    "\n",
    "    return log_probs + penalties\n",
    "\n",
    "\n",
    "def gather_helper(tensor, indices, batch_size, beam_size):\n",
    "    \"\"\"\n",
    "        tensor: [batch_size, beam_size, d]\n",
    "        indices: [batch_size, beam_size]\n",
    "    Returns:\n",
    "        new_tensor: new_t[:, i] = t[:, new_parents[:, i]]\n",
    "    \"\"\"\n",
    "    range_ = tf.expand_dims(tf.range(batch_size) * beam_size, axis=1)\n",
    "    # flatten\n",
    "    indices = tf.reshape(indices + range_, [-1])\n",
    "    output = tf.gather(tf.reshape(tensor, [batch_size * beam_size, -1]),\n",
    "                       indices)\n",
    "\n",
    "    if tensor.shape.ndims == 2:\n",
    "        return tf.reshape(output, [batch_size, beam_size])\n",
    "\n",
    "    elif tensor.shape.ndims == 3:\n",
    "        d = tensor.shape[-1].value\n",
    "        return tf.reshape(output, [batch_size, beam_size, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "class BeamDecoderOutput(collections.namedtuple(\n",
    "        \"BeamDecoderOutput\", (\"logits\", \"ids\", \"parents\"))):\n",
    "    \"\"\"\n",
    "        logits: [batch_size, beam_size, vocab_size]\n",
    "        ids: [batch_size, beam_size], best words ids now\n",
    "        parents: [batch_size, beam_size], previous step beam index ids\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamDecoderCellStates(collections.namedtuple(\n",
    "        \"BeamDecoderCellStates\", (\"cell_states\", \"log_probs\"))):\n",
    "    \"\"\"\n",
    "        cell_states: [batch_size, beam_size, num_units]\n",
    "        log_probs: [batch_size, beam_size]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class BeamSearchDecodeCell(object):\n",
    "\n",
    "    def __init__(self, embeddings, cell, dec_init_states,\n",
    "                 output_layer, batch_size, dtype, beam_size,\n",
    "                 vocab_size, div_gamma=None, div_prob=None):\n",
    "        \"\"\"\n",
    "            div_gamma: (float) relative weight of penalties\n",
    "            div_prob: (float) prob to apply penalties\n",
    "        \"\"\"\n",
    "        self._embeddings = embeddings\n",
    "        self._vocab_size = vocab_size\n",
    "        self._cell = cell\n",
    "        self._dec_init_states = dec_init_states\n",
    "        self._output_layer = output_layer\n",
    "        self._batch_size = batch_size\n",
    "        self._start_token = tf.nn.embedding_lookup(embeddings, 0)\n",
    "        self._end_id = 1\n",
    "        self._dtype = dtype\n",
    "\n",
    "        self._beam_size = beam_size\n",
    "        self._div_gamma = div_gamma\n",
    "        self._div_prob = div_prob\n",
    "        if hasattr(self._cell, \"_memory\"):\n",
    "            indices = np.repeat(np.arange(self._batch_size), self._beam_size)\n",
    "            self._cell._memory = tf.gather(self._cell._memory, indices)\n",
    "\n",
    "    @property\n",
    "    def output_dtype(self):\n",
    "        \"\"\"Generate the structure for initial TensorArrays in dynamic_decode\"\"\"\n",
    "        return BeamDecoderOutput(logits=self._dtype,\n",
    "                                 ids=tf.int32, parents=tf.int32)\n",
    "\n",
    "    def _initial_state(self):\n",
    "        # t: [batch_size, num_units]\n",
    "        cell_states = nest.map_structure(\n",
    "            lambda t: tile_beam(t, self._beam_size), self._dec_init_states)\n",
    "\n",
    "        # another \"log_probs\" initial states: accumulative log_prob!\n",
    "        log_probs = tf.zeros([self._batch_size, self._beam_size],\n",
    "                             dtype=self._dtype)\n",
    "\n",
    "        return BeamDecoderCellStates(cell_states, log_probs)\n",
    "\n",
    "    def initialize(self):\n",
    "        # initial cell states\n",
    "        cell_states = self._initial_state()\n",
    "\n",
    "        # inputs: SOS, [embed_size] -> [batch_size, beam_size, embed_size]\n",
    "        inputs = tf.tile(tf.reshape(self._start_token, [1, 1, -1]),\n",
    "                         multiples=[self._batch_size, self._beam_size, 1])\n",
    "\n",
    "        # initial ending signals: [batch_size, beam_size]\n",
    "        decode_finished = tf.zeros([self._batch_size, self._beam_size],\n",
    "                                   dtype=tf.bool)\n",
    "\n",
    "        return cell_states, inputs, decode_finished\n",
    "\n",
    "    def step(self, time_index, beam_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            logits: [batch_size, beam_size, vocab_size]\n",
    "            ids: [batch_size, beam_size], best words ids now\n",
    "            parents: [batch_size, beam_size], previous step beam index ids\n",
    "        \"\"\"\n",
    "        # 1-1: merge batch -> [batch_size*beam_size, ...]\n",
    "        cell_states = nest.map_structure(\n",
    "            merge_batch_beam, beam_states.cell_states)\n",
    "        inputs = merge_batch_beam(inputs)\n",
    "\n",
    "        # 1-2: perform cell ops to get new logits\n",
    "        new_h, new_cell_states = self._cell.__call__(inputs, cell_states)\n",
    "        logits = self._output_layer(new_h)\n",
    "\n",
    "        # 1-3: split batch beam -> [batch_size, beam_size, ...]\n",
    "        logits = split_batch_beam(logits, self._beam_size)\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: split_batch_beam(t, self._beam_size), new_cell_states)\n",
    "\n",
    "        # 2-1: compute log_probs, [batch_size, beam_size, vocab_size]\n",
    "        step_log_probs = tf.nn.log_softmax(logits)\n",
    "        step_log_probs = mask_log_probs(\n",
    "            step_log_probs, self._end_id, decode_finished)\n",
    "\n",
    "        # 2-2: add cumulative log_probs and \"diversity penalty\"\n",
    "        log_probs = tf.expand_dims(beam_states.log_probs, axis=-1)\n",
    "        log_probs = log_probs + step_log_probs\n",
    "        log_probs = add_diversity_penalty(log_probs, self._div_gamma,\n",
    "                                          self._div_prob, self._batch_size,\n",
    "                                          self._beam_size, self._vocab_size)\n",
    "\n",
    "        # 3-1: flatten, if time_index = 0, consider only one beam\n",
    "        # log_probs[:, 0]: [batch_size, vocab_size]\n",
    "        shape = [self._batch_size, self._beam_size * self._vocab_size]\n",
    "        log_probs_flat = tf.reshape(log_probs, shape)\n",
    "        log_probs_flat = tf.cond(time_index > 0, lambda: log_probs_flat,\n",
    "                                 lambda: log_probs[:, 0])\n",
    "\n",
    "        # 3-2: compute the top (beam_size) beams, [batch_size, beam_size]\n",
    "        new_log_probs, indices = tf.nn.top_k(log_probs_flat, self._beam_size)\n",
    "\n",
    "        # 3-3: obtain ids and parent beams\n",
    "        new_ids = indices % self._vocab_size\n",
    "        # //: floor division, know which beam it belongs to\n",
    "        new_parents = indices // self._vocab_size\n",
    "\n",
    "        # 4-1: compute new states\n",
    "        new_inputs = tf.nn.embedding_lookup(self._embeddings, new_ids)\n",
    "\n",
    "        decode_finished = gather_helper(\n",
    "            decode_finished, new_parents, self._batch_size, self._beam_size)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            decode_finished, tf.equal(new_ids, self._end_id))\n",
    "\n",
    "        new_cell_states = nest.map_structure(\n",
    "            lambda t: gather_helper(t, new_parents, self._batch_size,\n",
    "                                    self._beam_size), new_cell_states)\n",
    "\n",
    "        # 4-2: create new state and output of decoder\n",
    "        new_beam_states = BeamDecoderCellStates(cell_states=new_cell_states,\n",
    "                                                log_probs=new_log_probs)\n",
    "        new_output = BeamDecoderOutput(logits=logits, ids=new_ids,\n",
    "                                       parents=new_parents)\n",
    "\n",
    "        return (new_output, new_beam_states, new_inputs, new_decode_finished)\n",
    "\n",
    "    def finalize(self, final_outputs, final_cell_states):\n",
    "        \"\"\"\n",
    "            final_outputs: [max_time, logits] structure of tensor\n",
    "            final_cell_states: BeamDecoderCellStates\n",
    "        Returns:\n",
    "            [max_time, batch_size, beam_size, ] stucture of tensor\n",
    "        \"\"\"\n",
    "        # reverse the time dimension\n",
    "        max_iter = tf.shape(final_outputs.ids)[0]\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        # initial states\n",
    "        def create_ta(d):\n",
    "            return tf.TensorArray(dtype=d, size=max_iter)\n",
    "\n",
    "        f_time_index = tf.constant(0, dtype=tf.int32)\n",
    "        # final output dtype\n",
    "        final_dtype = DecoderOutput(logits=self._dtype, ids=tf.int32)\n",
    "        f_output_ta = nest.map_structure(create_ta, final_dtype)\n",
    "\n",
    "        # initial parents: [batch_size, beam_size]\n",
    "        f_parents = tf.tile(\n",
    "            tf.expand_dims(tf.range(self._beam_size), axis=0),\n",
    "            multiples=[self._batch_size, 1])\n",
    "\n",
    "        def condition(f_time_index, output_ta, f_parents):\n",
    "            return tf.less(f_time_index, max_iter)\n",
    "\n",
    "        def body(f_time_index, output_ta, f_parents):\n",
    "            # get ids, logits and parents predicted at this time step\n",
    "            input_t = nest.map_structure(lambda t: t[f_time_index],\n",
    "                                         final_outputs)\n",
    "\n",
    "            # parents: reversed version shows the next position to go\n",
    "            new_beam_state = nest.map_structure(\n",
    "                lambda t: gather_helper(t, f_parents, self._batch_size,\n",
    "                                        self._beam_size),\n",
    "                input_t)\n",
    "\n",
    "            # create new output\n",
    "            new_output = DecoderOutput(logits=new_beam_state.logits,\n",
    "                                       ids=new_beam_state.ids)\n",
    "\n",
    "            # write beam ids\n",
    "            output_ta = nest.map_structure(\n",
    "                lambda ta, out: ta.write(f_time_index, out),\n",
    "                output_ta, new_output)\n",
    "\n",
    "            return (f_time_index + 1), output_ta, input_t.parents\n",
    "\n",
    "        with tf.variable_scope(\"beam_search_decoding\"):\n",
    "            res = tf.while_loop(\n",
    "                    condition,\n",
    "                    body,\n",
    "                    loop_vars=[f_time_index, f_output_ta, f_parents],\n",
    "                    back_prop=False)\n",
    "\n",
    "        # stack the structure and reverse back\n",
    "        final_outputs = nest.map_structure(lambda ta: ta.stack(), res[1])\n",
    "        final_outputs = nest.map_structure(lambda t: tf.reverse(t, axis=[0]),\n",
    "                                           final_outputs)\n",
    "\n",
    "        return DecoderOutput(logits=final_outputs.logits,\n",
    "                             ids=final_outputs.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic decode function\n",
    "from tensorflow.contrib.framework import nest\n",
    "\n",
    "\n",
    "def transpose_batch_time(tensor):\n",
    "    ndims = tensor.shape.ndims\n",
    "    if ndims == 2:\n",
    "        return tf.transpose(tensor, [1, 0])\n",
    "\n",
    "    elif ndims == 3:\n",
    "        return tf.transpose(tensor, [1, 0, 2])\n",
    "\n",
    "    else:\n",
    "        return tf.transpose(tensor, [1, 0, 2, 3])\n",
    "\n",
    "\n",
    "# Dynamic decode function\n",
    "def dynamic_decode(decoder_cell, max_iter):\n",
    "    max_iter = tf.convert_to_tensor(max_iter, dtype=tf.int32)\n",
    "\n",
    "    # TensorArray: wrap dynamic-sized, per-time-step, write-once Tensor arrays\n",
    "    def create_tensor_array(d):\n",
    "        # initial size = 0\n",
    "        return tf.TensorArray(dtype=d, size=0, dynamic_size=True)\n",
    "\n",
    "    time_index = tf.constant(0, dtype=tf.int32)\n",
    "    # nest.map_structure: applies func to each entry in structure\n",
    "    output_tensor_arrays = nest.map_structure(\n",
    "        create_tensor_array, decoder_cell.output_dtype)\n",
    "\n",
    "    cell_states, inputs, decode_finished = decoder_cell.initialize()\n",
    "\n",
    "    # tf.while_loop(cond, body, vars): Repeat body while condition cond is true\n",
    "    def condition(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        \"\"\"\n",
    "            if all \"decode_finished\" are True, return \"False\"\n",
    "        \"\"\"\n",
    "        return tf.logical_not(tf.reduce_all(decode_finished))\n",
    "\n",
    "    def body(time_index, output_ta, cell_states, inputs, decode_finished):\n",
    "        sts = decoder_cell.step(time_index, cell_states, inputs,\n",
    "                                decode_finished)\n",
    "        new_output, new_cell_states, new_inputs, new_decode_finished = sts\n",
    "\n",
    "        # TensorArray.write(index, value): register value and returns new TAs\n",
    "        output_ta = nest.map_structure(\n",
    "            lambda ta, out: ta.write(time_index, out),\n",
    "            output_ta, new_output)\n",
    "\n",
    "        new_decode_finished = tf.logical_or(\n",
    "            tf.greater_equal(time_index, max_iter),\n",
    "            new_decode_finished)\n",
    "\n",
    "        return (time_index + 1, output_ta, new_cell_states, new_inputs,\n",
    "                new_decode_finished)\n",
    "\n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "\n",
    "        res = tf.while_loop(\n",
    "            condition,\n",
    "            body,\n",
    "            loop_vars=[time_index, output_tensor_arrays, cell_states,\n",
    "                       inputs, decode_finished],\n",
    "            back_prop=False)\n",
    "\n",
    "    # get final outputs and states\n",
    "    final_output_ta, final_cell_states = res[1], res[2]\n",
    "\n",
    "    # TA.stack(): stack all tensors in TensorArray, [max_iter+1, batch_size, _]\n",
    "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_output_ta)\n",
    "\n",
    "    # finalize the computation from the decoder cell\n",
    "    final_outputs = decoder_cell.finalize(final_outputs, final_cell_states)\n",
    "\n",
    "    # transpose the final output\n",
    "    final_outputs = nest.map_structure(transpose_batch_time, final_outputs)\n",
    "\n",
    "    return final_outputs, final_cell_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                  num_layers, num_units, cell_type,\n",
    "                  state_pass=True, infer_batch_size=None,\n",
    "                  attention_wrap=None, attn_num_units=128,\n",
    "                  target_ids=None, infer_type=\"greedy\", beam_size=None,\n",
    "                  max_iter=20, time_major=False, dtype=tf.float32,\n",
    "                  forget_bias=1.0, name=\"decoder\"):\n",
    "    \"\"\"\n",
    "    decoder: build rnn decoder with attention and\n",
    "        target_ids: [batch_size, max_time]\n",
    "        mode: train or infer\n",
    "        infer_type: greedy decode or beam search\n",
    "        attention_wrap: a wrapper to enable attention mechanism\n",
    "\n",
    "    Returns:\n",
    "        train_outputs: logits, [batch_size, max_time, vocab_size]\n",
    "        infer_outputs: namedtuple(logits, ids), [batch_size, max_time, d]\n",
    "    \"\"\"\n",
    "    # parameter checking\n",
    "    if infer_batch_size is None:\n",
    "        txt = \"infer_batch_size not specified, infer output will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "    elif infer_type == \"beam_search\" and beam_size is None:\n",
    "        raise ValueError(\"Inference by beam search must specify beam_size.\")\n",
    "\n",
    "    if target_ids is None:\n",
    "        txt = \"target_ids not specified, train_outputs will be 'None'.\"\n",
    "        warnings.warn(txt)\n",
    "\n",
    "    # Build decoder\n",
    "    with tf.variable_scope(name):\n",
    "        vocab_size = embeddings.shape[0].value\n",
    "\n",
    "        # decoder rnn_cell\n",
    "        cell = build_rnn_cell(num_layers, num_units, cell_type, forget_bias)\n",
    "        dec_init_states = encoder_states if state_pass else None\n",
    "        output_layer = tf.layers.Dense(\n",
    "            vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        # wrap with attention\n",
    "        if attention_wrap is not None:\n",
    "            # need batch-major\n",
    "            if time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "\n",
    "            cell = attention_wrap(\n",
    "                cell, memory, dec_init_states, attn_num_units,\n",
    "                num_units, dtype)\n",
    "\n",
    "            dec_init_states = cell.initial_state()\n",
    "\n",
    "        # Decode - for training\n",
    "        # pad the token sequences with SOS (Start of Sentence)\n",
    "        train_outputs = None\n",
    "        if target_ids is not None:\n",
    "            input_ids = tf.pad(target_ids, [[0, 0], [1, 0]], constant_values=0)\n",
    "            embed_inputs = tf.nn.embedding_lookup(embeddings, input_ids)\n",
    "\n",
    "            decoder_outputs, decoder_states = tf.nn.dynamic_rnn(\n",
    "                cell, embed_inputs,\n",
    "                initial_state=dec_init_states,\n",
    "                dtype=dtype,\n",
    "                swap_memory=True)\n",
    "\n",
    "            # logits\n",
    "            train_outputs = output_layer(decoder_outputs)\n",
    "\n",
    "        # Decode - for inference\n",
    "        infer_outputs = None\n",
    "        if infer_batch_size is not None:\n",
    "            if dec_init_states is None:\n",
    "                dec_init_states = cell.zero_state(infer_batch_size, dtype)\n",
    "\n",
    "            if infer_type == \"beam_search\":\n",
    "                decoder_cell = BeamSearchDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype, beam_size, vocab_size,\n",
    "                    div_gamma=None, div_prob=None)\n",
    "\n",
    "            else:\n",
    "                decoder_cell = GreedyDecodeCell(\n",
    "                    embeddings, cell, dec_init_states, output_layer,\n",
    "                    infer_batch_size, dtype)\n",
    "\n",
    "            # namedtuple(logits, ids)\n",
    "            infer_outputs, _ = dynamic_decode(decoder_cell, max_iter)\n",
    "\n",
    "    return train_outputs, infer_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlai/Desktop/ECE276C_RL/env3/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: infer_batch_size not specified, infer output will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "target_ids = tf.placeholder(tf.int64, [None, None])\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                       num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                       state_pass=False,\n",
    "                       attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                       target_ids=target_ids, name=\"decoder4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00099691, 0.00099768, 0.00099618, ..., 0.00099778,\n",
       "         0.00099546, 0.0009989 ],\n",
       "        [0.00099634, 0.00099817, 0.00099603, ..., 0.00099829,\n",
       "         0.00099445, 0.00099971],\n",
       "        [0.00099569, 0.00099853, 0.00099628, ..., 0.00099856,\n",
       "         0.0009938 , 0.00099992],\n",
       "        [0.00099515, 0.00099879, 0.00099676, ..., 0.00099867,\n",
       "         0.00099337, 0.00099979]],\n",
       "\n",
       "       [[0.00099691, 0.00099768, 0.00099618, ..., 0.00099778,\n",
       "         0.00099546, 0.0009989 ],\n",
       "        [0.00099641, 0.00099817, 0.00099599, ..., 0.00099828,\n",
       "         0.00099437, 0.00099979],\n",
       "        [0.00099673, 0.00099839, 0.00099607, ..., 0.00099819,\n",
       "         0.00099344, 0.00100022],\n",
       "        [0.00099721, 0.00099773, 0.00099628, ..., 0.00099831,\n",
       "         0.00099357, 0.00099972]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "results = sess.run(tf.nn.softmax(logits), feed_dict={source_ids: [[3, 3, 3], [4, 5, 6]],\n",
    "                                                     target_ids: [[8, 8, 8], [8, 10, 12]]})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlai/Desktop/ECE276C_RL/env3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: target_ids not specified, train_outputs will be 'None'.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "logits, infer_outputs = build_decoder(encoder_outputs, encoder_states, embeddings,\n",
    "                        num_layers=2, num_units=256, cell_type=\"LSTM\",\n",
    "                        state_pass=True, infer_batch_size=3,\n",
    "                        attention_wrap=AttentionWrapper, attn_num_units=128,\n",
    "                        infer_type=\"beam_search\", beam_size=5,\n",
    "                        max_iter=10, name=\"a4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(logits=array([[[[ 1.46219484e-03,  5.12904604e-04,  2.94958765e-04, ...,\n",
       "           4.17573145e-03, -1.71831273e-03,  2.39557121e-03],\n",
       "         [ 1.46219484e-03,  5.12904604e-04,  2.94958765e-04, ...,\n",
       "           4.17573145e-03, -1.71831273e-03,  2.39557121e-03],\n",
       "         [ 1.46219484e-03,  5.12904604e-04,  2.94958765e-04, ...,\n",
       "           4.17573145e-03, -1.71831273e-03,  2.39557121e-03],\n",
       "         [ 1.46219484e-03,  5.12904604e-04,  2.94958765e-04, ...,\n",
       "           4.17573145e-03, -1.71831273e-03,  2.39557121e-03],\n",
       "         [ 1.46219484e-03,  5.12904604e-04,  2.94958765e-04, ...,\n",
       "           4.17573145e-03, -1.71831273e-03,  2.39557121e-03]],\n",
       "\n",
       "        [[ 2.47002440e-03, -6.38919009e-05, -5.78964653e-04, ...,\n",
       "           4.05973848e-03, -2.42886506e-03,  1.15140039e-03],\n",
       "         [ 2.47002440e-03, -6.38919009e-05, -5.78964653e-04, ...,\n",
       "           4.05973848e-03, -2.42886506e-03,  1.15140039e-03],\n",
       "         [ 2.47002440e-03, -6.38919009e-05, -5.78964653e-04, ...,\n",
       "           4.05973848e-03, -2.42886506e-03,  1.15140039e-03],\n",
       "         [ 2.47002440e-03, -6.38919009e-05, -5.78964653e-04, ...,\n",
       "           4.05973848e-03, -2.42886506e-03,  1.15140039e-03],\n",
       "         [ 2.47002440e-03, -6.38919009e-05, -5.78964653e-04, ...,\n",
       "           4.05973848e-03, -2.42886506e-03,  1.15140039e-03]],\n",
       "\n",
       "        [[ 3.28163430e-03, -6.66712702e-04, -9.65652056e-04, ...,\n",
       "           3.27688991e-03, -2.83154380e-03,  3.29964096e-04],\n",
       "         [ 3.22549208e-03, -6.70180365e-04, -1.14609522e-03, ...,\n",
       "           3.38085950e-03, -3.03816469e-03,  7.96711538e-06],\n",
       "         [ 3.32099735e-03, -4.80660237e-04, -1.03785726e-03, ...,\n",
       "           3.40234442e-03, -2.81368080e-03,  1.95570174e-04],\n",
       "         [ 2.55582528e-03, -1.27734069e-03, -2.00887187e-03, ...,\n",
       "           3.64020304e-03, -2.89012352e-03,  1.20877172e-04],\n",
       "         [ 3.28163430e-03, -6.66712702e-04, -9.65652056e-04, ...,\n",
       "           3.27688991e-03, -2.83154380e-03,  3.29964096e-04]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.79681804e-03, -1.10918982e-03, -3.22560803e-03, ...,\n",
       "           2.21931352e-03, -2.80039292e-03, -2.08780589e-03],\n",
       "         [ 2.79681804e-03, -1.10918982e-03, -3.22560803e-03, ...,\n",
       "           2.21931352e-03, -2.80039292e-03, -2.08780589e-03],\n",
       "         [ 2.64674518e-03, -2.14284589e-03, -3.34851397e-03, ...,\n",
       "           2.30400404e-03, -2.69705616e-03, -2.56428192e-03],\n",
       "         [ 2.79681804e-03, -1.10918982e-03, -3.22560803e-03, ...,\n",
       "           2.21931352e-03, -2.80039292e-03, -2.08780589e-03],\n",
       "         [ 2.79681804e-03, -1.10918982e-03, -3.22560803e-03, ...,\n",
       "           2.21931352e-03, -2.80039292e-03, -2.08780589e-03]],\n",
       "\n",
       "        [[ 2.61119148e-03, -1.41576631e-03, -3.43624689e-03, ...,\n",
       "           2.13103672e-03, -2.76534748e-03, -2.00212351e-03],\n",
       "         [ 2.61119148e-03, -1.41576631e-03, -3.43624689e-03, ...,\n",
       "           2.13103672e-03, -2.76534748e-03, -2.00212351e-03],\n",
       "         [ 2.61119148e-03, -1.41576631e-03, -3.43624689e-03, ...,\n",
       "           2.13103672e-03, -2.76534748e-03, -2.00212351e-03],\n",
       "         [ 2.61119148e-03, -1.41576631e-03, -3.43624689e-03, ...,\n",
       "           2.13103672e-03, -2.76534748e-03, -2.00212351e-03],\n",
       "         [ 2.61119148e-03, -1.41576631e-03, -3.43624689e-03, ...,\n",
       "           2.13103672e-03, -2.76534748e-03, -2.00212351e-03]],\n",
       "\n",
       "        [[ 2.43658549e-03, -1.83838466e-03, -3.60653060e-03, ...,\n",
       "           2.08696537e-03, -2.76098633e-03, -1.89296191e-03],\n",
       "         [ 2.32719211e-03, -1.64199620e-03, -3.47152050e-03, ...,\n",
       "           2.37430469e-03, -2.70587765e-03, -1.78841618e-03],\n",
       "         [ 2.28063762e-03, -2.93189730e-03, -3.62830213e-03, ...,\n",
       "           2.12361920e-03, -2.68262369e-03, -2.37511192e-03],\n",
       "         [ 2.24381965e-03, -1.44850707e-03, -2.92259874e-03, ...,\n",
       "           2.65909988e-03, -2.93846289e-03, -1.24291913e-03],\n",
       "         [ 2.24232394e-03, -2.47090915e-03, -4.22336254e-03, ...,\n",
       "           2.35620793e-03, -3.03729670e-03, -2.05416023e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 5.10351080e-03,  2.24023592e-03,  1.45751191e-03, ...,\n",
       "           1.67925213e-03, -1.95419276e-03,  1.22887385e-03],\n",
       "         [ 5.10351080e-03,  2.24023592e-03,  1.45751191e-03, ...,\n",
       "           1.67925213e-03, -1.95419276e-03,  1.22887385e-03],\n",
       "         [ 5.10351080e-03,  2.24023592e-03,  1.45751191e-03, ...,\n",
       "           1.67925213e-03, -1.95419276e-03,  1.22887385e-03],\n",
       "         [ 5.10351080e-03,  2.24023592e-03,  1.45751191e-03, ...,\n",
       "           1.67925213e-03, -1.95419276e-03,  1.22887385e-03],\n",
       "         [ 5.10351080e-03,  2.24023592e-03,  1.45751191e-03, ...,\n",
       "           1.67925213e-03, -1.95419276e-03,  1.22887385e-03]],\n",
       "\n",
       "        [[ 5.85220009e-03,  1.96624408e-03,  2.22315453e-03, ...,\n",
       "           1.93549902e-03, -2.91965995e-03,  1.77559943e-03],\n",
       "         [ 5.85220009e-03,  1.96624408e-03,  2.22315453e-03, ...,\n",
       "           1.93549902e-03, -2.91965995e-03,  1.77559943e-03],\n",
       "         [ 5.85220009e-03,  1.96624408e-03,  2.22315453e-03, ...,\n",
       "           1.93549902e-03, -2.91965995e-03,  1.77559943e-03],\n",
       "         [ 5.85220009e-03,  1.96624408e-03,  2.22315453e-03, ...,\n",
       "           1.93549902e-03, -2.91965995e-03,  1.77559943e-03],\n",
       "         [ 5.85220009e-03,  1.96624408e-03,  2.22315453e-03, ...,\n",
       "           1.93549902e-03, -2.91965995e-03,  1.77559943e-03]],\n",
       "\n",
       "        [[ 9.77880880e-03,  2.09977245e-03,  8.06103111e-04, ...,\n",
       "           1.73004949e-03, -4.91962908e-03,  2.10812408e-03],\n",
       "         [ 9.41574760e-03,  2.28031073e-03,  1.16491057e-04, ...,\n",
       "           2.45907251e-03, -5.14651276e-03,  2.07401696e-03],\n",
       "         [ 1.00238025e-02,  2.63077673e-03,  4.78436414e-04, ...,\n",
       "           1.42275263e-03, -4.50004730e-03,  1.48073258e-03],\n",
       "         [ 1.18472921e-02,  3.61115881e-03, -3.72974377e-04, ...,\n",
       "           2.33464409e-03, -5.40287979e-03,  1.40051579e-03],\n",
       "         [ 9.41574760e-03,  2.28031073e-03,  1.16491057e-04, ...,\n",
       "           2.45907251e-03, -5.14651276e-03,  2.07401696e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.30412430e-02, -1.84726540e-03, -4.83921380e-04, ...,\n",
       "           3.32737016e-03, -1.19831720e-02,  5.13479766e-03],\n",
       "         [ 1.25274118e-02, -9.34456184e-04, -3.15871905e-04, ...,\n",
       "           4.08955943e-03, -1.16439704e-02,  4.35170904e-03],\n",
       "         [ 1.19152432e-02,  1.30756234e-04, -1.14109178e-04, ...,\n",
       "           4.94729262e-03, -1.11552579e-02,  3.41023924e-03],\n",
       "         [ 1.32368030e-02, -7.32476532e-04, -5.89722185e-04, ...,\n",
       "           4.68992256e-03, -1.13444515e-02,  4.81309928e-03],\n",
       "         [ 1.33613860e-02, -2.41827359e-03, -5.87064831e-04, ...,\n",
       "           2.81434646e-03, -1.21470429e-02,  5.63476328e-03]],\n",
       "\n",
       "        [[ 1.05245607e-02, -6.18159829e-04,  3.46708315e-04, ...,\n",
       "           4.86079790e-03, -1.15190251e-02,  4.83904127e-03],\n",
       "         [ 9.87211615e-03,  4.87870158e-04,  5.54592290e-04, ...,\n",
       "           5.72508341e-03, -1.09002180e-02,  3.81974922e-03],\n",
       "         [ 1.11419298e-02, -1.68483122e-03,  1.59290707e-04, ...,\n",
       "           4.00464935e-03, -1.20128188e-02,  5.77186840e-03],\n",
       "         [ 1.12000490e-02, -4.19412740e-04,  7.98084366e-05, ...,\n",
       "           5.52151818e-03, -1.12126432e-02,  5.29464707e-03],\n",
       "         [ 1.05245607e-02, -6.18159829e-04,  3.46708315e-04, ...,\n",
       "           4.86079790e-03, -1.15190251e-02,  4.83904127e-03]],\n",
       "\n",
       "        [[ 7.92786758e-03,  1.10841740e-03,  1.07071537e-03, ...,\n",
       "           6.38786145e-03, -1.04021095e-02,  4.09053080e-03],\n",
       "         [ 8.57461616e-03,  2.44627499e-05,  8.56895756e-04, ...,\n",
       "           5.56551665e-03, -1.11289537e-02,  5.14451880e-03],\n",
       "         [ 9.23239905e-03, -1.08362432e-03,  6.62858656e-04, ...,\n",
       "           4.70286794e-03, -1.17531875e-02,  6.15577679e-03],\n",
       "         [ 9.13558435e-03,  1.74749366e-04,  6.13948388e-04, ...,\n",
       "           6.21045334e-03, -1.08749177e-02,  5.54807018e-03],\n",
       "         [ 9.85395722e-03, -2.15034885e-03,  4.87539393e-04, ...,\n",
       "           3.84609168e-03, -1.22505631e-02,  7.08158454e-03]]],\n",
       "\n",
       "\n",
       "       [[[ 7.82407005e-04,  6.16638572e-04, -3.85552901e-03, ...,\n",
       "           2.71060970e-04, -2.68405443e-03, -1.24537991e-03],\n",
       "         [ 7.82406656e-04,  6.16638339e-04, -3.85552901e-03, ...,\n",
       "           2.71061406e-04, -2.68405327e-03, -1.24538015e-03],\n",
       "         [ 7.82407005e-04,  6.16638572e-04, -3.85552901e-03, ...,\n",
       "           2.71060970e-04, -2.68405443e-03, -1.24537991e-03],\n",
       "         [ 7.82406656e-04,  6.16638339e-04, -3.85552901e-03, ...,\n",
       "           2.71061406e-04, -2.68405327e-03, -1.24538015e-03],\n",
       "         [ 7.82406656e-04,  6.16638339e-04, -3.85552901e-03, ...,\n",
       "           2.71061406e-04, -2.68405327e-03, -1.24538015e-03]],\n",
       "\n",
       "        [[ 2.97680171e-03, -6.03789464e-04, -4.22546221e-03, ...,\n",
       "           9.32472991e-04, -4.12712013e-03,  2.62222020e-04],\n",
       "         [ 3.06307408e-03, -2.62631977e-04, -3.54783190e-03, ...,\n",
       "           1.18668913e-03, -4.70959721e-03,  9.22504114e-04],\n",
       "         [ 3.06307408e-03, -2.62631977e-04, -3.54783190e-03, ...,\n",
       "           1.18668913e-03, -4.70959721e-03,  9.22504114e-04],\n",
       "         [ 2.74589146e-03, -8.35425279e-04, -4.17642947e-03, ...,\n",
       "           5.17766981e-04, -4.45020432e-03,  3.55987024e-04],\n",
       "         [ 2.78415019e-03, -2.46480078e-04, -3.38403229e-03, ...,\n",
       "           1.13795383e-03, -4.79735900e-03,  9.46691260e-04]],\n",
       "\n",
       "        [[ 3.97149147e-03, -5.80455700e-04, -3.74348066e-03, ...,\n",
       "           1.53735839e-03, -5.04700374e-03,  1.57634704e-03],\n",
       "         [ 3.50857945e-03, -1.09550101e-03, -3.62240896e-03, ...,\n",
       "           9.12043732e-04, -5.58581017e-03,  1.74705568e-03],\n",
       "         [ 3.44219920e-03, -1.07593858e-03, -3.28811049e-03, ...,\n",
       "           2.09969492e-03, -6.26706891e-03,  2.63442728e-03],\n",
       "         [ 3.75810149e-03, -1.00124301e-03, -3.67198326e-03, ...,\n",
       "           2.44980259e-03, -6.16405485e-03,  2.48510181e-03],\n",
       "         [ 3.50857945e-03, -1.09550101e-03, -3.62240896e-03, ...,\n",
       "           9.12043732e-04, -5.58581017e-03,  1.74705568e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 6.99653337e-03,  4.77198185e-03, -7.20930519e-04, ...,\n",
       "          -5.33773471e-03, -2.59669870e-03,  4.04934026e-03],\n",
       "         [ 1.80812716e-03,  1.13239721e-03,  2.33853539e-03, ...,\n",
       "           2.44524051e-03, -2.75937840e-03,  3.50393285e-03],\n",
       "         [ 6.99653337e-03,  4.77198185e-03, -7.20930519e-04, ...,\n",
       "          -5.33773471e-03, -2.59669870e-03,  4.04934026e-03],\n",
       "         [ 1.80812716e-03,  1.13239721e-03,  2.33853539e-03, ...,\n",
       "           2.44524051e-03, -2.75937840e-03,  3.50393285e-03],\n",
       "         [ 7.66050676e-03,  6.06781524e-03, -4.19405929e-04, ...,\n",
       "          -5.70736360e-03, -2.17172271e-03,  3.48562142e-03]],\n",
       "\n",
       "        [[ 6.33773068e-03,  4.46365168e-03, -2.17019551e-05, ...,\n",
       "          -5.45810396e-03, -2.73082289e-03,  4.28893883e-03],\n",
       "         [ 7.48912524e-03,  6.44874061e-03,  3.62169114e-04, ...,\n",
       "          -6.07000198e-03, -2.09293840e-03,  3.54451337e-03],\n",
       "         [ 7.48912524e-03,  6.44874061e-03,  3.62169114e-04, ...,\n",
       "          -6.07000198e-03, -2.09293840e-03,  3.54451337e-03],\n",
       "         [ 7.21352035e-03,  5.81248058e-03, -2.32945604e-04, ...,\n",
       "          -6.59608282e-03, -2.24938430e-03,  4.25903220e-03],\n",
       "         [ 6.06362382e-03,  3.82721797e-03, -6.15488272e-04, ...,\n",
       "          -5.98640135e-03, -2.87987199e-03,  5.00312867e-03]],\n",
       "\n",
       "        [[ 6.46256935e-03,  5.39881457e-03,  4.30219661e-04, ...,\n",
       "          -6.59475429e-03, -2.50537833e-03,  4.49961936e-03],\n",
       "         [ 6.62174402e-03,  5.75498166e-03,  6.07912254e-04, ...,\n",
       "          -6.42491644e-03, -2.33215187e-03,  4.15852200e-03],\n",
       "         [ 6.18807971e-03,  4.76478552e-03, -1.63980760e-04, ...,\n",
       "          -7.12218508e-03, -2.65404116e-03,  5.21202665e-03],\n",
       "         [ 6.89942855e-03,  6.39927806e-03,  1.20594678e-03, ...,\n",
       "          -5.89336921e-03, -2.17400468e-03,  3.44524789e-03],\n",
       "         [ 5.02596097e-03,  3.10886651e-03,  4.12194931e-05, ...,\n",
       "          -5.89833083e-03, -3.18484730e-03,  5.19879395e-03]]]],\n",
       "      dtype=float32), ids=array([[[739, 516, 594, 739, 110],\n",
       "        [739, 739, 739, 739, 739],\n",
       "        [739, 882, 883, 290, 739],\n",
       "        [882, 882, 882, 883, 882],\n",
       "        [882, 882, 882, 882, 290],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 494, 924, 329, 895]],\n",
       "\n",
       "       [[  0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0],\n",
       "        [  0, 318, 800, 671, 318],\n",
       "        [318, 318, 318, 318, 318],\n",
       "        [318, 318, 318, 800, 318],\n",
       "        [318, 318, 318, 871, 871],\n",
       "        [871, 318, 871, 768, 318],\n",
       "        [871, 871, 871, 871, 318],\n",
       "        [871, 871, 871, 871, 871],\n",
       "        [871, 871, 871, 871, 871],\n",
       "        [871, 871, 871, 871, 721]],\n",
       "\n",
       "       [[295, 829, 750, 447, 556],\n",
       "        [447, 447, 447, 447, 671],\n",
       "        [725, 447, 422, 671, 447],\n",
       "        [725, 422, 725, 725, 671],\n",
       "        [725, 871, 725, 725, 725],\n",
       "        [725, 871, 725, 725, 725],\n",
       "        [871, 725, 725, 725, 725],\n",
       "        [725, 725, 456, 456, 871],\n",
       "        [456, 669, 456, 669, 456],\n",
       "        [456, 669, 669, 456, 669],\n",
       "        [669, 456, 669, 456, 456]]], dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "inf_results = sess.run(infer_outputs, feed_dict={source_ids: [[3, 3, 3], [4, 5, 6], [10, 10, 10]]})\n",
    "inf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[739, 516, 594, 739, 110],\n",
       "        [739, 739, 739, 739, 739],\n",
       "        [739, 882, 883, 290, 739],\n",
       "        [882, 882, 882, 883, 882],\n",
       "        [882, 882, 882, 882, 290],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 882, 882, 882, 882],\n",
       "        [882, 494, 924, 329, 895]],\n",
       "\n",
       "       [[  0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0],\n",
       "        [  0, 318, 800, 671, 318],\n",
       "        [318, 318, 318, 318, 318],\n",
       "        [318, 318, 318, 800, 318],\n",
       "        [318, 318, 318, 871, 871],\n",
       "        [871, 318, 871, 768, 318],\n",
       "        [871, 871, 871, 871, 318],\n",
       "        [871, 871, 871, 871, 871],\n",
       "        [871, 871, 871, 871, 871],\n",
       "        [871, 871, 871, 871, 721]],\n",
       "\n",
       "       [[295, 829, 750, 447, 556],\n",
       "        [447, 447, 447, 447, 671],\n",
       "        [725, 447, 422, 671, 447],\n",
       "        [725, 422, 725, 725, 671],\n",
       "        [725, 871, 725, 725, 725],\n",
       "        [725, 871, 725, 725, 725],\n",
       "        [871, 725, 725, 725, 725],\n",
       "        [725, 725, 456, 456, 871],\n",
       "        [456, 669, 456, 669, 456],\n",
       "        [456, 669, 669, 456, 669],\n",
       "        [669, 456, 669, 456, 456]]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_results.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "def compute_loss(source_ids, target_ids, sequence_mask, embeddings,\n",
    "                 enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "                 dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "                 infer_batch_size, infer_type=\"greedy\", beam_size=None, max_iter=20,\n",
    "                 attn_wrapper=None, attn_num_units=128, l2_regularize=None,\n",
    "                 name=\"Seq2seq\"):\n",
    "    \"\"\"\n",
    "    Creates a Seq2seq model and returns cross entropy loss.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        # build encoder\n",
    "        encoder_outputs, encoder_states = build_encoder(\n",
    "            embeddings, source_ids, enc_num_layers, enc_num_units,\n",
    "            enc_cell_type, bidir=enc_bidir, name=\"%s_encoder\" % name)\n",
    "\n",
    "        # build decoder: logits, [batch_size, max_time, vocab_size]\n",
    "        train_logits, infer_outputs = build_decoder(\n",
    "            encoder_outputs, encoder_states, embeddings,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type,\n",
    "            state_pass, infer_batch_size, attn_wrapper, attn_num_units,\n",
    "            target_ids, infer_type, beam_size, max_iter,\n",
    "            name=\"%s_decoder\" % name)\n",
    "\n",
    "        # compute loss\n",
    "        with tf.name_scope('loss'):\n",
    "            final_ids = tf.pad(target_ids, [[0, 0], [0, 1]], constant_values=1)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=train_logits, labels=final_ids)\n",
    "\n",
    "            losses = tf.boolean_mask(losses, sequence_mask)\n",
    "            reduced_loss = tf.reduce_mean(losses)\n",
    "            CE = tf.reduce_sum(losses)\n",
    "\n",
    "            if l2_regularize is None:\n",
    "                return CE, reduced_loss, train_logits, infer_outputs\n",
    "            else:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(v)\n",
    "                                    for v in tf.trainable_variables()\n",
    "                                    if not('bias' in v.name)])\n",
    "\n",
    "                total_loss = reduced_loss + l2_regularize * l2_loss\n",
    "                return CE, total_loss, train_logits, infer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(sess, CE, mask, feed_dict):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a batch of data\n",
    "    \"\"\"\n",
    "    CE_words = sess.run(CE, feed_dict=feed_dict)\n",
    "    N_words = np.sum(mask)\n",
    "    return np.exp(CE_words / N_words)\n",
    "\n",
    "\n",
    "def loadfile(filename, is_source, max_length):\n",
    "    \"\"\"\n",
    "    Load and clean data\n",
    "    \"\"\"\n",
    "    def clean(row):\n",
    "        row = np.array(row.split(), dtype=np.int32)\n",
    "        leng = len(row)\n",
    "        if leng < max_length:\n",
    "            if is_source:\n",
    "                # represents constant zero padding\n",
    "                pads = -np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((pads, row))\n",
    "            else:\n",
    "                # represents EOS token\n",
    "                pads = -2 * np.ones(max_length - leng, dtype=np.int32)\n",
    "                row = np.concatenate((row, pads))\n",
    "        elif leng > max_length:\n",
    "            row = row[:max_length]\n",
    "        return row\n",
    "\n",
    "    df = pd.read_csv(filename, header=None, index_col=None)\n",
    "    data = np.array(df[0].apply(lambda t: clean(t)).tolist(), dtype=np.int32)\n",
    "    return data\n",
    "\n",
    "# saving and load\n",
    "def load(saver, sess, logdir):\n",
    "    print(\"Trying to restore saved checkpoints from {} ...\".format(logdir),\n",
    "          end=\"\")\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(logdir)\n",
    "    if ckpt:\n",
    "        print(\"  Checkpoint found: {}\".format(ckpt.model_checkpoint_path))\n",
    "        global_step = int(ckpt.model_checkpoint_path\n",
    "                          .split('/')[-1]\n",
    "                          .split('-')[-1])\n",
    "        print(\"  Global step was: {}\".format(global_step))\n",
    "        print(\"  Restoring...\", end=\"\")\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\" Done.\")\n",
    "        return global_step\n",
    "    else:\n",
    "        print(\" No checkpoint found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save(saver, sess, logdir, step):\n",
    "    model_name = 'model.ckpt'\n",
    "    checkpoint_path = os.path.join(logdir, model_name)\n",
    "    print('Storing checkpoint to {} ...'.format(logdir), end=\"\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "\n",
    "    saver.save(sess, checkpoint_path, global_step=step)\n",
    "    print(' Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML configuration\n",
    "def get_model_config(config):\n",
    "    enc_num_layers = config[\"encoder\"][\"num_layers\"]\n",
    "    enc_num_units = config[\"encoder\"][\"num_units\"]\n",
    "    enc_cell_type = config[\"encoder\"][\"cell_type\"]\n",
    "    enc_bidir = config[\"encoder\"][\"bidirectional\"]\n",
    "    dec_num_layers = config[\"decoder\"][\"num_layers\"]\n",
    "    dec_num_units = config[\"decoder\"][\"num_units\"]\n",
    "    dec_cell_type = config[\"decoder\"][\"cell_type\"]\n",
    "    state_pass = config[\"decoder\"][\"state_pass\"]\n",
    "    infer_batch_size = config[\"inference\"][\"infer_batch_size\"]\n",
    "    infer_type = config[\"inference\"][\"type\"]\n",
    "    beam_size = config[\"inference\"][\"beam_size\"]\n",
    "    max_iter = config[\"inference\"][\"max_length\"]\n",
    "    attn_num_units = config[\"decoder\"][\"attn_num_units\"]\n",
    "    l2_regularize = config[\"training\"][\"l2_regularize\"]\n",
    "\n",
    "    return (enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "            dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "            infer_batch_size, infer_type, beam_size, max_iter,\n",
    "            attn_num_units, l2_regularize)\n",
    "\n",
    "\n",
    "def get_training_config(config):\n",
    "    train_config = config[\"training\"]\n",
    "    logdir = train_config[\"logdir\"]\n",
    "    restore_from = train_config[\"restore_from\"]\n",
    "\n",
    "    learning_rate = train_config[\"learning_rate\"]\n",
    "    gpu_fraction = train_config[\"gpu_fraction\"]\n",
    "    max_checkpoints = train_config[\"max_checkpoints\"]\n",
    "    train_steps = train_config[\"train_steps\"]\n",
    "    batch_size = train_config[\"batch_size\"]\n",
    "    print_every = train_config[\"print_every\"]\n",
    "    checkpoint_every = train_config[\"checkpoint_every\"]\n",
    "\n",
    "    s_filename = train_config[\"train_source_file\"]\n",
    "    t_filename = train_config[\"train_target_file\"]\n",
    "    s_max_leng = train_config[\"source_max_length\"]\n",
    "    t_max_leng = train_config[\"target_max_length\"]\n",
    "\n",
    "    dev_s_filename = train_config[\"dev_source_file\"]\n",
    "    dev_t_filename = train_config[\"dev_target_file\"]\n",
    "\n",
    "    loss_fig = train_config[\"loss_fig\"]\n",
    "    perp_fig = train_config[\"perplexity_fig\"]\n",
    "\n",
    "    return (logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    "            train_steps, batch_size, print_every, checkpoint_every,\n",
    "            s_filename, t_filename, s_max_leng, t_max_leng, dev_s_filename,\n",
    "            dev_t_filename, loss_fig, perp_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "config_details = {\n",
    "    \"Name\": \"Seq2seq_BiLSTM_Attn_BeamSearch\",\n",
    "    \"embeddings\": {\n",
    "        \"vocab_size\": 1000,\n",
    "        \"embed_size\": 128,\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"decoder\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_units\": 256,\n",
    "        \"cell_type\": \"LSTM\",\n",
    "        \"state_pass\": True,\n",
    "        \"wrapper\": \"Attention\",\n",
    "        \"attn_num_units\": 128,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"infer_batch_size\": 15,\n",
    "        \"type\": \"beam_search\",\n",
    "        \"beam_size\": 5,\n",
    "        \"max_length\": 20,\n",
    "        \"infer_source_file\": \"./example/dev_source.txt\",\n",
    "        \"infer_source_max_length\": 25,\n",
    "        \"output_path\": \"./prediction.txt\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"l2_regularize\": None,\n",
    "        \"logdir\": \"./log_s2sattn/\",\n",
    "        \"restore_from\": \"./log_s2sattn/\",\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "        \"gpu_fraction\": 0.05,\n",
    "        \"max_checkpoints\": 10000,\n",
    "        \"train_steps\": 5000,\n",
    "        \"print_every\": 20,\n",
    "        \"checkpoint_every\": 1000,\n",
    "        \"train_source_file\": \"./example/train_source.txt\",\n",
    "        \"train_target_file\": \"./example/train_target.txt\",\n",
    "        \"dev_source_file\": \"./example/dev_source.txt\",\n",
    "        \"dev_target_file\": \"./example/dev_target.txt\",\n",
    "        \"source_max_length\": 25,\n",
    "        \"target_max_length\": 25,\n",
    "        \"loss_fig\": \"./training_loss_over_time\",\n",
    "        \"perplexity_fig\": \"./perplexity_over_time\",\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config_seq2seqAttn_beamsearch.yaml', \"w\") as f:\n",
    "    yaml.dump({\"configuration\": config_details}, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('config_seq2seqAttn_beamsearch.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config = config[\"configuration\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embeddings ...\n",
      "\tDone.\n",
      "Building model architecture ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# loading configurations\n",
    "name = config[\"Name\"]\n",
    "\n",
    "# Construct or load embeddings\n",
    "print(\"Initializing embeddings ...\")\n",
    "vocab_size = config[\"embeddings\"][\"vocab_size\"]\n",
    "embed_size = config[\"embeddings\"][\"embed_size\"]\n",
    "embeddings = init_embeddings(vocab_size, embed_size, name=name)\n",
    "print(\"\\tDone.\")\n",
    "\n",
    "# Build the model and compute losses\n",
    "source_ids = tf.placeholder(tf.int32, [None, None], name=\"source\")\n",
    "target_ids = tf.placeholder(tf.int32, [None, None], name=\"target\")\n",
    "sequence_mask = tf.placeholder(tf.bool, [None, None], name=\"mask\")\n",
    "\n",
    "attn_wrappers = {\n",
    "    \"None\": None,\n",
    "    \"Attention\": AttentionWrapper,\n",
    "    \"ECM\": ECMWrapper,\n",
    "}\n",
    "attn_wrapper = attn_wrappers.get(config[\"decoder\"][\"wrapper\"])\n",
    "\n",
    "(enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    " dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    " infer_batch_size, infer_type, beam_size, max_iter,\n",
    " attn_num_units, l2_regularize) = get_model_config(config)\n",
    "\n",
    "print(\"Building model architecture ...\")\n",
    "CE, loss, logits, infer_outputs = compute_loss(\n",
    "    source_ids, target_ids, sequence_mask, embeddings,\n",
    "    enc_num_layers, enc_num_units, enc_cell_type, enc_bidir,\n",
    "    dec_num_layers, dec_num_units, dec_cell_type, state_pass,\n",
    "    infer_batch_size, infer_type, beam_size, max_iter,\n",
    "    attn_wrapper, attn_num_units, l2_regularize, name)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronlai/Desktop/ECE276C_RL/env3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to restore saved checkpoints from ./log_s2sattn/ ... No checkpoint found.\n"
     ]
    }
   ],
   "source": [
    "# Preparing for training\n",
    "(logdir, restore_from, learning_rate, gpu_fraction, max_checkpoints,\n",
    " train_steps, batch_size, print_every, checkpoint_every, s_filename,\n",
    " t_filename, s_max_leng, t_max_leng, dev_s_filename, dev_t_filename,\n",
    " loss_fig, perp_fig) = get_training_config(config)\n",
    "\n",
    "# Even if we restored the model, we will treat it as new training\n",
    "# if the trained model is written into an arbitrary location.\n",
    "is_overwritten_training = logdir != restore_from\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                   epsilon=1e-4)\n",
    "trainable = tf.trainable_variables()\n",
    "optim = optimizer.minimize(loss, var_list=trainable)\n",
    "\n",
    "# Set up session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False,\n",
    "                                        gpu_options=gpu_options))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Saver for storing checkpoints of the model.\n",
    "saver = tf.train.Saver(var_list=tf.trainable_variables(),\n",
    "                       max_to_keep=max_checkpoints)\n",
    "\n",
    "try:\n",
    "    saved_global_step = load(saver, sess, restore_from)\n",
    "    if is_overwritten_training or saved_global_step is None:\n",
    "        # The first training step will be saved_global_step + 1,\n",
    "        # therefore we put -1 here for new or overwritten trainings.\n",
    "        saved_global_step = -1\n",
    "\n",
    "except Exception:\n",
    "    print(\"Something went wrong while restoring checkpoint. \"\n",
    "          \"Training is terminated to avoid the overwriting.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "\n",
    "source_data = loadfile(s_filename, is_source=True,\n",
    "                       max_length=s_max_leng) + embed_shift\n",
    "target_data = loadfile(t_filename, is_source=False,\n",
    "                       max_length=t_max_leng) + embed_shift\n",
    "masks = (target_data != -1)\n",
    "n_data = len(source_data)\n",
    "\n",
    "dev_source_data = None\n",
    "if dev_s_filename is not None:\n",
    "    dev_source_data = loadfile(dev_s_filename, is_source=True,\n",
    "                               max_length=s_max_leng) + embed_shift\n",
    "    dev_target_data = loadfile(dev_t_filename, is_source=False,\n",
    "                               max_length=t_max_leng) + embed_shift\n",
    "    dev_masks = (dev_target_data != -1)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n",
      "step 0, loss = 6.910056, perp: 998.526, dev_prep: 998.214, (3.154 sec/step)\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n",
      "\n",
      "Storing checkpoint to ./log_s2sattn/ ... Done.\n"
     ]
    }
   ],
   "source": [
    "last_saved_step = saved_global_step\n",
    "num_steps = saved_global_step + train_steps\n",
    "losses = []\n",
    "steps = []\n",
    "perps = []\n",
    "dev_perps = []\n",
    "\n",
    "print(\"Start training ...\")\n",
    "try:\n",
    "    for step in range(saved_global_step + 1, num_steps):\n",
    "        start_time = time.time()\n",
    "        rand_indexes = np.random.choice(n_data, batch_size)\n",
    "        source_batch = source_data[rand_indexes]\n",
    "        target_batch = target_data[rand_indexes]\n",
    "        mask_batch = masks[rand_indexes]\n",
    "\n",
    "        feed_dict = {\n",
    "            source_ids: source_batch,\n",
    "            target_ids: target_batch,\n",
    "            sequence_mask: mask_batch,\n",
    "        }\n",
    "\n",
    "        loss_value, _ = sess.run([loss, optim], feed_dict=feed_dict)\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            # train perplexity\n",
    "            t_perp = compute_perplexity(sess, CE, mask_batch, feed_dict)\n",
    "            perps.append(t_perp)\n",
    "\n",
    "            # dev perplexity\n",
    "            dev_str = \"\"\n",
    "            if dev_source_data is not None:\n",
    "                dev_feed_dict = {\n",
    "                    source_ids: dev_source_data,\n",
    "                    target_ids: dev_target_data,\n",
    "                    sequence_mask: dev_masks,\n",
    "                }\n",
    "                dev_perp = compute_perplexity(\n",
    "                    sess, CE, dev_masks, dev_feed_dict)\n",
    "                dev_perps.append(dev_perp)\n",
    "                dev_str = \"dev_prep: {:.3f}, \".format(dev_perp)\n",
    "\n",
    "            steps.append(step)\n",
    "            info = 'step {:d}, loss = {:.6f}, '\n",
    "            info += 'perp: {:.3f}, {}({:.3f} sec/step)'\n",
    "            print(info.format(step, loss_value, t_perp, dev_str, duration))\n",
    "\n",
    "        if step % checkpoint_every == 0:\n",
    "            save(saver, sess, logdir, step)\n",
    "            last_saved_step = step\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Introduce a line break after ^C so save message is on its own line.\n",
    "    print()\n",
    "\n",
    "finally:\n",
    "    if step > last_saved_step:\n",
    "        save(saver, sess, logdir, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "# plt.savefig(loss_fig)\n",
    "\n",
    "# plot perplexity\n",
    "plt.figure()\n",
    "if len(perps) > len(steps):\n",
    "    perps.pop()\n",
    "plt.plot(steps, perps, label=\"train\")\n",
    "if dev_source_data is not None:\n",
    "    plt.plot(steps, dev_perps, label=\"dev\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig(perp_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading inference data ...\")\n",
    "\n",
    "# id_0, id_1, id_2 preserved for SOS, EOS, constant zero padding\n",
    "embed_shift = 3\n",
    "filename = config[\"inference\"][\"infer_source_file\"]\n",
    "max_leng = config[\"inference\"][\"infer_source_max_length\"]\n",
    "source_data = loadfile(filename, is_source=True,\n",
    "                       max_length=max_leng) + embed_shift\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inferring ...\n",
      "\tDone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start inferring ...\")\n",
    "final_result = []\n",
    "n_data = source_data.shape[0]\n",
    "n_pad = n_data % infer_batch_size\n",
    "if n_pad > 0:\n",
    "    n_pad = infer_batch_size - n_pad\n",
    "\n",
    "pad = np.zeros((n_pad, max_leng), dtype=np.int32)\n",
    "source_data = np.concatenate((source_data, pad))\n",
    "\n",
    "for ith in range(int(len(source_data) / infer_batch_size)):\n",
    "    start = ith * infer_batch_size\n",
    "    end = (ith + 1) * infer_batch_size\n",
    "    batch = source_data[start:end]\n",
    "\n",
    "    result = sess.run(infer_outputs, feed_dict={source_ids: batch})\n",
    "    result = result.ids[:, :, 0]\n",
    "\n",
    "    if result.shape[1] < max_iter:\n",
    "        l_pad = max_iter - result.shape[1]\n",
    "        result = np.concatenate(\n",
    "            (result, np.ones((infer_batch_size, l_pad))), axis=1)\n",
    "\n",
    "    final_result.append(result)\n",
    "\n",
    "final_result = np.concatenate(final_result)[:n_data] - 3\n",
    "final_result[final_result < 0] = -1\n",
    "final_result = final_result.astype(str).tolist()\n",
    "final_result = list(map(lambda t: \" \".join(t), final_result))\n",
    "\n",
    "df = pd.DataFrame(data={\"0\": final_result})\n",
    "df.to_csv(config[\"inference\"][\"output_path\"], header=None, index=None)\n",
    "print(\"\\tDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.52708425227248"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "random_indexes = np.random.choice(1000, infer_batch_size)\n",
    "s = dev_source_data[random_indexes]\n",
    "t = dev_target_data[random_indexes]\n",
    "m = (t != -1)\n",
    "\n",
    "feed_dict = {\n",
    "    source_ids: s,\n",
    "    target_ids: t,\n",
    "    sequence_mask: m,\n",
    "}\n",
    "\n",
    "compute_perplexity(sess, CE, dev_masks, dev_feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
